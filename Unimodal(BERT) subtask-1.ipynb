{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7f9ae242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Labels: 20\n",
      "Label Names: {'Slogans', \"Misrepresentation of Someone's Position (Straw Man)\", 'Exaggeration/Minimisation', 'Whataboutism', 'Name calling/Labeling', 'Causal Oversimplification', 'Doubt', 'Black-and-white Fallacy/Dictatorship', 'Appeal to fear/prejudice', 'Reductio ad hitlerum', 'Flag-waving', 'Bandwagon', 'Presenting Irrelevant Data (Red Herring)', 'Glittering generalities (Virtue)', 'Thought-terminating clichÃ©', 'Appeal to authority', 'Smears', 'Loaded Language', 'Obfuscation, Intentional vagueness, Confusion', 'Repetition'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "json_file_path = '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/subtask1/train.json'\n",
    "\n",
    "# Swap the file opening and data loading statements\n",
    "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "labels = [sample.get(\"labels\", []) for sample in data]\n",
    "\n",
    "# lists to get all labels\n",
    "all_labels = [label for sublist in labels for label in sublist]\n",
    "\n",
    "num_unique_labels = len(set(all_labels))\n",
    "print(f\"Number of Unique Labels: {num_unique_labels}\")\n",
    "\n",
    "print(\"Label Names:\", set(all_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3fc9e2d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5600, 128) (700, 128) (700, 128)\n",
      "(5600, 128) (700, 128) (700, 128)\n",
      "(5600, 20, 20) (700, 20, 20) (700, 20, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 5600, 700\n  y sizes: 5600\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 236\u001b[0m\n\u001b[1;32m    234\u001b[0m meme_classifier \u001b[38;5;241m=\u001b[39m MemeClassification(label_tree)\n\u001b[1;32m    235\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m--> 236\u001b[0m \u001b[43mmeme_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/subtask1/train.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[221], line 166\u001b[0m, in \u001b[0;36mMemeClassification.run_experiment\u001b[0;34m(self, json_file_path, num_classes)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model(num_classes)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    169\u001b[0m hierarchical_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_with_hierarchy(X_test, attention_masks_test, y_test_one_hot)\n",
      "Cell \u001b[0;32mIn[221], line 62\u001b[0m, in \u001b[0;36mMemeClassification.train_model\u001b[0;34m(self, X_train, y_train, X_val, y_val, attention_masks_train, attention_masks_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train, X_val, y_val, attention_masks_train, attention_masks_val, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# automatic class weights calculation\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/python/miniconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/python/miniconda3/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1960\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1953\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1954\u001b[0m         label,\n\u001b[1;32m   1955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1956\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1957\u001b[0m         ),\n\u001b[1;32m   1958\u001b[0m     )\n\u001b[1;32m   1959\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1960\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 5600, 700\n  y sizes: 5600\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import deque\n",
    "from collections.abc import Iterable\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report\n",
    "\n",
    "class MemeClassification:\n",
    "    def __init__(self, label_tree):\n",
    "        self.label_tree = label_tree\n",
    "        self.label_binarizer = None\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        self.model = None\n",
    "\n",
    "    def load_data(self, json_file_path):\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        texts = [sample[\"text\"] for sample in data]\n",
    "        labels = [sample.get(\"labels\", []) for sample in data]\n",
    "\n",
    "        self.label_binarizer = MultiLabelBinarizer()\n",
    "        y = self.label_binarizer.fit_transform(labels)\n",
    "\n",
    "        return texts, y\n",
    "\n",
    "    def tokenize_and_pad(self, texts):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for text in texts:\n",
    "            encoded_text = self.tokenizer(text, max_length=128, truncation=True, padding='max_length', return_tensors='tf')\n",
    "            input_ids.append(encoded_text['input_ids'])\n",
    "            attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "        input_ids = np.concatenate(input_ids, axis=0)\n",
    "        attention_masks = np.concatenate(attention_masks, axis=0)\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "    def build_model(self, num_classes):\n",
    "        input_ids_input = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "        attention_masks_input = Input(shape=(128,), dtype=tf.int32, name=\"attention_masks\")\n",
    "\n",
    "        bert_output = self.bert_model(input_ids_input, attention_mask=attention_masks_input)[0]\n",
    "        pooled_output = GlobalAveragePooling1D()(bert_output)\n",
    "        output_layer = Dense(num_classes, activation='sigmoid')(pooled_output)\n",
    "\n",
    "        self.model = Model(inputs=[input_ids_input, attention_masks_input], outputs=output_layer)\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_val, y_val, attention_masks_train, attention_masks_val, epochs=3, batch_size=8):\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            [X_train, attention_masks_train],\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=([X_val, attention_masks_val], y_val),\n",
    "            class_weight='auto' # automatic class weights calculation\n",
    "        )\n",
    "       \n",
    "    def calculate_hierarchy_distance(self, node1, node2):\n",
    "        # Check if the nodes are leaf nodes\n",
    "        if node1 not in self.label_tree or node2 not in self.label_tree:\n",
    "            return None\n",
    "\n",
    "        # Find ancestors of each leaf node\n",
    "        ancestors1 = set()\n",
    "        current_node = node1\n",
    "        while current_node:\n",
    "            ancestors1.add(current_node)\n",
    "            current_node = next((parent for parent, children in self.label_tree.items() if current_node in children), None)\n",
    "\n",
    "        ancestors2 = set()\n",
    "        current_node = node2\n",
    "        while current_node:\n",
    "            ancestors2.add(current_node)\n",
    "            current_node = next((parent for parent, children in self.label_tree.items() if current_node in children), None)\n",
    "\n",
    "        # Find common ancestors\n",
    "        common_ancestors = ancestors1.intersection(ancestors2)\n",
    "\n",
    "        # distance based on levels\n",
    "        distance = len(ancestors1) + len(ancestors2) - 2 * len(common_ancestors)\n",
    "\n",
    "        return distance\n",
    "    \n",
    "    # from the cited article on the task page, i came up with this evaulation calculation\n",
    "    def evaluate_with_hierarchy(self, X_test, attention_masks_test, y_test_one_hot):\n",
    "        predictions = self.model.predict([X_test, attention_masks_test])\n",
    "\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "        total_f1 = 0.0\n",
    "        total_samples = len(y_test_one_hot)\n",
    "\n",
    "        for i in range(total_samples):\n",
    "            predicted_probabilities = predictions[i]\n",
    "\n",
    "            # Decoding predicted labels\n",
    "            predicted_labels = [self.label_binarizer.classes_[j] for j in range(len(self.label_binarizer.classes_)) if\n",
    "                                predicted_probabilities[j] > 0.5]\n",
    "            gold_labels = [self.label_binarizer.classes_[j] for j in range(len(self.label_binarizer.classes_)) if\n",
    "                           y_test_one_hot[i][j] == 1]\n",
    "\n",
    "            # Checks if the prediction is a leaf node and is the correct label\n",
    "            if set(predicted_labels).issubset(set(gold_labels)):\n",
    "                reward = 1.0  # Full reward\n",
    "            elif any(\n",
    "                    self.calculate_hierarchy_distance(predicted_label, gold_label) > 0 for predicted_label in\n",
    "                    predicted_labels for gold_label in gold_labels):\n",
    "                reward = 0.5  # Partial reward\n",
    "            else:\n",
    "                reward = 0.0  # Null reward\n",
    "\n",
    "            # total_precision, total_recall on reward\n",
    "            total_precision += reward\n",
    "            total_recall += reward\n",
    "\n",
    "        # average metrics\n",
    "        average_precision = total_precision / total_samples\n",
    "        average_recall = total_recall / total_samples\n",
    "\n",
    "        # hierarchical F1\n",
    "        hierarchical_f1 = 2 * (average_precision * average_recall) / (average_precision + average_recall) if (\n",
    "                average_precision + average_recall) != 0 else 0.0\n",
    "\n",
    "        return hierarchical_f1\n",
    "\n",
    "    \n",
    "    def run_experiment(self, json_file_path, num_classes):\n",
    "        data = self.load_data(json_file_path)\n",
    "        texts, y = self.preprocess_data(data)\n",
    "        input_ids, attention_masks = self.tokenize_and_pad(texts)\n",
    "        \n",
    "       # split the data\n",
    "        X_train, X_temp, y_train, y_temp, attention_masks_train, attention_masks_temp = train_test_split(\n",
    "            input_ids, y, attention_masks, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        X_val, X_test, y_val, y_test, attention_masks_val, attention_masks_test = train_test_split(\n",
    "            X_temp, y_temp, attention_masks_temp, test_size=0.5, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Convert integer labels to one-hot encoded labels\n",
    "        y_train_one_hot = to_categorical(y_train, num_classes)\n",
    "        y_val_one_hot = to_categorical(y_val, num_classes)\n",
    "        y_test_one_hot = to_categorical(y_test, num_classes)\n",
    "        \n",
    "        print(X_train.shape, X_val.shape, X_test.shape)\n",
    "        print(attention_masks_train.shape, attention_masks_val.shape, attention_masks_test.shape)\n",
    "        print(y_train_one_hot.shape, y_val_one_hot.shape, y_test_one_hot.shape)\n",
    "\n",
    "        # build model\n",
    "        self.build_model(num_classes)\n",
    "        # train the model\n",
    "        self.train_model(X_train, attention_masks_train, y_train, X_val, attention_masks_val, y_val)\n",
    "\n",
    "        # Evaluate the model\n",
    "        hierarchical_f1 = self.evaluate_with_hierarchy(X_test, attention_masks_test, y_test_one_hot)\n",
    "\n",
    "        print(f\"Average Hierarchical F1: {hierarchical_f1}\")\n",
    "\n",
    "        # model.predict for the test set\n",
    "        predictions = self.model.predict([X_test, attention_masks_test])\n",
    "\n",
    "        # Decoding the encoded labels\n",
    "        decoded_predictions = np.argmax(predictions, axis=1)\n",
    "        decoded_y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "        # classification report\n",
    "        print(classification_report(decoded_y_test, decoded_predictions))\n",
    "        \n",
    "# hierarchical tree\n",
    "label_tree = {\n",
    "    'Persuasion': {\n",
    "        'Pathos': {\n",
    "            'Appeal to Emotion(visual)': 1,\n",
    "            'Exaggeration/Minimisation': 2,\n",
    "            'Loaded Language': 3,\n",
    "            'Flag waving': 4,\n",
    "            'Appeal to fear/prejudice': 5,\n",
    "            'Transfer': 6\n",
    "        },\n",
    "        'Ethos': {\n",
    "            'Transfer': 6,\n",
    "            'Glittering generalities': 7,\n",
    "            'Appeal to authority': 8,\n",
    "            'Bandwagon': 9,\n",
    "            'Ad Hominem': {\n",
    "                'Name calling/Labelling': 10,\n",
    "                'Doubt': 11,\n",
    "                'Smears': 12,\n",
    "                'Reduction and Hitlerium': 13,\n",
    "                'Whataboutism': 14\n",
    "            }\n",
    "        },\n",
    "        'Logos': {\n",
    "            'Repetition': 15,\n",
    "            'Obfuscation, Intentional vagueness, Confusion': 16,\n",
    "            'Justification': {\n",
    "                'Flag waving': 4,\n",
    "                'Appeal to fear/prejudice': 5,\n",
    "                'Appeal to Authority': 8,\n",
    "                'Bandwagon': 9,\n",
    "                'Slogans': 17\n",
    "            },\n",
    "            'Reasoning': {\n",
    "                'Distraction': {\n",
    "                    'Whataboutism': 14,\n",
    "                    'Presenting Irrelevant Data (Red Herring)': 18,\n",
    "                    'Straw Man': 19\n",
    "                },\n",
    "                'Simplification': {\n",
    "                    'Black-and-white Fallacy/Dictatorship': 20,\n",
    "                    'Casual Oversimplification': 21,\n",
    "                    'Thought-terminating clichÃ©': 22\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "meme_classifier = MemeClassification(label_tree)\n",
    "num_classes = 20\n",
    "meme_classifier.run_experiment('/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/subtask1/train.json', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53544297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
