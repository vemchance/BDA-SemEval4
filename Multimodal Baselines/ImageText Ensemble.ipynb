{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1bfdfd-06df-4eea-ba40-040256298d73",
   "metadata": {},
   "source": [
    "A simpler Text + Image Encoder model as a baseline. Uses BERT + VGG16 initiailly, but can be changed to anything else. At present VGG16 has a linear layer, so it's not easy to access the output features in the model. However if the vision model is updated, the code can be changed in the ensemble model structure to accept the output layer of a different model. Note as well the input size would need to be changed in the dataloader.\n",
    "\n",
    "TO DO:\n",
    "- Model selection for NLP and Vision\n",
    "- Dataloader for model selection\n",
    "- Ensemble model for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053763b1-f276-4148-ade4-b9a2a84bb83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    BertForSequenceClassification)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryF1Score, MulticlassF1Score, MultilabelConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75735d-43ae-4e17-a683-35e53aa91b83",
   "metadata": {},
   "source": [
    "# Data\n",
    "Same junk from the VisionTextEncoder notebook, but this time using the binary classification as an experiment.\n",
    "\n",
    "TO DO:\n",
    "- Better data loading, just copying and pasting atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60722791-9444-4222-9193-0b0b54cb006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\train'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2b\\train.json')\n",
    "df = pd.merge(df, images_df, on='image')\n",
    "df.fillna(' ', inplace=True)\n",
    "\n",
    "path_val = r'X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\val'\n",
    "images_val = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path_val) for f in filenames]\n",
    "images_df_val = pd.DataFrame(images_val, columns=['filepath'])\n",
    "images_df_val['image'] = images_df_val['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df_val = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2b\\val.json')\n",
    "df_val = pd.merge(df_val, images_df_val, on='image')\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['encoded_labels'] = le.fit_transform(df['label']).tolist()\n",
    "df_val['encoded_labels'] = le.fit_transform(df_val['label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c76e38b-14fd-48ed-8eec-9e2a9e534d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists\n",
    "\n",
    "images = [str(i) for i in df['filepath'].values]\n",
    "texts = [str(i) for i in df['text'].astype(str).values.tolist()]\n",
    "labels = df['encoded_labels'].values\n",
    "\n",
    "images_val = [str(i) for i in df_val['filepath'].values]\n",
    "texts_val = [str(i) for i in df_val['text'].astype(str).values.tolist()]\n",
    "labels_val = df_val['encoded_labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae4b69-d9ad-4e15-b14c-e9119e168d43",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3745b1bc-3076-455b-acf0-cf97afd97287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations for the raw images\n",
    "# change size here for different model\n",
    "\n",
    "model_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3cea54d-79f8-4cbc-bc41-1282fd3a30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img, txt, lbs, tokenizer, n_classes, transform):\n",
    "        self.image = img\n",
    "        self.text = txt\n",
    "        self.labels = lbs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_classes = n_classes\n",
    "        self.transforms = transform  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        image = self.image[idx]\n",
    "\n",
    "        text_encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        sample = {'input_ids': text_encoded['input_ids'],\n",
    "                  'attention_mask': text_encoded['attention_mask'],\n",
    "                  'image': image,\n",
    "                  \"label\": label}\n",
    "        sample = {k:v.squeeze() for k,v in sample.items()}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97c642-7112-4d14-851a-634fcc94b904",
   "metadata": {},
   "source": [
    "PyTorch has a problem stacking multiple inputs, the function below converts the data to a list but the model training has to change for this to work. For now batch_size = 1 allows this to work, or sometimes it works depending on the inputs. The collate_fn custom input is good enough but produces the inputs as a list which are more difficult to work with. See more: https://discuss.pytorch.org/t/making-custom-image-to-image-dataset-using-collate-fn-and-dataloader/55951\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attn_mask = [item['attention_mask'] for item in batch]\n",
    "    img = [item['image'] for item in batch]\n",
    "    target = [item['label'] for item in batch]\n",
    "    return [input_ids, attn_mask, img, target] -->\n",
    "\n",
    "Probably the truncation for the text is the problem, seems to work for now but doesn't always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456741b4-be91-4894-a4bf-8f04cf3b9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, use_fast=False)\n",
    "train_dataset = VisionTextDataset(img=images, txt=texts, lbs=labels, tokenizer=tokenizer, n_classes=2, transform=model_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa3d36b-c6c0-4c08-8fa4-55011af96ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([16, 512])\n",
      "attention_mask torch.Size([16, 512])\n",
      "image torch.Size([16, 3, 224, 224])\n",
      "label torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dadee9e-a354-4f68-ae27-56674cf01788",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VisionTextDataset(img=images_val, txt=texts_val, lbs=labels_val, tokenizer=tokenizer, n_classes=2, transform=model_transforms)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb81d83-a78e-4d63-b925-58f9b8923c4a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8510574d-1fe6-4a8e-ae87-9c6d22d7070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "text_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "vision_model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "#num_ftrs = vision_model.classifier[0].in_features\n",
    "#vision_model.classifier[0] = nn.Linear(num_ftrs, args.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34254648-0ec6-4eac-8856-ce8f443083e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, image_model, text_model, num_classes):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.text_model = text_model\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            1000 +                             # this is hardcoded to 1000 for VGG16, as last layer is linear\n",
    "            text_model.config.hidden_size,     # usual method is something like model.fc.out_features\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features, text_features, attention_masks):\n",
    "        img_feat = self.image_model(image_features)\n",
    "        txt_feat_bert = self.text_model(text_features, attention_mask=attention_masks).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        combined_feat = torch.cat((img_feat, txt_feat_bert), dim=1)\n",
    "        return self.fc(combined_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cacf92b2-0b08-4e8e-b1a1-a02e47967328",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = EnsembleModel(vision_model, text_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ec602-c572-4404-bdd8-00b550c84870",
   "metadata": {},
   "source": [
    "# Simple Model (Binary Classification - Subtask2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7f26d-3fc7-4543-b93e-1b03e9217458",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd4a5931-357f-465e-9cc8-ae4462d8a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12a17633f024e95875c1ac3d3532151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2545\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de52936724d143b2829c19bbcbe548ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.0064\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592dfe212c694ac5b6555005a50ca020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.0014\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b275542d844601810875cada16e0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.0019\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7891de7e29a7423bb95f903ff3819a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.Adam(ensemble_model.parameters(), lr=learning_rate)\n",
    "n_epochs = 5\n",
    "\n",
    "actual, prediction = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_losses = []\n",
    "    ensemble_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                 attention_masks=batch['attention_mask'],\n",
    "                       image_features=batch['image'])\n",
    "        label = batch['label']\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b5598-af0b-48db-b56a-552f22d00f1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training and Val Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12d48e0b-9efa-4e24-abfe-497a8151a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.Adam(ensemble_model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_epoch(ensemble_model, train_dataloader):\n",
    "    actuals, predictions = [], []\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                 attention_masks=batch['attention_mask'],\n",
    "                                 image_features=batch['image'])\n",
    "        label = batch['label']\n",
    "    \n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        actuals.extend(label.cpu().numpy().astype(int))\n",
    "        predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predicted_labels = predictions.argmax(1)\n",
    "    accuracy = (predicted_labels == actuals).mean()\n",
    "\n",
    "    return train_loss/len(train_dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a05daa42-512a-4748-b511-ee48cf700c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(ensemble_model, val_dataloader):\n",
    "    \n",
    "    actuals, predictions = [], []\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    with torch.no_grad():    \n",
    "    \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            # get the inputs;\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                \n",
    "            # forward + backward + optimize\n",
    "            outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                     attention_masks=batch['attention_mask'],\n",
    "                                     image_features=batch['image'])\n",
    "            label = batch['label']\n",
    "        \n",
    "            cur_val_loss = criterion(outputs, label)\n",
    "    \n",
    "            actuals.extend(label.cpu().numpy().astype(int))\n",
    "            predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy())\n",
    "    \n",
    "            val_loss += cur_val_loss.item()\n",
    "            \n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predicted_labels = predictions.argmax(1)\n",
    "    accuracy = (predicted_labels == actuals).mean()\n",
    "\n",
    "    return val_loss/len(val_dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdfe9174-53df-4679-aae5-1ac4ff98384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d942a1bd-9894-45c2-830b-dc6d6345815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b57a67d5654e1597b2fb42f206f723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:1 / 10,train loss:0.10252, train acc: 0.95833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f518dad0e9f249c28fdfa4abb705041d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:1 / 10 Val loss:0.63411, Val acc:0.77333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62b0051f2e44660ae8024e1d586d961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:2 / 10,train loss:0.02184, train acc: 0.99917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be96f0b20d164dc29722af9fb6e2c601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:2 / 10 Val loss:0.80614, Val acc:0.80000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d8c43d69e14167b4402791dd47f4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:3 / 10,train loss:0.00761, train acc: 0.99917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab0e8c2ba624a64b6eb020519777a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:3 / 10 Val loss:0.66051, Val acc:0.80667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2260e0c09ed4ec2ad62ff97988cb577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:4 / 10,train loss:0.00544, train acc: 1.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3f36d94bce49efad7753247adae839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:4 / 10 Val loss:0.83755, Val acc:0.77333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd786c30973745b58049a96002b88105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:5 / 10,train loss:0.00224, train acc: 1.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433636fa385c4104ad1478a87c169ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:5 / 10 Val loss:1.01516, Val acc:0.80667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3029348aa244c62ad0467bbb7d5048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:6 / 10,train loss:0.00112, train acc: 1.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb91c57b1f7e4edfad3ea0c024722e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:6 / 10 Val loss:0.94118, Val acc:0.79333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c59b7a964d42df9dd3bbfb00ab736c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:7 / 10,train loss:0.00088, train acc: 1.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef9700a43a0437a962c9b65ea5ae1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:7 / 10 Val loss:0.96382, Val acc:0.78667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8ef258c43247ab8f1255e0f660c30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:8 / 10,train loss:0.00054, train acc: 1.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a8a44aa68a425cb3d675043ee915a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:8 / 10 Val loss:0.95280, Val acc:0.79333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741f254963384573bee7e8f605360ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:9 / 10,train loss:0.00036, train acc: 1.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d749b3d90c8a4aa888196819700a9060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:9 / 10 Val loss:0.98356, Val acc:0.77333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f76171e82db40a29bc37bc66fc1f546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:10 / 10,train loss:0.00037, train acc: 1.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b18f91f426433586ea40ab18d2499a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:10 / 10 Val loss:1.20003, Val acc:0.78000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train_epoch(ensemble_model=ensemble_model, train_dataloader=train_dataloader)\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {NUM_EPOCHS},train loss:{train_loss:.5f}, train acc: {train_acc:.5f}\")\n",
    "    val_loss, val_acc = val_epoch(ensemble_model=ensemble_model, val_dataloader=val_dataloader)\n",
    "\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {NUM_EPOCHS} Val loss:{val_loss:.5f}, Val acc:{val_acc:.5f}\")\n",
    "\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45a38981-c7bf-47dd-a06f-7c03031d9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ensemble_model, 'Bert+VGG16_ensemble')\n",
    "torch.save(ensemble_model.state_dict(),'Bert+VGG16_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcc559-06fd-4664-a6bd-9e0698186489",
   "metadata": {},
   "source": [
    "## Model Structure for Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a868d-f392-4b33-a36a-cf64f9f1dd9c",
   "metadata": {},
   "source": [
    "TO DO\n",
    "Rewrite the dataloader and ensemble model to accept different models, similiar to Unimodal baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33679a71-6b76-4b02-a358-7a6712c91a4e",
   "metadata": {},
   "source": [
    "# Simple Model - Multilabel Classification (Subtask2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42d977-6f1e-4ce5-8a0d-c5ab9970a85e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data\n",
    "NOTE: There is ONE instance in the dataset where no technique is recorded. This is in the training data: prop_meme_24430.png. For now I am going to reclassify it as No Technique but have some concerns about how underrepresented this class is throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d884ca-7dd2-4cc1-86fd-dc0fbc5c4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2a_images\\train_images\\train_images'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2a\\train.json')\n",
    "df = pd.merge(df, images_df, on='image')\n",
    "\n",
    "# reclassify the no technique\n",
    "index = df[df['labels'].apply(lambda x: len(x)) == 0].index.values.astype(int)[0]\n",
    "df.loc[index, 'labels']=['No Technique']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5595b328-ba14-4579-9b88-ef5c8a229d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['labels'].values\n",
    "unique_labels = []\n",
    "\n",
    "for i in labels:\n",
    "    for x in i:\n",
    "        unique_labels.append(x)\n",
    "unique_labels = list(set(unique_labels))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "# try a one-hot as well\n",
    "ml = MultiLabelBinarizer()\n",
    "\n",
    "df['one_hot_labels'] = ml.fit_transform(df['labels'])[:,1:].tolist()\n",
    "df['encoded_labels'] = df['labels'].apply(le.transform)\n",
    "\n",
    "images = [str(i) for i in df['filepath'].values]\n",
    "texts = [str(i) for i in df['text'].astype(str).values.tolist()]\n",
    "labels = df['encoded_labels'].values\n",
    "one_hot_labels = df['one_hot_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9842c6-1ed3-411f-be04-b5b6b098e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2a_images\\validation_images\\validation_images'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df_val = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2a\\validation.json')\n",
    "df_val = pd.merge(df_val, images_df, on='image')\n",
    "\n",
    "# reclassify the no technique - there isn't one in val\n",
    "# index = df_val[df_val['labels'].apply(lambda x: len(x)) == 0].index.values.astype(int)[0]\n",
    "# df_val.loc[index, 'labels']=['No Technique']\n",
    "\n",
    "val_labels = df_val['labels'].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "# try a one-hot as well\n",
    "ml = MultiLabelBinarizer()\n",
    "\n",
    "df_val['one_hot_labels'] = ml.fit_transform(df_val['labels'])[:,1:].tolist()\n",
    "df_val['encoded_labels'] = df_val['labels'].apply(le.transform)\n",
    "\n",
    "images_val = [str(i) for i in df_val['filepath'].values]\n",
    "texts_val = [str(i) for i in df_val['text'].astype(str).values.tolist()]\n",
    "labels_val =df_val['encoded_labels'].values\n",
    "one_hot_labels_val = df_val['one_hot_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cfd585-21ce-418c-9e95-6e7f7c8573ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels need to be padded\n",
    "# unless onehot is used\n",
    "\n",
    "model_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "\n",
    "class VisionTextDataset_Multi(torch.utils.data.Dataset):\n",
    "    def __init__(self, img, txt, lbs, tokenizer, n_classes, transform):\n",
    "        self.image = img\n",
    "        self.text = txt\n",
    "        self.labels = lbs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_classes = n_classes\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        image = self.image[idx]        \n",
    "            \n",
    "        text_encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        \n",
    "        def pad_tensor(t):\n",
    "            t = torch.tensor(t)\n",
    "            padding = self.n_classes - t.size()[0]\n",
    "            t = torch.nn.functional.pad(t,(0,padding))\n",
    "            return t\n",
    "        \n",
    "        \n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        label = pad_tensor((self.labels[idx]))\n",
    "\n",
    "        sample = {'input_ids': text_encoded['input_ids'],\n",
    "                  'attention_mask': text_encoded['attention_mask'],\n",
    "                  'image': image,\n",
    "                  \"label\": label}\n",
    "        sample = {k:v.squeeze() for k,v in sample.items()}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33db7f34-1d0a-421c-a432-3db9a7f88b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([16, 512]) torch.int64\n",
      "attention_mask torch.Size([16, 512]) torch.int64\n",
      "image torch.Size([16, 3, 224, 224]) torch.float32\n",
      "label torch.Size([16, 23]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, use_fast=False)\n",
    "train_dataset = VisionTextDataset_Multi(img=images, txt=texts, lbs=one_hot_labels, tokenizer=tokenizer, n_classes=23, transform=model_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size(), v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d5076d-12bf-431e-bf33-fdb45214a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VisionTextDataset_Multi(img=images_val, txt=texts_val, lbs=one_hot_labels_val, tokenizer=tokenizer, n_classes=23, transform=model_transforms)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6064d5-29cf-4384-9dbd-f4b69c81157e",
   "metadata": {},
   "source": [
    "## Training\n",
    "Just reuse the same ensemble, but BCE with logits instead for one hot, and change label to label.float()\n",
    "\n",
    "TO DO: Accuracy. Should be calculating when each 1 is matched and then average over the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec7caf4-f3ae-41e2-ba49-6ee7b1cdf068",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model_multi = EnsembleModel(vision_model, text_model, 23)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.Adam(ensemble_model.parameters(), lr=learning_rate)\n",
    "metric = BinaryAccuracy()\n",
    "f1 = BinaryF1Score()\n",
    "\n",
    "def train_epoch(ensemble_model, train_dataloader):\n",
    "    \n",
    "    actuals, predictions = [], []\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                 attention_masks=batch['attention_mask'],\n",
    "                                 image_features=batch['image'])\n",
    "        label = batch['label']\n",
    "    \n",
    "        loss = criterion(outputs, label.float()) # label.float()\n",
    "\n",
    "        actuals.extend(label.cpu().numpy().astype(int))\n",
    "        predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy().astype(float))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "  \n",
    "    pred = torch.tensor(predictions)\n",
    "    act = torch.tensor(actuals)\n",
    "    accuracy = metric(pred, act)\n",
    "    f1_score = f1(pred,act)\n",
    "\n",
    "    return train_loss/len(train_dataloader), accuracy, f1_score\n",
    "\n",
    "def val_epoch(ensemble_model, val_dataloader):\n",
    "    \n",
    "    actuals, predictions = [], []\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    with torch.no_grad():    \n",
    "    \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            # get the inputs;\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                \n",
    "            # forward + backward + optimize\n",
    "            outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                     attention_masks=batch['attention_mask'],\n",
    "                                     image_features=batch['image'])\n",
    "            label = batch['label']\n",
    "        \n",
    "            cur_val_loss = criterion(outputs, label.float()) # label.float\n",
    "    \n",
    "            actuals.extend(label.cpu().numpy().astype(int))\n",
    "            predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy().astype(float))\n",
    "    \n",
    "            val_loss += cur_val_loss.item()\n",
    "\n",
    "    pred = torch.tensor(predictions)\n",
    "    act = torch.tensor(actuals)\n",
    "    accuracy = metric(pred, act)\n",
    "    f1_score = f1(pred,act)\n",
    "  \n",
    "    return val_loss/len(val_dataloader), accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3c44977-aad2-4a57-9ad5-d60513a9fb2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40ccb69561c46ae9ca12f3c17aa572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:1 / 10,train loss:0.38184,train acc: 0.90261, train f1: 0.05405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c43e64d9474d80888868ffa14f1982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:1 / 10 val loss:0.32424, val acc:0.89878, val f1:0.00342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc38191565614116852dbf55cd40ea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:2 / 10,train loss:0.26192,train acc: 0.90313, train f1: 0.05753\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53109ea3bcc04012b6659ca41f60e216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:2 / 10 val loss:0.31462, val acc:0.89722, val f1:0.00672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbec025e927941a8b1d01d24b2f59c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:3 / 10,train loss:0.23705,train acc: 0.90661, train f1: 0.12255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f520d6afd4bc880685d6e7905f309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:3 / 10 val loss:0.31971, val acc:0.89452, val f1:0.02256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7bcf83972a45fd9815fd01a36b41f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:4 / 10,train loss:0.21708,train acc: 0.91061, train f1: 0.19688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3448b7aedaf14dc2979f58b9f2fb32e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:4 / 10 val loss:0.31947, val acc:0.89513, val f1:0.05039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a7c267846e49848e253252da49ad43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:5 / 10,train loss:0.19308,train acc: 0.91617, train f1: 0.27844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a530402f50c42ec8855fdba3422d271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:5 / 10 val loss:0.32516, val acc:0.89287, val f1:0.05954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f7ebab44404dbe85e542e76777e324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:6 / 10,train loss:0.16592,train acc: 0.92235, train f1: 0.36260\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd5a49a378744428bf16d38a0f7da82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:6 / 10 val loss:0.35322, val acc:0.88809, val f1:0.06672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5de68b9059c450587c8d3c33b9ae6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:7 / 10,train loss:0.13926,train acc: 0.92609, train f1: 0.40725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f93c0a3acd4f1eab08399cbc8f6326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:7 / 10 val loss:0.36222, val acc:0.89000, val f1:0.08796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6f3c5144074501ad9eab54bbe07cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:8 / 10,train loss:0.11799,train acc: 0.92939, train f1: 0.45209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18c7796cd3c42c8ba0c1c4c82252e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:8 / 10 val loss:0.44378, val acc:0.88052, val f1:0.08278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2224ecc8fe0743bc8a231c2f4c17ee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:9 / 10,train loss:0.09709,train acc: 0.93191, train f1: 0.48042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70de1d0de4a403bb14db1bc6a3a1c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:9 / 10 val loss:0.48728, val acc:0.88009, val f1:0.09692\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db40499d9cc4772b3ea8f37806716a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:10 / 10,train loss:0.07298,train acc: 0.93583, train f1: 0.51890\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194c0123940f494081d54eddb26aa2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:10 / 10 val loss:0.47236, val acc:0.88261, val f1:0.10477\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss, train_acc, train_f1 = train_epoch(ensemble_model=ensemble_model_multi, train_dataloader=train_dataloader)\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {n_epochs},train loss:{train_loss:.5f},train acc: {train_acc:.5f}, train f1: {train_f1:.5f}\")\n",
    "    val_loss, val_acc, val_f1 = val_epoch(ensemble_model=ensemble_model_multi, val_dataloader=val_dataloader)\n",
    "\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {n_epochs} val loss:{val_loss:.5f}, val acc:{val_acc:.5f}, val f1:{val_f1:.5f}\")\n",
    "\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "904c4b35-d3e0-456a-9f85-146aae2d849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ensemble_model, 'Bert+VGG16_ensemble_subtask2a')\n",
    "torch.save(ensemble_model.state_dict(),'Bert+VGG16_subtask2a_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2c683-51dc-48bd-9cd8-73f6655ef0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2552e79-f6fe-4b2d-83f6-98f170885906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c05b9a2-02a0-433f-8401-cb3dcf68c6c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Other Models - free to use if you can get it working\n",
    "Better method, but model gets upset with the layers in the final part of the classifier and I can't remember how to fix it.\n",
    "\n",
    "Other layers not included but will add back in\n",
    "\n",
    "- self.bn = nn.BatchNorm1d(value) < batchnorm\n",
    "- self.dropout = nn.Dropout(drop_prob) < dropout\n",
    "- self.classify = nn.Linear(in_features = 512, out_features = num_classes) < linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c23bf377-302f-4631-b030-f6e2cf5e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "\n",
    "args.img_embed_pool_type = \"avg\"\n",
    "args.num_image_embeds = 2\n",
    "args.hidden_size = 768\n",
    "args.img_hidden_size = 4098\n",
    "args.n_classes = len(list(set(labels))) # taken from our label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e403d716-df1b-4f4d-abdf-176bd95ada70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text encoder\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\") # change the pretrained model here\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.out = nn.Linear(args.hidden_size, 512)\n",
    "        self.out2 = nn.Linear(512, args.n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        output = self.out(output)\n",
    "        output = self.out2(output)\n",
    "        return output\n",
    "\n",
    "# image encoder\n",
    "# see https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        model = torchvision.models.vgg16(pretrained=True) # change the pretrained model here, or weights\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "        pool_func = (\n",
    "            nn.AdaptiveAvgPool2d\n",
    "            if args.img_embed_pool_type == \"avg\"\n",
    "            else nn.AdaptiveMaxPool2d\n",
    "        )\n",
    "\n",
    "        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n",
    "            self.pool = pool_func((args.num_image_embeds, 1))\n",
    "        elif args.num_image_embeds == 4:\n",
    "            self.pool = pool_func((2, 2))\n",
    "        elif args.num_image_embeds == 6:\n",
    "            self.pool = pool_func((3, 2))\n",
    "        elif args.num_image_embeds == 8:\n",
    "            self.pool = pool_func((4, 2))\n",
    "        elif args.num_image_embeds == 9:\n",
    "            self.pool = pool_func((3, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
    "        out = self.pool(self.model(x))\n",
    "        out = torch.flatten(out, start_dim=2)\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out  # BxNx2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f3ce52e-d09a-4f57-b033-e2f9312c8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the model outputs\n",
    "\n",
    "class MultimodalConcatBertClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MultimodalConcatBertClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.txtenc = BertEncoder(args)\n",
    "        self.imgenc = ImageEncoder(args)\n",
    "\n",
    "        last_size = args.hidden_size + (args.img_hidden_size * args.num_image_embeds)\n",
    "        self.clf = nn.ModuleList()\n",
    "\n",
    "        self.clf.append(nn.Linear(last_size, args.n_classes))\n",
    "\n",
    "    def forward(self, txt, mask, img):\n",
    "        txt = self.txtenc(txt, mask)\n",
    "        img = self.imgenc(img)   \n",
    "        \n",
    "        out = torch.cat((txt, img), dim=1)\n",
    "        out = self.concat(out)\n",
    "        for layer in self.clf:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45509fdf-9399-4a0c-b551-3d1a1b957c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MultimodalConcatBertClf(\n",
       "  (txtenc): BertEncoder(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (out2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (imgenc): ImageEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (25): ReLU(inplace=True)\n",
       "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (27): ReLU(inplace=True)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (pool): AdaptiveAvgPool2d(output_size=(2, 1))\n",
       "  )\n",
       "  (clf): ModuleList(\n",
       "    (0): Linear(in_features=8964, out_features=2, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultimodalConcatBertClf(args)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a853f97-4e1d-42d5-af27-0b521eb6f4e4",
   "metadata": {},
   "source": [
    "## FusionNet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "82014849-1b82-4d55-887d-0fd725cde98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where text and image model are the same ones defined\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, drop_prob = 0.1):\n",
    "        super(FusionNet, self).__init__()\n",
    "        self.text_model =text_model()\n",
    "        self.image_model = vision_model()\n",
    "        \n",
    "        self.pooler = nn.Linear(in_features=768, out_features=768)\n",
    "        \n",
    "        self.concat = nn.Linear(in_features=768+2048, out_features= 512)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "    \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.classify = nn.Linear(in_features = 512, out_features = num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, text_features, image_features):\n",
    "        text_features = self.text_model(text_features)\n",
    "        image_features = self.image_model(image_features)\n",
    "        \n",
    "        text_features = torch.tanh(self.pooler(text_features))\n",
    "        text_features = self.dropout(text_features)\n",
    "\n",
    "        text_features = self.bn1(text_features)\n",
    "        image_features = self.bn2(image_features)\n",
    "      \n",
    "        fused =  torch.cat((text_features, image_features), dim=1)\n",
    "      \n",
    "        x = self.concat(fused)\n",
    "  \n",
    "        x = F.tanh(self.bn(x))          \n",
    "  \n",
    "        x = F.tanh(self.classify(x)) \n",
    "  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a8be4-28d2-431d-a5e2-961d352f52b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392ac86-781a-426c-bf14-ec771bad1211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac6d55-1142-44c3-bbd1-284dd6a75fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69194ffb-0e61-4dd2-a4ce-56a402d08cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
