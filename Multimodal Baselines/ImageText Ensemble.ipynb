{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1bfdfd-06df-4eea-ba40-040256298d73",
   "metadata": {},
   "source": [
    "A simpler Text + Image Encoder model as a baseline. Uses BERT + VGG16 initiailly, but can be changed to anything else. At present VGG16 has a linear layer, so it's not easy to access the output features in the model. However if the vision model is updated, the code can be changed in the ensemble model structure to accept the output layer of a different model. Note as well the input size would need to be changed in the dataloader.\n",
    "\n",
    "TO DO:\n",
    "- Model selection for NLP and Vision\n",
    "- Dataloader for model selection\n",
    "- Ensemble model for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "053763b1-f276-4148-ade4-b9a2a84bb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    BertForSequenceClassification)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryF1Score, MulticlassF1Score, MultilabelConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75735d-43ae-4e17-a683-35e53aa91b83",
   "metadata": {},
   "source": [
    "# Data\n",
    "Same junk from the VisionTextEncoder notebook, but this time using the binary classification as an experiment.\n",
    "\n",
    "TO DO:\n",
    "- Better data loading, just copying and pasting atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "60722791-9444-4222-9193-0b0b54cb006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\train'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2b\\train.json')\n",
    "df = pd.merge(df, images_df, on='image')\n",
    "df.fillna(' ', inplace=True)\n",
    "\n",
    "path_val = r'X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\val'\n",
    "images_val = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path_val) for f in filenames]\n",
    "images_df_val = pd.DataFrame(images_val, columns=['filepath'])\n",
    "images_df_val['image'] = images_df_val['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df_val = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2b\\val.json')\n",
    "df_val = pd.merge(df_val, images_df_val, on='image')\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['encoded_labels'] = le.fit_transform(df['label']).tolist()\n",
    "df_val['encoded_labels'] = le.fit_transform(df_val['label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c76e38b-14fd-48ed-8eec-9e2a9e534d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists\n",
    "\n",
    "images = [str(i) for i in df['filepath'].values]\n",
    "texts = [str(i) for i in df['text'].astype(str).values.tolist()]\n",
    "labels = df['encoded_labels'].values\n",
    "\n",
    "images_val = [str(i) for i in df_val['filepath'].values]\n",
    "texts_val = [str(i) for i in df_val['text'].astype(str).values.tolist()]\n",
    "labels_val = df_val['encoded_labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae4b69-d9ad-4e15-b14c-e9119e168d43",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3745b1bc-3076-455b-acf0-cf97afd97287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations for the raw images\n",
    "# change size here for different model\n",
    "\n",
    "model_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f3cea54d-79f8-4cbc-bc41-1282fd3a30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img, txt, lbs, tokenizer, n_classes, transform):\n",
    "        self.image = img\n",
    "        self.text = txt\n",
    "        self.labels = lbs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_classes = n_classes\n",
    "        self.transforms = transform  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        image = self.image[idx]\n",
    "\n",
    "        text_encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        sample = {'input_ids': text_encoded['input_ids'],\n",
    "                  'attention_mask': text_encoded['attention_mask'],\n",
    "                  'image': image,\n",
    "                  \"label\": label}\n",
    "        sample = {k:v.squeeze() for k,v in sample.items()}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97c642-7112-4d14-851a-634fcc94b904",
   "metadata": {},
   "source": [
    "PyTorch has a problem stacking multiple inputs, the function below converts the data to a list but the model training has to change for this to work. For now batch_size = 1 allows this to work, or sometimes it works depending on the inputs. The collate_fn custom input is good enough but produces the inputs as a list which are more difficult to work with. See more: https://discuss.pytorch.org/t/making-custom-image-to-image-dataset-using-collate-fn-and-dataloader/55951\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attn_mask = [item['attention_mask'] for item in batch]\n",
    "    img = [item['image'] for item in batch]\n",
    "    target = [item['label'] for item in batch]\n",
    "    return [input_ids, attn_mask, img, target] -->\n",
    "\n",
    "Probably the truncation for the text is the problem, seems to work for now but doesn't always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "456741b4-be91-4894-a4bf-8f04cf3b9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, use_fast=False)\n",
    "train_dataset = VisionTextDataset(img=images, txt=texts, lbs=labels, tokenizer=tokenizer, n_classes=2, transform=model_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7fa3d36b-c6c0-4c08-8fa4-55011af96ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 512])\n",
      "tensor([[  101, 10093, 26576,  ...,     0,     0,     0],\n",
      "        [  101, 13433,  2615,  ...,     0,     0,     0],\n",
      "        [  101,  2268,  1032,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2644,  2206,  ...,     0,     0,     0],\n",
      "        [  101, 12295, 14451,  ...,     0,     0,     0],\n",
      "        [  101,  2025,  2017,  ...,     0,     0,     0]])\n",
      "attention_mask torch.Size([8, 512])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "image torch.Size([8, 3, 224, 224])\n",
      "tensor([[[[0.1216, 0.1333, 0.1176,  ..., 0.1098, 0.1020, 0.2745],\n",
      "          [0.1216, 0.1294, 0.1176,  ..., 0.1098, 0.1020, 0.2745],\n",
      "          [0.1176, 0.1176, 0.1255,  ..., 0.1098, 0.1020, 0.2745],\n",
      "          ...,\n",
      "          [0.1647, 0.1608, 0.1490,  ..., 0.2196, 0.1882, 0.3725],\n",
      "          [0.1569, 0.1725, 0.1529,  ..., 0.2157, 0.1804, 0.3647],\n",
      "          [0.1569, 0.1765, 0.1569,  ..., 0.2039, 0.1843, 0.3569]],\n",
      "\n",
      "         [[0.0902, 0.1020, 0.0980,  ..., 0.1059, 0.0980, 0.2706],\n",
      "          [0.0863, 0.0980, 0.0980,  ..., 0.1059, 0.0980, 0.2706],\n",
      "          [0.0824, 0.0863, 0.1059,  ..., 0.1059, 0.0980, 0.2706],\n",
      "          ...,\n",
      "          [0.1843, 0.1843, 0.1765,  ..., 0.2353, 0.2039, 0.3882],\n",
      "          [0.1804, 0.1961, 0.1804,  ..., 0.2314, 0.2000, 0.3804],\n",
      "          [0.1804, 0.2039, 0.1882,  ..., 0.2235, 0.2039, 0.3765]],\n",
      "\n",
      "         [[0.1725, 0.1843, 0.1686,  ..., 0.1333, 0.1216, 0.2941],\n",
      "          [0.1686, 0.1765, 0.1686,  ..., 0.1333, 0.1216, 0.2941],\n",
      "          [0.1608, 0.1608, 0.1725,  ..., 0.1333, 0.1216, 0.2941],\n",
      "          ...,\n",
      "          [0.2392, 0.2353, 0.2314,  ..., 0.2431, 0.2157, 0.4000],\n",
      "          [0.2275, 0.2471, 0.2353,  ..., 0.2392, 0.2118, 0.3922],\n",
      "          [0.2275, 0.2549, 0.2431,  ..., 0.2353, 0.2157, 0.3882]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.3098, 0.3922, 0.4235,  ..., 0.4196, 0.5137, 0.5059],\n",
      "          [0.4471, 0.4510, 0.4431,  ..., 0.4667, 0.5059, 0.4941],\n",
      "          [0.4627, 0.4667, 0.4549,  ..., 0.4706, 0.4745, 0.4510]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.3843, 0.4431, 0.4510,  ..., 0.4118, 0.5059, 0.4980],\n",
      "          [0.4627, 0.4627, 0.4627,  ..., 0.4667, 0.5020, 0.4863],\n",
      "          [0.4745, 0.4784, 0.4745,  ..., 0.4863, 0.4863, 0.4667]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.4510, 0.4863, 0.4824,  ..., 0.4353, 0.5451, 0.5451],\n",
      "          [0.4902, 0.4824, 0.4824,  ..., 0.5020, 0.5412, 0.5216],\n",
      "          [0.5137, 0.5294, 0.5255,  ..., 0.5569, 0.5569, 0.5176]]],\n",
      "\n",
      "\n",
      "        [[[0.6902, 0.7137, 0.7216,  ..., 0.6039, 0.5922, 0.5843],\n",
      "          [0.6118, 0.6275, 0.6039,  ..., 0.6118, 0.5961, 0.5882],\n",
      "          [0.5137, 0.4902, 0.3765,  ..., 0.6157, 0.6039, 0.5961],\n",
      "          ...,\n",
      "          [0.6706, 0.6510, 0.6353,  ..., 0.4275, 0.4392, 0.4471],\n",
      "          [0.5569, 0.5725, 0.6039,  ..., 0.4275, 0.4471, 0.4549],\n",
      "          [0.3569, 0.3843, 0.4314,  ..., 0.5882, 0.6039, 0.6078]],\n",
      "\n",
      "         [[0.6745, 0.7020, 0.7098,  ..., 0.6039, 0.6000, 0.5961],\n",
      "          [0.6039, 0.6196, 0.6000,  ..., 0.6118, 0.6039, 0.6000],\n",
      "          [0.5137, 0.4941, 0.3804,  ..., 0.6157, 0.6118, 0.6078],\n",
      "          ...,\n",
      "          [0.0549, 0.0667, 0.0627,  ..., 0.8549, 0.8510, 0.8549],\n",
      "          [0.0667, 0.0784, 0.0824,  ..., 0.8431, 0.8314, 0.8353],\n",
      "          [0.0588, 0.0510, 0.0431,  ..., 0.8784, 0.8706, 0.8667]],\n",
      "\n",
      "         [[0.7373, 0.7451, 0.7490,  ..., 0.5882, 0.5804, 0.5765],\n",
      "          [0.6588, 0.6588, 0.6431,  ..., 0.5961, 0.5882, 0.5804],\n",
      "          [0.5804, 0.5490, 0.4471,  ..., 0.6000, 0.5922, 0.5882],\n",
      "          ...,\n",
      "          [0.0784, 0.0745, 0.0667,  ..., 0.4353, 0.4392, 0.4431],\n",
      "          [0.1137, 0.1098, 0.1059,  ..., 0.4471, 0.4471, 0.4510],\n",
      "          [0.0784, 0.0706, 0.0588,  ..., 0.6157, 0.6118, 0.6118]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.5725, 0.7098, 0.7216,  ..., 0.5882, 0.5804, 0.5843],\n",
      "          [0.5765, 0.7098, 0.7255,  ..., 0.5922, 0.5882, 0.5882],\n",
      "          [0.5843, 0.7137, 0.7333,  ..., 0.5961, 0.5882, 0.5922],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.2078, 0.1725, 0.1725],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.1804, 0.1804, 0.1804],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.1686, 0.1647, 0.1608]],\n",
      "\n",
      "         [[0.4353, 0.5412, 0.5333,  ..., 0.4745, 0.4667, 0.4706],\n",
      "          [0.4392, 0.5451, 0.5373,  ..., 0.4784, 0.4745, 0.4745],\n",
      "          [0.4471, 0.5451, 0.5451,  ..., 0.4824, 0.4745, 0.4784],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.4706, 0.4314, 0.4314],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.4392, 0.4392, 0.4392],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 0.4275, 0.4235, 0.4196]],\n",
      "\n",
      "         [[0.2588, 0.3451, 0.3255,  ..., 0.0039, 0.0000, 0.0000],\n",
      "          [0.2627, 0.3490, 0.3294,  ..., 0.0078, 0.0039, 0.0039],\n",
      "          [0.2706, 0.3490, 0.3333,  ..., 0.0118, 0.0039, 0.0078],\n",
      "          ...,\n",
      "          [0.9922, 0.9922, 0.9922,  ..., 0.2235, 0.1882, 0.1882],\n",
      "          [0.9922, 0.9922, 0.9922,  ..., 0.1961, 0.1961, 0.1961],\n",
      "          [0.9922, 0.9922, 0.9922,  ..., 0.1843, 0.1804, 0.1765]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9882, 0.9843, 0.9804,  ..., 0.5922, 0.5725, 0.5725],\n",
      "          [0.9804, 0.9765, 0.9725,  ..., 0.0863, 0.0706, 0.0667],\n",
      "          [0.9765, 0.9725, 0.9725,  ..., 0.0824, 0.0863, 0.0706],\n",
      "          ...,\n",
      "          [0.1255, 0.1294, 0.1451,  ..., 0.0824, 0.0863, 0.0941],\n",
      "          [0.1020, 0.1059, 0.1216,  ..., 0.0902, 0.0824, 0.0863],\n",
      "          [0.1020, 0.0980, 0.1176,  ..., 0.0863, 0.0863, 0.0745]],\n",
      "\n",
      "         [[0.9882, 0.9922, 0.9922,  ..., 0.6235, 0.6196, 0.6196],\n",
      "          [0.9804, 0.9843, 0.9843,  ..., 0.1804, 0.1804, 0.1765],\n",
      "          [0.9765, 0.9804, 0.9804,  ..., 0.2000, 0.1882, 0.1765],\n",
      "          ...,\n",
      "          [0.0667, 0.0706, 0.0902,  ..., 0.1137, 0.1059, 0.0980],\n",
      "          [0.0471, 0.0510, 0.0667,  ..., 0.1059, 0.0941, 0.0902],\n",
      "          [0.0471, 0.0431, 0.0627,  ..., 0.0902, 0.0784, 0.0667]],\n",
      "\n",
      "         [[0.9882, 0.9882, 0.9882,  ..., 0.6314, 0.6235, 0.6235],\n",
      "          [0.9804, 0.9804, 0.9804,  ..., 0.1961, 0.1922, 0.1843],\n",
      "          [0.9765, 0.9765, 0.9765,  ..., 0.2235, 0.2157, 0.1961],\n",
      "          ...,\n",
      "          [0.0667, 0.0745, 0.0902,  ..., 0.0863, 0.0745, 0.0706],\n",
      "          [0.0471, 0.0510, 0.0667,  ..., 0.0745, 0.0549, 0.0510],\n",
      "          [0.0471, 0.0431, 0.0627,  ..., 0.0627, 0.0431, 0.0275]]]])\n",
      "label torch.Size([8])\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size())\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6dadee9e-a354-4f68-ae27-56674cf01788",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VisionTextDataset(img=images_val, txt=texts_val, lbs=labels_val, tokenizer=tokenizer, n_classes=2, transform=model_transforms)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb81d83-a78e-4d63-b925-58f9b8923c4a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8510574d-1fe6-4a8e-ae87-9c6d22d7070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "vision_model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "#num_ftrs = vision_model.classifier[0].in_features\n",
    "#vision_model.classifier[0] = nn.Linear(num_ftrs, args.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "34254648-0ec6-4eac-8856-ce8f443083e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, image_model, text_model, num_classes):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.text_model = text_model\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            1000 +                             # this is hardcoded to 1000 for VGG16, as last layer is linear\n",
    "            text_model.config.hidden_size,     # usual method is something like model.fc.out_features\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features, text_features, attention_masks):\n",
    "        img_feat = self.image_model(image_features)\n",
    "        txt_feat_bert = self.text_model(text_features, attention_mask=attention_masks).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        combined_feat = torch.cat((img_feat, txt_feat_bert), dim=1)\n",
    "        return self.fc(combined_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cacf92b2-0b08-4e8e-b1a1-a02e47967328",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = EnsembleModel(vision_model, text_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ec602-c572-4404-bdd8-00b550c84870",
   "metadata": {},
   "source": [
    "# Simple Model (Binary Classification - Subtask2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7f26d-3fc7-4543-b93e-1b03e9217458",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd4a5931-357f-465e-9cc8-ae4462d8a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12a17633f024e95875c1ac3d3532151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2545\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de52936724d143b2829c19bbcbe548ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.0064\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592dfe212c694ac5b6555005a50ca020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.0014\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b275542d844601810875cada16e0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.0019\n",
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7891de7e29a7423bb95f903ff3819a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensemble_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.Adam(ensemble_model.parameters(), lr=learning_rate)\n",
    "n_epochs = 5\n",
    "\n",
    "actual, prediction = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_losses = []\n",
    "    ensemble_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                 attention_masks=batch['attention_mask'],\n",
    "                       image_features=batch['image'])\n",
    "        label = batch['label']\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b5598-af0b-48db-b56a-552f22d00f1a",
   "metadata": {},
   "source": [
    "## Training and Val Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "12d48e0b-9efa-4e24-abfe-497a8151a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.Adam(ensemble_model.parameters(), lr=learning_rate)\n",
    "metric = BinaryAccuracy()\n",
    "f1 = BinaryF1Score()\n",
    "\n",
    "def train_epoch(ensemble_model, train_dataloader):\n",
    "    actuals, predictions = [], []\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                 attention_masks=batch['attention_mask'],\n",
    "                                 image_features=batch['image'])\n",
    "        label = batch['label']\n",
    "    \n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        actuals.extend(label.cpu().numpy().astype(int))\n",
    "        predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predicted_labels = predictions.argmax(1)\n",
    "    accuracy = (predicted_labels == actuals).mean()\n",
    "    \n",
    "    f1_score = f1(torch.tensor(predicted_labels), torch.tensor(actuals))\n",
    "\n",
    "    return train_loss/len(train_dataloader), accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a05daa42-512a-4748-b511-ee48cf700c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(ensemble_model, val_dataloader):\n",
    "    \n",
    "    actuals, predictions = [], []\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    with torch.no_grad():    \n",
    "    \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            # get the inputs;\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                \n",
    "            # forward + backward + optimize\n",
    "            outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                     attention_masks=batch['attention_mask'],\n",
    "                                     image_features=batch['image'])\n",
    "            label = batch['label']\n",
    "        \n",
    "            cur_val_loss = criterion(outputs, label)\n",
    "    \n",
    "            actuals.extend(label.cpu().numpy().astype(int))\n",
    "            predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy())\n",
    "    \n",
    "            val_loss += cur_val_loss.item()\n",
    "            \n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predicted_labels = predictions.argmax(1)\n",
    "    accuracy = (predicted_labels == actuals).mean()\n",
    "    \n",
    "    f1_score = f1(torch.tensor(predicted_labels), torch.tensor(actuals))\n",
    "\n",
    "    return val_loss/len(val_dataloader), accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bdfe9174-53df-4679-aae5-1ac4ff98384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d942a1bd-9894-45c2-830b-dc6d6345815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14e7b603ff14e4dbea4ef22f38d5751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[189], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m----> 3\u001b[0m     train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensemble_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Epoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,train loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train f1:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     val_loss, val_acc, val_f1 \u001b[38;5;241m=\u001b[39m val_epoch(ensemble_model\u001b[38;5;241m=\u001b[39mensemble_model, val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader)\n",
      "Cell \u001b[1;32mIn[150], line 35\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(ensemble_model, train_dataloader)\u001b[0m\n\u001b[0;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     37\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predictions)\n\u001b[0;32m     38\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc, train_f1 = train_epoch(ensemble_model=ensemble_model, train_dataloader=train_dataloader)\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {NUM_EPOCHS},train loss:{train_loss:.5f}, train acc: {train_acc:.5f}, train f1:{train_f1}\")\n",
    "    val_loss, val_acc, val_f1 = val_epoch(ensemble_model=ensemble_model, val_dataloader=val_dataloader)\n",
    "\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {NUM_EPOCHS} Val loss:{val_loss:.5f}, Val acc:{val_acc:.5f}, Val f1:{val_f1:.5f}\")\n",
    "\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "45a38981-c7bf-47dd-a06f-7c03031d9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = f'Bert+VGG16_ensemble_{num_epochs}'\n",
    "save_path = 'path here'\n",
    "\n",
    "torch.save(ensemble_model, os.path.join(save_path, model_params+'.pth'))\n",
    "torch.save(ensemble_model.state_dict(), os.path.join(save_path, model_params+'_weights.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcc559-06fd-4664-a6bd-9e0698186489",
   "metadata": {},
   "source": [
    "## Model Structure for Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a868d-f392-4b33-a36a-cf64f9f1dd9c",
   "metadata": {},
   "source": [
    "TO DO\n",
    "Rewrite the dataloader and ensemble model to accept different models, similiar to Unimodal baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33679a71-6b76-4b02-a358-7a6712c91a4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Simple Model - Multilabel Classification (Subtask2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42d977-6f1e-4ce5-8a0d-c5ab9970a85e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data\n",
    "NOTE: There is ONE instance in the dataset where no technique is recorded. This is in the training data: prop_meme_24430.png. For now I am going to reclassify it as No Technique but have some concerns about how underrepresented this class is throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d884ca-7dd2-4cc1-86fd-dc0fbc5c4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2a_images\\train_images\\train_images'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2a\\train.json')\n",
    "df = pd.merge(df, images_df, on='image')\n",
    "\n",
    "# reclassify the no technique\n",
    "index = df[df['labels'].apply(lambda x: len(x)) == 0].index.values.astype(int)[0]\n",
    "df.loc[index, 'labels']=['No Technique']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5595b328-ba14-4579-9b88-ef5c8a229d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['labels'].values\n",
    "unique_labels = []\n",
    "\n",
    "for i in labels:\n",
    "    for x in i:\n",
    "        unique_labels.append(x)\n",
    "unique_labels = list(set(unique_labels))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "# try a one-hot as well\n",
    "ml = MultiLabelBinarizer()\n",
    "\n",
    "df['one_hot_labels'] = ml.fit_transform(df['labels'])[:,1:].tolist()\n",
    "df['encoded_labels'] = df['labels'].apply(le.transform)\n",
    "\n",
    "images = [str(i) for i in df['filepath'].values]\n",
    "texts = [str(i) for i in df['text'].astype(str).values.tolist()]\n",
    "labels = df['encoded_labels'].values\n",
    "one_hot_labels = df['one_hot_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9842c6-1ed3-411f-be04-b5b6b098e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2a_images\\validation_images\\validation_images'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df_val = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2a\\validation.json')\n",
    "df_val = pd.merge(df_val, images_df, on='image')\n",
    "\n",
    "# reclassify the no technique - there isn't one in val\n",
    "# index = df_val[df_val['labels'].apply(lambda x: len(x)) == 0].index.values.astype(int)[0]\n",
    "# df_val.loc[index, 'labels']=['No Technique']\n",
    "\n",
    "val_labels = df_val['labels'].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "# try a one-hot as well\n",
    "ml = MultiLabelBinarizer()\n",
    "\n",
    "df_val['one_hot_labels'] = ml.fit_transform(df_val['labels'])[:,1:].tolist()\n",
    "df_val['encoded_labels'] = df_val['labels'].apply(le.transform)\n",
    "\n",
    "images_val = [str(i) for i in df_val['filepath'].values]\n",
    "texts_val = [str(i) for i in df_val['text'].astype(str).values.tolist()]\n",
    "labels_val =df_val['encoded_labels'].values\n",
    "one_hot_labels_val = df_val['one_hot_labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cfd585-21ce-418c-9e95-6e7f7c8573ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels need to be padded\n",
    "# unless onehot is used\n",
    "\n",
    "model_transforms = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "\n",
    "class VisionTextDataset_Multi(torch.utils.data.Dataset):\n",
    "    def __init__(self, img, txt, lbs, tokenizer, n_classes, transform):\n",
    "        self.image = img\n",
    "        self.text = txt\n",
    "        self.labels = lbs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_classes = n_classes\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        image = self.image[idx]        \n",
    "            \n",
    "        text_encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        \n",
    "        def pad_tensor(t):\n",
    "            t = torch.tensor(t)\n",
    "            padding = self.n_classes - t.size()[0]\n",
    "            t = torch.nn.functional.pad(t,(0,padding))\n",
    "            return t\n",
    "        \n",
    "        \n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        label = pad_tensor((self.labels[idx]))\n",
    "\n",
    "        sample = {'input_ids': text_encoded['input_ids'],\n",
    "                  'attention_mask': text_encoded['attention_mask'],\n",
    "                  'image': image,\n",
    "                  \"label\": label}\n",
    "        sample = {k:v.squeeze() for k,v in sample.items()}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33db7f34-1d0a-421c-a432-3db9a7f88b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([16, 512]) torch.int64\n",
      "attention_mask torch.Size([16, 512]) torch.int64\n",
      "image torch.Size([16, 3, 224, 224]) torch.float32\n",
      "label torch.Size([16, 23]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True, use_fast=False)\n",
    "train_dataset = VisionTextDataset_Multi(img=images, txt=texts, lbs=one_hot_labels, tokenizer=tokenizer, n_classes=23, transform=model_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size(), v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d5076d-12bf-431e-bf33-fdb45214a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VisionTextDataset_Multi(img=images_val, txt=texts_val, lbs=one_hot_labels_val, tokenizer=tokenizer, n_classes=23, transform=model_transforms)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6064d5-29cf-4384-9dbd-f4b69c81157e",
   "metadata": {},
   "source": [
    "## Training\n",
    "Just reuse the same ensemble, but BCE with logits instead for one hot, and change label to label.float()\n",
    "\n",
    "TO DO: Accuracy. Should be calculating when each 1 is matched and then average over the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec7caf4-f3ae-41e2-ba49-6ee7b1cdf068",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model_multi = EnsembleModel(vision_model, text_model, 23)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.Adam(ensemble_model.parameters(), lr=learning_rate)\n",
    "metric = BinaryAccuracy()\n",
    "f1 = BinaryF1Score()\n",
    "\n",
    "def train_epoch(ensemble_model, train_dataloader):\n",
    "    \n",
    "    actuals, predictions = [], []\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.train()\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            \n",
    "        # forward + backward + optimize\n",
    "        outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                 attention_masks=batch['attention_mask'],\n",
    "                                 image_features=batch['image'])\n",
    "        label = batch['label']\n",
    "    \n",
    "        loss = criterion(outputs, label.float()) # label.float()\n",
    "\n",
    "        actuals.extend(label.cpu().numpy().astype(int))\n",
    "        predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy().astype(float))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "  \n",
    "    pred = torch.tensor(predictions)\n",
    "    act = torch.tensor(actuals)\n",
    "    accuracy = metric(pred, act)\n",
    "    f1_score = f1(pred,act)\n",
    "\n",
    "    return train_loss/len(train_dataloader), accuracy, f1_score\n",
    "\n",
    "def val_epoch(ensemble_model, val_dataloader):\n",
    "    \n",
    "    actuals, predictions = [], []\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    with torch.no_grad():    \n",
    "    \n",
    "        for batch in tqdm(val_dataloader):\n",
    "            # get the inputs;\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                \n",
    "            # forward + backward + optimize\n",
    "            outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                     attention_masks=batch['attention_mask'],\n",
    "                                     image_features=batch['image'])\n",
    "            label = batch['label']\n",
    "        \n",
    "            cur_val_loss = criterion(outputs, label.float()) # label.float\n",
    "    \n",
    "            actuals.extend(label.cpu().numpy().astype(int))\n",
    "            predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy().astype(float))\n",
    "    \n",
    "            val_loss += cur_val_loss.item()\n",
    "\n",
    "    pred = torch.tensor(predictions)\n",
    "    act = torch.tensor(actuals)\n",
    "    accuracy = metric(pred, act)\n",
    "    f1_score = f1(pred,act)\n",
    "  \n",
    "    return val_loss/len(val_dataloader), accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3c44977-aad2-4a57-9ad5-d60513a9fb2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40ccb69561c46ae9ca12f3c17aa572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:1 / 10,train loss:0.38184,train acc: 0.90261, train f1: 0.05405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c43e64d9474d80888868ffa14f1982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:1 / 10 val loss:0.32424, val acc:0.89878, val f1:0.00342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc38191565614116852dbf55cd40ea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:2 / 10,train loss:0.26192,train acc: 0.90313, train f1: 0.05753\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53109ea3bcc04012b6659ca41f60e216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:2 / 10 val loss:0.31462, val acc:0.89722, val f1:0.00672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbec025e927941a8b1d01d24b2f59c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:3 / 10,train loss:0.23705,train acc: 0.90661, train f1: 0.12255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f520d6afd4bc880685d6e7905f309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:3 / 10 val loss:0.31971, val acc:0.89452, val f1:0.02256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7bcf83972a45fd9815fd01a36b41f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:4 / 10,train loss:0.21708,train acc: 0.91061, train f1: 0.19688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3448b7aedaf14dc2979f58b9f2fb32e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:4 / 10 val loss:0.31947, val acc:0.89513, val f1:0.05039\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a7c267846e49848e253252da49ad43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:5 / 10,train loss:0.19308,train acc: 0.91617, train f1: 0.27844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a530402f50c42ec8855fdba3422d271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:5 / 10 val loss:0.32516, val acc:0.89287, val f1:0.05954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f7ebab44404dbe85e542e76777e324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:6 / 10,train loss:0.16592,train acc: 0.92235, train f1: 0.36260\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd5a49a378744428bf16d38a0f7da82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:6 / 10 val loss:0.35322, val acc:0.88809, val f1:0.06672\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5de68b9059c450587c8d3c33b9ae6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:7 / 10,train loss:0.13926,train acc: 0.92609, train f1: 0.40725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f93c0a3acd4f1eab08399cbc8f6326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:7 / 10 val loss:0.36222, val acc:0.89000, val f1:0.08796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6f3c5144074501ad9eab54bbe07cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:8 / 10,train loss:0.11799,train acc: 0.92939, train f1: 0.45209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18c7796cd3c42c8ba0c1c4c82252e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:8 / 10 val loss:0.44378, val acc:0.88052, val f1:0.08278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2224ecc8fe0743bc8a231c2f4c17ee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:9 / 10,train loss:0.09709,train acc: 0.93191, train f1: 0.48042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70de1d0de4a403bb14db1bc6a3a1c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:9 / 10 val loss:0.48728, val acc:0.88009, val f1:0.09692\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db40499d9cc4772b3ea8f37806716a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:10 / 10,train loss:0.07298,train acc: 0.93583, train f1: 0.51890\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194c0123940f494081d54eddb26aa2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:10 / 10 val loss:0.47236, val acc:0.88261, val f1:0.10477\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss, train_acc, train_f1 = train_epoch(ensemble_model=ensemble_model_multi, train_dataloader=train_dataloader)\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {n_epochs},train loss:{train_loss:.5f},train acc: {train_acc:.5f}, train f1: {train_f1:.5f}\")\n",
    "    val_loss, val_acc, val_f1 = val_epoch(ensemble_model=ensemble_model_multi, val_dataloader=val_dataloader)\n",
    "\n",
    "    print(f\"\\n Epoch:{epoch + 1} / {n_epochs} val loss:{val_loss:.5f}, val acc:{val_acc:.5f}, val f1:{val_f1:.5f}\")\n",
    "\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "904c4b35-d3e0-456a-9f85-146aae2d849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ensemble_model, 'Bert+VGG16_ensemble_subtask2a')\n",
    "torch.save(ensemble_model.state_dict(),'Bert+VGG16_subtask2a_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72559311-6eb3-4b80-ab5f-f33986ea9185",
   "metadata": {},
   "source": [
    "# Evaluation (Val Set) Subtask2b (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1f5176f5-b25f-4e40-8ef4-504b0b3757e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the loader to return the ID of the image as well\n",
    "\n",
    "class VisionTextDataset_eval(torch.utils.data.Dataset):\n",
    "    def __init__(self, img, txt, lbs, tokenizer, n_classes, transform, id):\n",
    "        self.image = img\n",
    "        self.text = txt\n",
    "        self.labels = lbs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_classes = n_classes\n",
    "        self.transforms = transform\n",
    "        self.id = id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        image = self.image[idx]\n",
    "        id = self.id[idx]\n",
    "\n",
    "        text_encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        image = Image.open(image).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        # we return the ID as well now\n",
    "\n",
    "        sample = {'input_ids': text_encoded['input_ids'],\n",
    "                  'attention_mask': text_encoded['attention_mask'],\n",
    "                  'image': image,\n",
    "                  \"label\": label,\n",
    "                 'id': id} # return the ID here\n",
    "        sample = {k:v.squeeze() for k,v in sample.items()}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "add073c2-5269-43e6-9302-ed0d0ce4c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_val = [str(i) for i in df_val['filepath'].values]\n",
    "texts_val = [str(i) for i in df_val['text'].astype(str).values.tolist()]\n",
    "labels_val = df_val['encoded_labels'].values\n",
    "id = df_val['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e9a6fe88-2048-4739-abb8-d832df33c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 512])\n",
      "attention_mask torch.Size([1, 512])\n",
      "image torch.Size([1, 3, 224, 224])\n",
      "label torch.Size([1])\n",
      "id torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = VisionTextDataset_eval(img=images_val, txt=texts_val, lbs=labels_val, tokenizer=tokenizer, n_classes=2, transform=model_transforms, id=id)\n",
    "eval_data = DataLoader(eval_dataset, shuffle=False)\n",
    "batch = next(iter(eval_data))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4fae3317-2947-4997-855d-0d99fc66db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = EnsembleModel(vision_model, text_model, 2) # subtask2b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "397cf44a-319d-44aa-9756-bc7da7252797",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = []\n",
    "\n",
    "def eval(ensemble_model, path, data):\n",
    "    actuals, predictions = [], []\n",
    "\n",
    "    ensemble_model = EnsembleModel(vision_model, text_model, 2)\n",
    "    ensemble_model.load_state_dict(torch.load(path))\n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data):\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                     attention_masks=batch['attention_mask'],\n",
    "                                     image_features=batch['image'])\n",
    "            label = batch['label']\n",
    "            image_id = batch['id']\n",
    "        \n",
    "    \n",
    "            actuals.extend(label.cpu().numpy().astype(int))\n",
    "            predictions.extend(F.softmax(outputs, 1).cpu().detach().numpy())\n",
    "            \n",
    "            if int(F.softmax(outputs, 1).cpu().detach().numpy().argmax(1)) == 0:\n",
    "                pred = 'non_propagandistic'\n",
    "            else:\n",
    "                pred = 'propagandistic'            \n",
    "            \n",
    "            prediction_output.append({'id': str(int(image_id.cpu())),\n",
    "                                           'label': pred})\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predicted_labels = predictions.argmax(1)\n",
    "    accuracy = (predicted_labels == actuals).mean()\n",
    "\n",
    "    f1_score = f1(torch.tensor(predicted_labels), torch.tensor(actuals))\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "11a82aa5-73ee-42c3-a985-3fcc74bf9af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad35e5eb1e5448b0893c8c9a9cd022e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'X:\\\\PhD\\\\SemEval Task4\\\\Code\\\\Mock Coding\\\\multimodal-baselines\\\\Bert_VGG16_ensemble_8batch_epochs8_weights.pth'\n",
    "pred = eval(ensemble_model, path, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "935863dc-5911-4e37-9d8c-24fda26ade86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('X:\\\\PhD\\SemEval Task4\\\\Code\\\\Evaluation Code\\\\Subtask2b\\\\predictions_ensemble_v5.txt', 'w') as f:\n",
    "    f.write(json.dumps(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "82509b43-28a0-44eb-94b5-6b27d29de047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '46916', 'label': 'propagandistic'},\n",
       " {'id': '35775', 'label': 'propagandistic'},\n",
       " {'id': '44838', 'label': 'propagandistic'},\n",
       " {'id': '41835', 'label': 'propagandistic'},\n",
       " {'id': '46179', 'label': 'propagandistic'},\n",
       " {'id': '33035', 'label': 'propagandistic'},\n",
       " {'id': '30259', 'label': 'propagandistic'},\n",
       " {'id': '23569', 'label': 'non_propagandistic'},\n",
       " {'id': '15206', 'label': 'propagandistic'},\n",
       " {'id': '41803', 'label': 'propagandistic'},\n",
       " {'id': '45222', 'label': 'non_propagandistic'},\n",
       " {'id': '23263', 'label': 'propagandistic'},\n",
       " {'id': '28757', 'label': 'propagandistic'},\n",
       " {'id': '30193', 'label': 'propagandistic'},\n",
       " {'id': '44993', 'label': 'propagandistic'},\n",
       " {'id': '28534', 'label': 'propagandistic'},\n",
       " {'id': '56093', 'label': 'non_propagandistic'},\n",
       " {'id': '35044', 'label': 'propagandistic'},\n",
       " {'id': '42044', 'label': 'non_propagandistic'},\n",
       " {'id': '45095', 'label': 'non_propagandistic'},\n",
       " {'id': '35861', 'label': 'propagandistic'},\n",
       " {'id': '42887', 'label': 'propagandistic'},\n",
       " {'id': '24979', 'label': 'propagandistic'},\n",
       " {'id': '32543', 'label': 'propagandistic'},\n",
       " {'id': '56152', 'label': 'non_propagandistic'},\n",
       " {'id': '32102', 'label': 'propagandistic'},\n",
       " {'id': '43439', 'label': 'propagandistic'},\n",
       " {'id': '29244', 'label': 'propagandistic'},\n",
       " {'id': '30169', 'label': 'propagandistic'},\n",
       " {'id': '43042', 'label': 'propagandistic'},\n",
       " {'id': '31430', 'label': 'propagandistic'},\n",
       " {'id': '36093', 'label': 'propagandistic'},\n",
       " {'id': '12563', 'label': 'propagandistic'},\n",
       " {'id': '56046', 'label': 'non_propagandistic'},\n",
       " {'id': '35718', 'label': 'propagandistic'},\n",
       " {'id': '32038', 'label': 'propagandistic'},\n",
       " {'id': '26021', 'label': 'propagandistic'},\n",
       " {'id': '45056', 'label': 'non_propagandistic'},\n",
       " {'id': '45260', 'label': 'non_propagandistic'},\n",
       " {'id': '30377', 'label': 'propagandistic'},\n",
       " {'id': '31618', 'label': 'propagandistic'},\n",
       " {'id': '47220', 'label': 'propagandistic'},\n",
       " {'id': '56066', 'label': 'non_propagandistic'},\n",
       " {'id': '44910', 'label': 'non_propagandistic'},\n",
       " {'id': '56048', 'label': 'non_propagandistic'},\n",
       " {'id': '44201', 'label': 'propagandistic'},\n",
       " {'id': '47069', 'label': 'non_propagandistic'},\n",
       " {'id': '45083', 'label': 'non_propagandistic'},\n",
       " {'id': '12727', 'label': 'propagandistic'},\n",
       " {'id': '44169', 'label': 'propagandistic'},\n",
       " {'id': '35535', 'label': 'propagandistic'},\n",
       " {'id': '29099', 'label': 'propagandistic'},\n",
       " {'id': '31249', 'label': 'propagandistic'},\n",
       " {'id': '56016', 'label': 'non_propagandistic'},\n",
       " {'id': '41982', 'label': 'propagandistic'},\n",
       " {'id': '12884', 'label': 'propagandistic'},\n",
       " {'id': '45618', 'label': 'non_propagandistic'},\n",
       " {'id': '46899', 'label': 'non_propagandistic'},\n",
       " {'id': '56145', 'label': 'non_propagandistic'},\n",
       " {'id': '46902', 'label': 'non_propagandistic'},\n",
       " {'id': '46330', 'label': 'propagandistic'},\n",
       " {'id': '43681', 'label': 'propagandistic'},\n",
       " {'id': '31541', 'label': 'propagandistic'},\n",
       " {'id': '46097', 'label': 'non_propagandistic'},\n",
       " {'id': '31543', 'label': 'propagandistic'},\n",
       " {'id': '45308', 'label': 'propagandistic'},\n",
       " {'id': '47087', 'label': 'propagandistic'},\n",
       " {'id': '56082', 'label': 'non_propagandistic'},\n",
       " {'id': '48161', 'label': 'propagandistic'},\n",
       " {'id': '44685', 'label': 'propagandistic'},\n",
       " {'id': '45306', 'label': 'propagandistic'},\n",
       " {'id': '35018', 'label': 'propagandistic'},\n",
       " {'id': '41410', 'label': 'propagandistic'},\n",
       " {'id': '45630', 'label': 'non_propagandistic'},\n",
       " {'id': '29703', 'label': 'non_propagandistic'},\n",
       " {'id': '47897', 'label': 'propagandistic'},\n",
       " {'id': '28635', 'label': 'propagandistic'},\n",
       " {'id': '29705', 'label': 'non_propagandistic'},\n",
       " {'id': '42546', 'label': 'propagandistic'},\n",
       " {'id': '23679', 'label': 'non_propagandistic'},\n",
       " {'id': '33584', 'label': 'propagandistic'},\n",
       " {'id': '46172', 'label': 'propagandistic'},\n",
       " {'id': '56149', 'label': 'non_propagandistic'},\n",
       " {'id': '41628', 'label': 'propagandistic'},\n",
       " {'id': '26007', 'label': 'propagandistic'},\n",
       " {'id': '30866', 'label': 'propagandistic'},\n",
       " {'id': '29754', 'label': 'propagandistic'},\n",
       " {'id': '15321', 'label': 'propagandistic'},\n",
       " {'id': '24896', 'label': 'propagandistic'},\n",
       " {'id': '56147', 'label': 'non_propagandistic'},\n",
       " {'id': '24166', 'label': 'propagandistic'},\n",
       " {'id': '49350', 'label': 'propagandistic'},\n",
       " {'id': '32260', 'label': 'propagandistic'},\n",
       " {'id': '31000', 'label': 'propagandistic'},\n",
       " {'id': '46951', 'label': 'propagandistic'},\n",
       " {'id': '45161', 'label': 'propagandistic'},\n",
       " {'id': '24809', 'label': 'propagandistic'},\n",
       " {'id': '23490', 'label': 'propagandistic'},\n",
       " {'id': '46693', 'label': 'propagandistic'},\n",
       " {'id': '45019', 'label': 'propagandistic'},\n",
       " {'id': '56154', 'label': 'non_propagandistic'},\n",
       " {'id': '56062', 'label': 'non_propagandistic'},\n",
       " {'id': '56169', 'label': 'non_propagandistic'},\n",
       " {'id': '56187', 'label': 'non_propagandistic'},\n",
       " {'id': '24054', 'label': 'propagandistic'},\n",
       " {'id': '33345', 'label': 'propagandistic'},\n",
       " {'id': '56113', 'label': 'propagandistic'},\n",
       " {'id': '34212', 'label': 'propagandistic'},\n",
       " {'id': '33322', 'label': 'propagandistic'},\n",
       " {'id': '45674', 'label': 'non_propagandistic'},\n",
       " {'id': '43029', 'label': 'propagandistic'},\n",
       " {'id': '45473', 'label': 'propagandistic'},\n",
       " {'id': '30865', 'label': 'propagandistic'},\n",
       " {'id': '45592', 'label': 'propagandistic'},\n",
       " {'id': '44786', 'label': 'propagandistic'},\n",
       " {'id': '51094', 'label': 'non_propagandistic'},\n",
       " {'id': '46252', 'label': 'propagandistic'},\n",
       " {'id': '30529', 'label': 'propagandistic'},\n",
       " {'id': '45320', 'label': 'propagandistic'},\n",
       " {'id': '25611', 'label': 'propagandistic'},\n",
       " {'id': '30791', 'label': 'propagandistic'},\n",
       " {'id': '33505', 'label': 'propagandistic'},\n",
       " {'id': '32306', 'label': 'non_propagandistic'},\n",
       " {'id': '35558', 'label': 'propagandistic'},\n",
       " {'id': '35893', 'label': 'propagandistic'},\n",
       " {'id': '45704', 'label': 'propagandistic'},\n",
       " {'id': '42078', 'label': 'propagandistic'},\n",
       " {'id': '45454', 'label': 'non_propagandistic'},\n",
       " {'id': '56170', 'label': 'non_propagandistic'},\n",
       " {'id': '48801', 'label': 'propagandistic'},\n",
       " {'id': '36397', 'label': 'propagandistic'},\n",
       " {'id': '34847', 'label': 'propagandistic'},\n",
       " {'id': '27927', 'label': 'propagandistic'},\n",
       " {'id': '25475', 'label': 'propagandistic'},\n",
       " {'id': '45653', 'label': 'non_propagandistic'},\n",
       " {'id': '46765', 'label': 'propagandistic'},\n",
       " {'id': '45234', 'label': 'propagandistic'},\n",
       " {'id': '34217', 'label': 'propagandistic'},\n",
       " {'id': '44443', 'label': 'propagandistic'},\n",
       " {'id': '15392', 'label': 'propagandistic'},\n",
       " {'id': '25657', 'label': 'propagandistic'},\n",
       " {'id': '44541', 'label': 'propagandistic'},\n",
       " {'id': '34421', 'label': 'propagandistic'},\n",
       " {'id': '32511', 'label': 'propagandistic'},\n",
       " {'id': '44663', 'label': 'propagandistic'},\n",
       " {'id': '44900', 'label': 'propagandistic'},\n",
       " {'id': '12635', 'label': 'propagandistic'},\n",
       " {'id': '12740', 'label': 'propagandistic'},\n",
       " {'id': '46086', 'label': 'propagandistic'},\n",
       " {'id': '30279', 'label': 'propagandistic'}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b7814-91c5-45e8-aebb-19940865e6b9",
   "metadata": {},
   "source": [
    "# Evaluation (Dev Set) Subtask2b (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8c5cfb16-948a-4a57-bc5e-7adb1a744126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>image</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28493</td>\n",
       "      <td>PUTIN SCROLLING HIS INSTA FOR G20 NEWS BE LIKE...</td>\n",
       "      <td>prop_meme_12066.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12336</td>\n",
       "      <td>President Trump has proven\\nthat a business ma...</td>\n",
       "      <td>prop_meme_342.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24720</td>\n",
       "      <td>THE NOT A PRAYINGMAN, BUTFIRE WOULD\\nBE ON MY ...</td>\n",
       "      <td>prop_meme_3342.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23847</td>\n",
       "      <td>WHAT IF..\\nWE WERE HONEST WITH OUR K\\nGOVERNME...</td>\n",
       "      <td>prop_meme_4102.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35217</td>\n",
       "      <td>BANA\\nwwww\\nCUBA, 2016\\nVENEZUELA, 2019\\nFANTA...</td>\n",
       "      <td>prop_meme_10986.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>56136</td>\n",
       "      <td>J.Lo at 52:\\nMe at 35:\\nfb.com/DunderMifflin Meme</td>\n",
       "      <td>prop_meme_24949.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>12383</td>\n",
       "      <td>AM I THE BEST\\nPRESIDENT EVER?\\nYES OR NO?\\n</td>\n",
       "      <td>prop_meme_389.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>46093</td>\n",
       "      <td>Thank You, God\\nFor Giving Us Donald Trump!</td>\n",
       "      <td>prop_meme_18782.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>32814</td>\n",
       "      <td>Andrew Torba t @a\\n8h\\nNew York Times reporter...</td>\n",
       "      <td>prop_meme_9418.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>23347</td>\n",
       "      <td>This is now your real trends look when\\nyour\\n...</td>\n",
       "      <td>prop_meme_3716.png</td>\n",
       "      <td>X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0    28493  PUTIN SCROLLING HIS INSTA FOR G20 NEWS BE LIKE...   \n",
       "1    12336  President Trump has proven\\nthat a business ma...   \n",
       "2    24720  THE NOT A PRAYINGMAN, BUTFIRE WOULD\\nBE ON MY ...   \n",
       "3    23847  WHAT IF..\\nWE WERE HONEST WITH OUR K\\nGOVERNME...   \n",
       "4    35217  BANA\\nwwww\\nCUBA, 2016\\nVENEZUELA, 2019\\nFANTA...   \n",
       "..     ...                                                ...   \n",
       "295  56136  J.Lo at 52:\\nMe at 35:\\nfb.com/DunderMifflin Meme   \n",
       "296  12383       AM I THE BEST\\nPRESIDENT EVER?\\nYES OR NO?\\n   \n",
       "297  46093        Thank You, God\\nFor Giving Us Donald Trump!   \n",
       "298  32814  Andrew Torba t @a\\n8h\\nNew York Times reporter...   \n",
       "299  23347  This is now your real trends look when\\nyour\\n...   \n",
       "\n",
       "                   image                                           filepath  \n",
       "0    prop_meme_12066.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "1      prop_meme_342.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "2     prop_meme_3342.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "3     prop_meme_4102.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "4    prop_meme_10986.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "..                   ...                                                ...  \n",
       "295  prop_meme_24949.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "296    prop_meme_389.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "297  prop_meme_18782.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "298   prop_meme_9418.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "299   prop_meme_3716.png  X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev...  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2b_images\\dev'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2b\\dev_unlabeled.json')\n",
    "df = pd.merge(df, images_df, on='image')\n",
    "df.fillna(' ', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6fda2b04-c251-4f5a-bcc8-782339338e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_eval = [str(i) for i in df['filepath'].values]\n",
    "texts_eval = [str(i) for i in df['text'].astype(str).values.tolist()]\n",
    "id = df['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "be4734aa-a501-43f0-a280-df61fe048773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 512])\n",
      "attention_mask torch.Size([1, 512])\n",
      "image torch.Size([1, 3, 224, 224])\n",
      "label torch.Size([1])\n",
      "id torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# no labels, lazy replace labels with ID\n",
    "eval_dataset = VisionTextDataset_eval(img=images_eval, txt=texts_eval, lbs=id, tokenizer=tokenizer, n_classes=2, transform=model_transforms, id=id)\n",
    "eval_data = DataLoader(eval_dataset, shuffle=False)\n",
    "batch = next(iter(eval_data))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1b806aee-3f1e-4eca-ab64-0655a58abdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_output = []\n",
    "\n",
    "def submit_eval(ensemble_model, path, data):\n",
    "    # actuals, predictions = [], []\n",
    "\n",
    "    ensemble_model = EnsembleModel(vision_model, text_model, 2)\n",
    "    ensemble_model.load_state_dict(torch.load(path))\n",
    "    ensemble_model.to(device)\n",
    "    ensemble_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data):\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            outputs = ensemble_model(text_features=batch['input_ids'],\n",
    "                                     attention_masks=batch['attention_mask'],\n",
    "                                     image_features=batch['image'])\n",
    "            image_id = batch['id']\n",
    "        \n",
    "            if int(F.softmax(outputs, 1).cpu().detach().numpy().argmax(1)) == 0:\n",
    "                pred = 'non_propagandistic'\n",
    "            else:\n",
    "                pred = 'propagandistic'            \n",
    "            \n",
    "            prediction_output.append({'id': str(int(image_id.cpu())),\n",
    "                                           'label': pred})\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cd8048fa-86b4-4608-af49-1c1f1d4756ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07009003a35041e387b779aad00a29d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'X:\\\\PhD\\\\SemEval Task4\\\\Code\\\\Mock Coding\\\\multimodal-baselines\\\\Bert_VGG16_ensemble_8batch_epochs8_weights.pth'\n",
    "pred = submit_eval(ensemble_model, path, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "00f056cf-b505-408b-88cd-f9e57efc1c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '28493', 'label': 'propagandistic'},\n",
       " {'id': '12336', 'label': 'propagandistic'},\n",
       " {'id': '24720', 'label': 'propagandistic'},\n",
       " {'id': '23847', 'label': 'propagandistic'},\n",
       " {'id': '35217', 'label': 'propagandistic'},\n",
       " {'id': '45439', 'label': 'propagandistic'},\n",
       " {'id': '45266', 'label': 'propagandistic'},\n",
       " {'id': '44244', 'label': 'propagandistic'},\n",
       " {'id': '56158', 'label': 'non_propagandistic'},\n",
       " {'id': '10156', 'label': 'propagandistic'},\n",
       " {'id': '24246', 'label': 'propagandistic'},\n",
       " {'id': '45349', 'label': 'non_propagandistic'},\n",
       " {'id': '32161', 'label': 'propagandistic'},\n",
       " {'id': '34233', 'label': 'propagandistic'},\n",
       " {'id': '41883', 'label': 'propagandistic'},\n",
       " {'id': '46567', 'label': 'propagandistic'},\n",
       " {'id': '23831', 'label': 'propagandistic'},\n",
       " {'id': '35454', 'label': 'propagandistic'},\n",
       " {'id': '44043', 'label': 'propagandistic'},\n",
       " {'id': '33591', 'label': 'propagandistic'},\n",
       " {'id': '47095', 'label': 'non_propagandistic'},\n",
       " {'id': '35973', 'label': 'propagandistic'},\n",
       " {'id': '44370', 'label': 'propagandistic'},\n",
       " {'id': '29119', 'label': 'non_propagandistic'},\n",
       " {'id': '28106', 'label': 'non_propagandistic'},\n",
       " {'id': '23621', 'label': 'propagandistic'},\n",
       " {'id': '46048', 'label': 'propagandistic'},\n",
       " {'id': '24370', 'label': 'propagandistic'},\n",
       " {'id': '34392', 'label': 'propagandistic'},\n",
       " {'id': '25281', 'label': 'propagandistic'},\n",
       " {'id': '45132', 'label': 'non_propagandistic'},\n",
       " {'id': '29958', 'label': 'propagandistic'},\n",
       " {'id': '48299', 'label': 'non_propagandistic'},\n",
       " {'id': '32287', 'label': 'non_propagandistic'},\n",
       " {'id': '36122', 'label': 'propagandistic'},\n",
       " {'id': '32916', 'label': 'propagandistic'},\n",
       " {'id': '56042', 'label': 'non_propagandistic'},\n",
       " {'id': '24661', 'label': 'propagandistic'},\n",
       " {'id': '12335', 'label': 'propagandistic'},\n",
       " {'id': '35079', 'label': 'propagandistic'},\n",
       " {'id': '45443', 'label': 'non_propagandistic'},\n",
       " {'id': '34831', 'label': 'propagandistic'},\n",
       " {'id': '41710', 'label': 'propagandistic'},\n",
       " {'id': '32211', 'label': 'non_propagandistic'},\n",
       " {'id': '42876', 'label': 'propagandistic'},\n",
       " {'id': '44645', 'label': 'propagandistic'},\n",
       " {'id': '34229', 'label': 'propagandistic'},\n",
       " {'id': '35361', 'label': 'non_propagandistic'},\n",
       " {'id': '46699', 'label': 'propagandistic'},\n",
       " {'id': '41908', 'label': 'non_propagandistic'},\n",
       " {'id': '56168', 'label': 'non_propagandistic'},\n",
       " {'id': '56000', 'label': 'non_propagandistic'},\n",
       " {'id': '45003', 'label': 'non_propagandistic'},\n",
       " {'id': '56111', 'label': 'non_propagandistic'},\n",
       " {'id': '23400', 'label': 'propagandistic'},\n",
       " {'id': '56015', 'label': 'non_propagandistic'},\n",
       " {'id': '33762', 'label': 'non_propagandistic'},\n",
       " {'id': '45059', 'label': 'non_propagandistic'},\n",
       " {'id': '23579', 'label': 'propagandistic'},\n",
       " {'id': '50148', 'label': 'non_propagandistic'},\n",
       " {'id': '28325', 'label': 'propagandistic'},\n",
       " {'id': '15334', 'label': 'propagandistic'},\n",
       " {'id': '26043', 'label': 'non_propagandistic'},\n",
       " {'id': '45075', 'label': 'non_propagandistic'},\n",
       " {'id': '25562', 'label': 'propagandistic'},\n",
       " {'id': '56055', 'label': 'non_propagandistic'},\n",
       " {'id': '45667', 'label': 'propagandistic'},\n",
       " {'id': '45612', 'label': 'non_propagandistic'},\n",
       " {'id': '35000', 'label': 'propagandistic'},\n",
       " {'id': '24612', 'label': 'propagandistic'},\n",
       " {'id': '44642', 'label': 'propagandistic'},\n",
       " {'id': '23884', 'label': 'propagandistic'},\n",
       " {'id': '32323', 'label': 'propagandistic'},\n",
       " {'id': '47160', 'label': 'non_propagandistic'},\n",
       " {'id': '42088', 'label': 'non_propagandistic'},\n",
       " {'id': '34724', 'label': 'propagandistic'},\n",
       " {'id': '44974', 'label': 'non_propagandistic'},\n",
       " {'id': '25623', 'label': 'propagandistic'},\n",
       " {'id': '35551', 'label': 'non_propagandistic'},\n",
       " {'id': '28550', 'label': 'propagandistic'},\n",
       " {'id': '42299', 'label': 'propagandistic'},\n",
       " {'id': '46987', 'label': 'non_propagandistic'},\n",
       " {'id': '45047', 'label': 'propagandistic'},\n",
       " {'id': '45138', 'label': 'propagandistic'},\n",
       " {'id': '44019', 'label': 'propagandistic'},\n",
       " {'id': '43591', 'label': 'propagandistic'},\n",
       " {'id': '45801', 'label': 'propagandistic'},\n",
       " {'id': '36044', 'label': 'propagandistic'},\n",
       " {'id': '31135', 'label': 'propagandistic'},\n",
       " {'id': '41606', 'label': 'propagandistic'},\n",
       " {'id': '32452', 'label': 'non_propagandistic'},\n",
       " {'id': '32733', 'label': 'propagandistic'},\n",
       " {'id': '30313', 'label': 'propagandistic'},\n",
       " {'id': '36499', 'label': 'propagandistic'},\n",
       " {'id': '41355', 'label': 'propagandistic'},\n",
       " {'id': '42251', 'label': 'propagandistic'},\n",
       " {'id': '23300', 'label': 'propagandistic'},\n",
       " {'id': '35056', 'label': 'propagandistic'},\n",
       " {'id': '33794', 'label': 'propagandistic'},\n",
       " {'id': '32943', 'label': 'non_propagandistic'},\n",
       " {'id': '24844', 'label': 'propagandistic'},\n",
       " {'id': '31975', 'label': 'propagandistic'},\n",
       " {'id': '42108', 'label': 'propagandistic'},\n",
       " {'id': '43416', 'label': 'propagandistic'},\n",
       " {'id': '29839', 'label': 'non_propagandistic'},\n",
       " {'id': '44898', 'label': 'non_propagandistic'},\n",
       " {'id': '45172', 'label': 'non_propagandistic'},\n",
       " {'id': '35890', 'label': 'propagandistic'},\n",
       " {'id': '23669', 'label': 'non_propagandistic'},\n",
       " {'id': '45254', 'label': 'non_propagandistic'},\n",
       " {'id': '27762', 'label': 'propagandistic'},\n",
       " {'id': '32330', 'label': 'non_propagandistic'},\n",
       " {'id': '33456', 'label': 'propagandistic'},\n",
       " {'id': '36881', 'label': 'propagandistic'},\n",
       " {'id': '12674', 'label': 'propagandistic'},\n",
       " {'id': '12632', 'label': 'propagandistic'},\n",
       " {'id': '45239', 'label': 'non_propagandistic'},\n",
       " {'id': '49859', 'label': 'propagandistic'},\n",
       " {'id': '44846', 'label': 'propagandistic'},\n",
       " {'id': '29191', 'label': 'propagandistic'},\n",
       " {'id': '28339', 'label': 'propagandistic'},\n",
       " {'id': '45316', 'label': 'non_propagandistic'},\n",
       " {'id': '46478', 'label': 'propagandistic'},\n",
       " {'id': '29919', 'label': 'propagandistic'},\n",
       " {'id': '12438', 'label': 'propagandistic'},\n",
       " {'id': '35392', 'label': 'propagandistic'},\n",
       " {'id': '24494', 'label': 'propagandistic'},\n",
       " {'id': '33003', 'label': 'propagandistic'},\n",
       " {'id': '56059', 'label': 'non_propagandistic'},\n",
       " {'id': '47866', 'label': 'propagandistic'},\n",
       " {'id': '48519', 'label': 'propagandistic'},\n",
       " {'id': '43694', 'label': 'propagandistic'},\n",
       " {'id': '42340', 'label': 'propagandistic'},\n",
       " {'id': '44042', 'label': 'propagandistic'},\n",
       " {'id': '56089', 'label': 'non_propagandistic'},\n",
       " {'id': '35329', 'label': 'propagandistic'},\n",
       " {'id': '33948', 'label': 'propagandistic'},\n",
       " {'id': '35686', 'label': 'propagandistic'},\n",
       " {'id': '56096', 'label': 'non_propagandistic'},\n",
       " {'id': '41801', 'label': 'propagandistic'},\n",
       " {'id': '12668', 'label': 'propagandistic'},\n",
       " {'id': '45304', 'label': 'propagandistic'},\n",
       " {'id': '56033', 'label': 'non_propagandistic'},\n",
       " {'id': '41672', 'label': 'non_propagandistic'},\n",
       " {'id': '30421', 'label': 'propagandistic'},\n",
       " {'id': '42935', 'label': 'propagandistic'},\n",
       " {'id': '36684', 'label': 'non_propagandistic'},\n",
       " {'id': '56004', 'label': 'non_propagandistic'},\n",
       " {'id': '49677', 'label': 'propagandistic'},\n",
       " {'id': '56204', 'label': 'non_propagandistic'},\n",
       " {'id': '44847', 'label': 'propagandistic'},\n",
       " {'id': '32403', 'label': 'propagandistic'},\n",
       " {'id': '44749', 'label': 'propagandistic'},\n",
       " {'id': '35402', 'label': 'propagandistic'},\n",
       " {'id': '36576', 'label': 'propagandistic'},\n",
       " {'id': '23751', 'label': 'propagandistic'},\n",
       " {'id': '31841', 'label': 'non_propagandistic'},\n",
       " {'id': '41649', 'label': 'propagandistic'},\n",
       " {'id': '56133', 'label': 'non_propagandistic'},\n",
       " {'id': '43502', 'label': 'propagandistic'},\n",
       " {'id': '45546', 'label': 'propagandistic'},\n",
       " {'id': '30697', 'label': 'propagandistic'},\n",
       " {'id': '33798', 'label': 'propagandistic'},\n",
       " {'id': '42148', 'label': 'non_propagandistic'},\n",
       " {'id': '36705', 'label': 'propagandistic'},\n",
       " {'id': '44924', 'label': 'non_propagandistic'},\n",
       " {'id': '42932', 'label': 'propagandistic'},\n",
       " {'id': '24082', 'label': 'propagandistic'},\n",
       " {'id': '41622', 'label': 'non_propagandistic'},\n",
       " {'id': '32532', 'label': 'non_propagandistic'},\n",
       " {'id': '30833', 'label': 'non_propagandistic'},\n",
       " {'id': '56014', 'label': 'non_propagandistic'},\n",
       " {'id': '23318', 'label': 'propagandistic'},\n",
       " {'id': '10093', 'label': 'propagandistic'},\n",
       " {'id': '15502', 'label': 'propagandistic'},\n",
       " {'id': '47167', 'label': 'non_propagandistic'},\n",
       " {'id': '33501', 'label': 'propagandistic'},\n",
       " {'id': '43543', 'label': 'propagandistic'},\n",
       " {'id': '43605', 'label': 'propagandistic'},\n",
       " {'id': '47980', 'label': 'propagandistic'},\n",
       " {'id': '30144', 'label': 'propagandistic'},\n",
       " {'id': '46428', 'label': 'propagandistic'},\n",
       " {'id': '27865', 'label': 'propagandistic'},\n",
       " {'id': '36673', 'label': 'propagandistic'},\n",
       " {'id': '47068', 'label': 'non_propagandistic'},\n",
       " {'id': '42938', 'label': 'propagandistic'},\n",
       " {'id': '28908', 'label': 'propagandistic'},\n",
       " {'id': '36401', 'label': 'propagandistic'},\n",
       " {'id': '36399', 'label': 'propagandistic'},\n",
       " {'id': '42265', 'label': 'propagandistic'},\n",
       " {'id': '28431', 'label': 'propagandistic'},\n",
       " {'id': '41629', 'label': 'propagandistic'},\n",
       " {'id': '56019', 'label': 'non_propagandistic'},\n",
       " {'id': '45767', 'label': 'propagandistic'},\n",
       " {'id': '44682', 'label': 'propagandistic'},\n",
       " {'id': '33359', 'label': 'propagandistic'},\n",
       " {'id': '44792', 'label': 'non_propagandistic'},\n",
       " {'id': '42241', 'label': 'propagandistic'},\n",
       " {'id': '43529', 'label': 'propagandistic'},\n",
       " {'id': '45808', 'label': 'propagandistic'},\n",
       " {'id': '32389', 'label': 'propagandistic'},\n",
       " {'id': '41652', 'label': 'propagandistic'},\n",
       " {'id': '28374', 'label': 'propagandistic'},\n",
       " {'id': '28420', 'label': 'propagandistic'},\n",
       " {'id': '44574', 'label': 'propagandistic'},\n",
       " {'id': '45220', 'label': 'non_propagandistic'},\n",
       " {'id': '42310', 'label': 'non_propagandistic'},\n",
       " {'id': '42248', 'label': 'propagandistic'},\n",
       " {'id': '31145', 'label': 'propagandistic'},\n",
       " {'id': '13046', 'label': 'propagandistic'},\n",
       " {'id': '45317', 'label': 'propagandistic'},\n",
       " {'id': '32879', 'label': 'propagandistic'},\n",
       " {'id': '32022', 'label': 'propagandistic'},\n",
       " {'id': '41766', 'label': 'propagandistic'},\n",
       " {'id': '45401', 'label': 'propagandistic'},\n",
       " {'id': '25894', 'label': 'propagandistic'},\n",
       " {'id': '46337', 'label': 'propagandistic'},\n",
       " {'id': '12947', 'label': 'propagandistic'},\n",
       " {'id': '29450', 'label': 'propagandistic'},\n",
       " {'id': '32727', 'label': 'propagandistic'},\n",
       " {'id': '35510', 'label': 'propagandistic'},\n",
       " {'id': '34247', 'label': 'propagandistic'},\n",
       " {'id': '43504', 'label': 'propagandistic'},\n",
       " {'id': '56112', 'label': 'non_propagandistic'},\n",
       " {'id': '45395', 'label': 'propagandistic'},\n",
       " {'id': '50053', 'label': 'propagandistic'},\n",
       " {'id': '29644', 'label': 'propagandistic'},\n",
       " {'id': '45616', 'label': 'non_propagandistic'},\n",
       " {'id': '32140', 'label': 'non_propagandistic'},\n",
       " {'id': '46082', 'label': 'propagandistic'},\n",
       " {'id': '31296', 'label': 'propagandistic'},\n",
       " {'id': '31857', 'label': 'propagandistic'},\n",
       " {'id': '27768', 'label': 'propagandistic'},\n",
       " {'id': '30373', 'label': 'non_propagandistic'},\n",
       " {'id': '35265', 'label': 'propagandistic'},\n",
       " {'id': '12742', 'label': 'propagandistic'},\n",
       " {'id': '45194', 'label': 'non_propagandistic'},\n",
       " {'id': '56195', 'label': 'non_propagandistic'},\n",
       " {'id': '45645', 'label': 'non_propagandistic'},\n",
       " {'id': '47071', 'label': 'non_propagandistic'},\n",
       " {'id': '31448', 'label': 'propagandistic'},\n",
       " {'id': '56138', 'label': 'non_propagandistic'},\n",
       " {'id': '56061', 'label': 'non_propagandistic'},\n",
       " {'id': '25173', 'label': 'propagandistic'},\n",
       " {'id': '29821', 'label': 'propagandistic'},\n",
       " {'id': '30460', 'label': 'propagandistic'},\n",
       " {'id': '45414', 'label': 'propagandistic'},\n",
       " {'id': '46393', 'label': 'propagandistic'},\n",
       " {'id': '44246', 'label': 'propagandistic'},\n",
       " {'id': '45622', 'label': 'non_propagandistic'},\n",
       " {'id': '28845', 'label': 'propagandistic'},\n",
       " {'id': '45187', 'label': 'propagandistic'},\n",
       " {'id': '56052', 'label': 'non_propagandistic'},\n",
       " {'id': '28494', 'label': 'propagandistic'},\n",
       " {'id': '33394', 'label': 'propagandistic'},\n",
       " {'id': '33990', 'label': 'propagandistic'},\n",
       " {'id': '28219', 'label': 'propagandistic'},\n",
       " {'id': '35882', 'label': 'propagandistic'},\n",
       " {'id': '43419', 'label': 'propagandistic'},\n",
       " {'id': '24602', 'label': 'propagandistic'},\n",
       " {'id': '36695', 'label': 'propagandistic'},\n",
       " {'id': '30498', 'label': 'propagandistic'},\n",
       " {'id': '44249', 'label': 'propagandistic'},\n",
       " {'id': '44655', 'label': 'propagandistic'},\n",
       " {'id': '56080', 'label': 'non_propagandistic'},\n",
       " {'id': '33614', 'label': 'non_propagandistic'},\n",
       " {'id': '50625', 'label': 'propagandistic'},\n",
       " {'id': '23774', 'label': 'propagandistic'},\n",
       " {'id': '34167', 'label': 'propagandistic'},\n",
       " {'id': '47987', 'label': 'non_propagandistic'},\n",
       " {'id': '44980', 'label': 'propagandistic'},\n",
       " {'id': '36864', 'label': 'propagandistic'},\n",
       " {'id': '56023', 'label': 'non_propagandistic'},\n",
       " {'id': '25216', 'label': 'propagandistic'},\n",
       " {'id': '12842', 'label': 'propagandistic'},\n",
       " {'id': '44818', 'label': 'non_propagandistic'},\n",
       " {'id': '31536', 'label': 'propagandistic'},\n",
       " {'id': '36301', 'label': 'propagandistic'},\n",
       " {'id': '25398', 'label': 'propagandistic'},\n",
       " {'id': '56087', 'label': 'non_propagandistic'},\n",
       " {'id': '36892', 'label': 'propagandistic'},\n",
       " {'id': '29727', 'label': 'propagandistic'},\n",
       " {'id': '29444', 'label': 'propagandistic'},\n",
       " {'id': '35394', 'label': 'propagandistic'},\n",
       " {'id': '46705', 'label': 'propagandistic'},\n",
       " {'id': '29227', 'label': 'non_propagandistic'},\n",
       " {'id': '44852', 'label': 'propagandistic'},\n",
       " {'id': '30752', 'label': 'propagandistic'},\n",
       " {'id': '30225', 'label': 'propagandistic'},\n",
       " {'id': '43549', 'label': 'propagandistic'},\n",
       " {'id': '45281', 'label': 'propagandistic'},\n",
       " {'id': '47044', 'label': 'non_propagandistic'},\n",
       " {'id': '48973', 'label': 'propagandistic'},\n",
       " {'id': '15628', 'label': 'propagandistic'},\n",
       " {'id': '42126', 'label': 'non_propagandistic'},\n",
       " {'id': '56136', 'label': 'non_propagandistic'},\n",
       " {'id': '12383', 'label': 'propagandistic'},\n",
       " {'id': '46093', 'label': 'non_propagandistic'},\n",
       " {'id': '32814', 'label': 'propagandistic'},\n",
       " {'id': '23347', 'label': 'propagandistic'}]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6af25227-c95a-4296-8cc4-ae9352e8350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X:\\\\PhD\\SemEval Task4\\\\Code\\\\Evaluation Code\\\\Subtask2b\\\\subtask2b_bda.txt', 'w') as f:\n",
    "    f.write(json.dumps(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032f643-9558-4ba1-842d-58140a75a5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76741a4e-69bf-4769-959f-54bd63d95ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092a2ec-18a5-4bc0-bb9d-6374166eb7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b928da-81ec-49d8-b18e-994ccc5e2a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3789e1b-5413-45a0-a8d3-e4f0dac5bad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb83684-2bcb-419e-9103-ff368f5d1d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf96f1-62d7-4fd7-9064-0848d975d356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c05b9a2-02a0-433f-8401-cb3dcf68c6c1",
   "metadata": {},
   "source": [
    "# Other Models - free to use if you can get it working\n",
    "Better method, but model gets upset with the layers in the final part of the classifier and I can't remember how to fix it.\n",
    "\n",
    "Other layers not included but will add back in\n",
    "\n",
    "- self.bn = nn.BatchNorm1d(value) < batchnorm\n",
    "- self.dropout = nn.Dropout(drop_prob) < dropout\n",
    "- self.classify = nn.Linear(in_features = 512, out_features = num_classes) < linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c23bf377-302f-4631-b030-f6e2cf5e22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "\n",
    "args.img_embed_pool_type = \"avg\"\n",
    "args.num_image_embeds = 2\n",
    "args.hidden_size = 768\n",
    "args.img_hidden_size = 4098\n",
    "args.n_classes = len(list(set(labels))) # taken from our label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e403d716-df1b-4f4d-abdf-176bd95ada70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text encoder\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\") # change the pretrained model here\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        self.out = nn.Linear(args.hidden_size, 512)\n",
    "        self.out2 = nn.Linear(512, args.n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.drop(outputs.pooler_output)\n",
    "        output = self.out(output)\n",
    "        output = self.out2(output)\n",
    "        return output\n",
    "\n",
    "# image encoder\n",
    "# see https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        model = torchvision.models.vgg16(pretrained=True) # change the pretrained model here, or weights\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "\n",
    "        pool_func = (\n",
    "            nn.AdaptiveAvgPool2d\n",
    "            if args.img_embed_pool_type == \"avg\"\n",
    "            else nn.AdaptiveMaxPool2d\n",
    "        )\n",
    "\n",
    "        if args.num_image_embeds in [1, 2, 3, 5, 7]:\n",
    "            self.pool = pool_func((args.num_image_embeds, 1))\n",
    "        elif args.num_image_embeds == 4:\n",
    "            self.pool = pool_func((2, 2))\n",
    "        elif args.num_image_embeds == 6:\n",
    "            self.pool = pool_func((3, 2))\n",
    "        elif args.num_image_embeds == 8:\n",
    "            self.pool = pool_func((4, 2))\n",
    "        elif args.num_image_embeds == 9:\n",
    "            self.pool = pool_func((3, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
    "        out = self.pool(self.model(x))\n",
    "        out = torch.flatten(out, start_dim=2)\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out  # BxNx2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f3ce52e-d09a-4f57-b033-e2f9312c8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the model outputs\n",
    "\n",
    "class MultimodalConcatBertClf(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(MultimodalConcatBertClf, self).__init__()\n",
    "        self.args = args\n",
    "        self.txtenc = BertEncoder(args)\n",
    "        self.imgenc = ImageEncoder(args)\n",
    "\n",
    "        last_size = args.hidden_size + (args.img_hidden_size * args.num_image_embeds)\n",
    "        self.clf = nn.ModuleList()\n",
    "\n",
    "        self.clf.append(nn.Linear(last_size, args.n_classes))\n",
    "\n",
    "    def forward(self, txt, mask, img):\n",
    "        txt = self.txtenc(txt, mask)\n",
    "        img = self.imgenc(img)   \n",
    "        \n",
    "        out = torch.cat((txt, img), dim=1)\n",
    "        out = self.concat(out)\n",
    "        for layer in self.clf:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45509fdf-9399-4a0c-b551-3d1a1b957c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MultimodalConcatBertClf(\n",
       "  (txtenc): BertEncoder(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (out): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (out2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (imgenc): ImageEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (25): ReLU(inplace=True)\n",
       "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (27): ReLU(inplace=True)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (pool): AdaptiveAvgPool2d(output_size=(2, 1))\n",
       "  )\n",
       "  (clf): ModuleList(\n",
       "    (0): Linear(in_features=8964, out_features=2, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultimodalConcatBertClf(args)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a853f97-4e1d-42d5-af27-0b521eb6f4e4",
   "metadata": {},
   "source": [
    "## FusionNet Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "82014849-1b82-4d55-887d-0fd725cde98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where text and image model are the same ones defined\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, drop_prob = 0.1):\n",
    "        super(FusionNet, self).__init__()\n",
    "        self.text_model =text_model()\n",
    "        self.image_model = vision_model()\n",
    "        \n",
    "        self.pooler = nn.Linear(in_features=768, out_features=768)\n",
    "        \n",
    "        self.concat = nn.Linear(in_features=768+2048, out_features= 512)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.bn1 = nn.BatchNorm1d(768)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "    \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.classify = nn.Linear(in_features = 512, out_features = num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, text_features, image_features):\n",
    "        text_features = self.text_model(text_features)\n",
    "        image_features = self.image_model(image_features)\n",
    "        \n",
    "        text_features = torch.tanh(self.pooler(text_features))\n",
    "        text_features = self.dropout(text_features)\n",
    "\n",
    "        text_features = self.bn1(text_features)\n",
    "        image_features = self.bn2(image_features)\n",
    "      \n",
    "        fused =  torch.cat((text_features, image_features), dim=1)\n",
    "      \n",
    "        x = self.concat(fused)\n",
    "  \n",
    "        x = F.tanh(self.bn(x))          \n",
    "  \n",
    "        x = F.tanh(self.classify(x)) \n",
    "  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a8be4-28d2-431d-a5e2-961d352f52b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392ac86-781a-426c-bf14-ec771bad1211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac6d55-1142-44c3-bbd1-284dd6a75fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69194ffb-0e61-4dd2-a4ce-56a402d08cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
