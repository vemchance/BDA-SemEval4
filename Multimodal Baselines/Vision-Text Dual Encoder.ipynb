{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e808c5ff-1cdc-4665-8466-934906d659da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoImageProcessor,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    VisionTextDualEncoderConfig,\n",
    "    ViTForImageClassification,\n",
    "    AutoModelForSequenceClassification)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import walk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28635b4f-db83-4342-a404-04d7e06b589b",
   "metadata": {},
   "source": [
    "Data feed in: set of image filenames, set of corresponding text, set of corresponding labels\n",
    "\n",
    "to do:\n",
    "- try to pretrain a model? https://github.com/NielsRogge/Transformers-Tutorials\n",
    "- Reqs.txt\n",
    "- Proper configs and classification head for BERT - finetune bert first as a sequence classification and pass this as a local model?\n",
    "- Fix loss function. Doesn't work with either one hot or categorical, stuck with the default func.\n",
    "- Change the dataset function to accept 'one hot or not' param instead of commenting out\n",
    "- Change padding in dataloader to label count not hard coded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2f1c7-7bd7-4808-ae5b-4573177c72f9",
   "metadata": {},
   "source": [
    "# Data Reading\n",
    "As usual, these are my paths and you would need to update to your own. Or build a cleverer way to do it - whichever you prefer. Uncommented next line for your own path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f17b9-3338-46ca-8d3f-857476202f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'this is the path for the images in train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5582b25d-6576-4951-b1a0-aa40733e88e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2a_images\\train_images\\train_images'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2a\\train.json') # your path\n",
    "df = pd.merge(df, images_df, on='image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21da9d-c03e-43ac-bf02-abe3dff47f19",
   "metadata": {},
   "source": [
    "Two types of labels are currently used - integers and one hot encoding. One hot is requried for BCE w/ logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28355054-9c9a-4adb-88a5-fd72b67f979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['labels'].values\n",
    "\n",
    "unique_labels = []\n",
    "\n",
    "for i in labels:\n",
    "    for x in i:\n",
    "        unique_labels.append(x)\n",
    "unique_labels = list(set(unique_labels))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "# try a one-hot as well\n",
    "ml = MultiLabelBinarizer()\n",
    "\n",
    "df['one_hot_labels'] = ml.fit_transform(df['labels'])[:,1:].tolist()\n",
    "df['labels'] = df['labels'].apply(le.transform) # actually you can do the above on this as well, but I didn't change it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bbd5b6a-4e26-4784-a4ef-50940bb18ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader is better without a dataframe, so this time we're going with lists\n",
    "\n",
    "images = [str(i) for i in df['filepath'].values]\n",
    "texts = [str(i) for i in df['text'].astype(str).values.tolist()]\n",
    "labels = df['labels'].values\n",
    "one_hot_labels = df['one_hot_labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b732292-b457-45e3-99b7-99cdca462c52",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1029ffaa-8700-40e2-9f9f-75003b69a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'X:\\PhD\\SemEval Task4\\Data\\subtask2a_images\\validation_images\\validation_images'\n",
    "images = [os.path.join(dirpath,f) for (dirpath, dirnames, filenames) in os.walk(path) for f in filenames]\n",
    "images_df = pd.DataFrame(images, columns=['filepath'])\n",
    "images_df['image'] = images_df['filepath'].str.split('\\\\').str[-1]\n",
    "\n",
    "df_val = pd.read_json(r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask2a\\validation.json')\n",
    "df_val = pd.merge(df_val, images_df, on='image')\n",
    "\n",
    "val_labels = df_val['labels'].values\n",
    "\n",
    "unique_labels = []\n",
    "\n",
    "for i in val_labels:\n",
    "    for x in i:\n",
    "        unique_labels.append(x)\n",
    "unique_labels = list(set(unique_labels))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "# try a one-hot as well\n",
    "ml = MultiLabelBinarizer()\n",
    "\n",
    "df_val['one_hot_labels'] = ml.fit_transform(df_val['labels'])[:,1:].tolist()\n",
    "df_val['labels'] = df_val['labels'].apply(le.transform)\n",
    "\n",
    "images_val = [str(i) for i in df_val['filepath'].values]\n",
    "texts_val = [str(i) for i in df_val['text'].astype(str).values.tolist()]\n",
    "labels_val = df_val['labels'].values\n",
    "one_hot_labels_val = df_val['one_hot_labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efb386-114e-476b-be85-b180a8a9e37c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e958ccb8-8532-46aa-a71a-03c1b30575e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img, txt, lbs, processor):\n",
    "        self.image = img\n",
    "        self.text = txt\n",
    "        self.labels = lbs\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image[idx]\n",
    "        text = self.text[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.processor(text, Image.open(image).convert('RGB'), is_split_into_words=True, \n",
    "                                  padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "\n",
    "        ### comment out below for not one-hot labels ####\n",
    "\n",
    "        def pad_tensor(t):\n",
    "             t = torch.tensor(t)\n",
    "             padding = 22 - t.size()[0] # change to label count\n",
    "             t = torch.nn.functional.pad(t, (0, padding))\n",
    "             return t\n",
    "            \n",
    "        encoding[\"labels\"] = label\n",
    "        encoding[\"labels\"] = pad_tensor(label)\n",
    "\n",
    "         ### comment out above for not one-hot labels ####\n",
    "\n",
    "        \n",
    "\n",
    "        # uncomment for one hot\n",
    "        #encoding[\"labels\"] = torch.tensor(label)\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f014dbe-979f-4832-9b84-0adc579de309",
   "metadata": {},
   "source": [
    "Test version of this is using BERT and ViT. Should change to something else as both are bad at the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04866cd9-4630-4498-abcf-e4c92385f5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2023,  2003,  2339,  2057,  1005,  2128,  2489,  1032,  1050,\n",
       "          1032, 23961, 24158,  2003,  2339,  2057,  1005,  2128,  3647,  1032,\n",
       "          1050,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'pixel_values': tensor([[[-0.8510, -0.8510, -0.8510,  ..., -0.8745, -0.8745, -0.8745],\n",
       "          [-0.8588, -0.8588, -0.8588,  ..., -0.8980, -0.8980, -0.8980],\n",
       "          [-0.8588, -0.8588, -0.8588,  ..., -0.8980, -0.8980, -0.8980],\n",
       "          ...,\n",
       "          [-0.1373, -0.0824, -0.0118,  ..., -0.8902, -0.9059, -0.9137],\n",
       "          [-0.4667, -0.4353, -0.3882,  ..., -0.8745, -0.8902, -0.8902],\n",
       "          [-0.6000, -0.6000, -0.6157,  ..., -0.8745, -0.8667, -0.8667]],\n",
       " \n",
       "         [[-0.8667, -0.8667, -0.8667,  ..., -0.8431, -0.8431, -0.8353],\n",
       "          [-0.8824, -0.8824, -0.8824,  ..., -0.8745, -0.8745, -0.8667],\n",
       "          [-0.8902, -0.8902, -0.8902,  ..., -0.8824, -0.8824, -0.8745],\n",
       "          ...,\n",
       "          [-0.8196, -0.8353, -0.8588,  ..., -0.9059, -0.9137, -0.9137],\n",
       "          [-0.9686, -0.9765, -0.9843,  ..., -0.8902, -0.8980, -0.8980],\n",
       "          [-0.9765, -0.9843, -0.9843,  ..., -0.8824, -0.8745, -0.8745]],\n",
       " \n",
       "         [[-0.7098, -0.7098, -0.7098,  ..., -0.6392, -0.6549, -0.7020],\n",
       "          [-0.6706, -0.6706, -0.6706,  ..., -0.6314, -0.6392, -0.6941],\n",
       "          [-0.6157, -0.6157, -0.6157,  ..., -0.5922, -0.6078, -0.6549],\n",
       "          ...,\n",
       "          [-0.6314, -0.6314, -0.6392,  ..., -0.7098, -0.7412, -0.7490],\n",
       "          [-0.7961, -0.7961, -0.7804,  ..., -0.6941, -0.7255, -0.7412],\n",
       "          [-0.7961, -0.7961, -0.8039,  ..., -0.6863, -0.7020, -0.7176]]]),\n",
       " 'labels': tensor([ 5, 20,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0], dtype=torch.int32)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://huggingface.co/transformers/v3.0.2/main_classes/tokenizer.html\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=False)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer) # image processor and tokenizer here\n",
    "\n",
    "train_dataset = VisionTextDataset(img=images, txt=texts, lbs=labels, processor=processor)\n",
    "encoded = train_dataset[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e14d588c-beb3-4ca2-94f5-af2825780060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([16, 512])\n",
      "token_type_ids torch.Size([16, 512])\n",
      "attention_mask torch.Size([16, 512])\n",
      "pixel_values torch.Size([16, 3, 224, 224])\n",
      "labels torch.Size([16, 22])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc42058a-0ca8-40da-9101-7b2e103b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VisionTextDataset(img=images_val, txt=texts_val, lbs=labels_val, processor=processor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890eb2e1-090b-4bd3-832e-3d73e0b3f16a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70db5f5-5a86-42cb-b8e8-ae6bc603b520",
   "metadata": {},
   "source": [
    "class transformers.PretrainedConfig\r\n",
    "\n",
    "\n",
    "Parameters for fine-tuning tasks\r\n",
    "\r\n",
    "architectures (List[str], optional) — Model architectures that can be used with the model pretrained weights.\r\n",
    "finetuning_task (str, optional) — Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.\r\n",
    "id2label (Dict[int, str], optional) — A map from index (for instance prediction index, or target index) to label.\r\n",
    "label2id (Dict[str, int], optional) — A map from label to index for the model.\r\n",
    "num_labels (int, optional) — Number of labels to use in the last layer added to the model, typically for a classification task.\r\n",
    "task_specific_params (Dict[str, Any], optional) — Additional keyword arguments to store for the current task.\r\n",
    "problem_type (str, optional) — Problem type for XxxForSequenceClassification models. Can be one of \"regression\", \"single_label_classification\" or \"multi_label_classification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c176d45-73ef-413a-9b1e-5afc412b456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca3939a-60a3-42a3-865e-f26454629516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([22]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([22, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "text_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 22,\n",
    "    problem_type=\"multi_label_classification\") # The number of output labels--2 for binary classification.\n",
    "\n",
    "image_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=22, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "852ac709-bc96-4bf3-bdf8-9550a3b4a710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = VisionTextDualEncoderConfig.from_vision_text_configs(image_model.config, text_model.config)\n",
    "model = VisionTextDualEncoderModel(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83d7b0f2-50ad-4a34-a5bf-a300ccf5c204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of VisionTextDualEncoderModel(\n",
       "  (vision_model): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (text_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cca2fa48-a1ea-435b-b76c-aba6c4476340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041cf38bc47341659d5ccdf786b0c435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7751808166503906\n",
      "Loss: 2.775420665740967\n",
      "Loss: 2.777583599090576\n",
      "Loss: 2.773996114730835\n",
      "Loss: 2.779356002807617\n",
      "Loss: 2.7746291160583496\n",
      "Loss: 2.77052640914917\n",
      "Loss: 2.772876739501953\n",
      "Loss: 2.7782959938049316\n",
      "Loss: 2.7724967002868652\n",
      "Loss: 2.773310422897339\n",
      "Loss: 2.7742972373962402\n",
      "Loss: 2.7758469581604004\n",
      "Loss: 2.7707138061523438\n",
      "Loss: 2.77187442779541\n",
      "Loss: 2.775981903076172\n",
      "Loss: 2.773958206176758\n",
      "Loss: 2.773177146911621\n",
      "Loss: 2.769627809524536\n",
      "Loss: 2.774658203125\n",
      "Loss: 2.7754101753234863\n",
      "Loss: 2.7730846405029297\n",
      "Loss: 2.77384614944458\n",
      "Loss: 2.772264003753662\n",
      "Loss: 2.768883228302002\n",
      "Loss: 2.7738752365112305\n",
      "Loss: 2.771636962890625\n",
      "Loss: 2.772867202758789\n",
      "Loss: 2.775146961212158\n",
      "Loss: 2.7732410430908203\n",
      "Loss: 2.7751519680023193\n",
      "Loss: 1.3916306495666504\n",
      "Validation: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbc6ef8fc354e1595b69734ec3e0596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.772672176361084\n",
      "Loss: 2.7727346420288086\n",
      "Loss: 2.772688150405884\n",
      "Loss: 2.7727203369140625\n",
      "Loss: 2.7727675437927246\n",
      "Loss: 2.7727439403533936\n",
      "Loss: 2.7727174758911133\n",
      "Loss: 2.772777557373047\n",
      "Loss: 2.7727270126342773\n",
      "Loss: 2.7727015018463135\n",
      "Loss: 2.77260684967041\n",
      "Loss: 2.7727503776550293\n",
      "Loss: 2.7726922035217285\n",
      "Loss: 2.7730939388275146\n",
      "Loss: 2.7727577686309814\n",
      "Loss: 2.7730915546417236\n",
      "Loss: 2.7727818489074707\n",
      "Loss: 2.7728257179260254\n",
      "Loss: 2.7727999687194824\n",
      "Loss: 2.7727620601654053\n",
      "Loss: 2.772846221923828\n",
      "Loss: 2.77278995513916\n",
      "Loss: 2.772728443145752\n",
      "Loss: 2.7729830741882324\n",
      "Loss: 2.7728424072265625\n",
      "Loss: 2.7727584838867188\n",
      "Loss: 2.772745132446289\n",
      "Loss: 2.7727410793304443\n",
      "Loss: 2.7727527618408203\n",
      "Loss: 2.772728443145752\n",
      "Loss: 2.77274751663208\n",
      "Loss: 1.387087106704712\n",
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c87dd417b3447887c728af2b6bcf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.772669553756714\n",
      "Loss: 2.772737503051758\n",
      "Loss: 2.772662878036499\n",
      "Loss: 2.7725720405578613\n",
      "Loss: 2.7725517749786377\n",
      "Loss: 2.772498607635498\n",
      "Loss: 2.7724294662475586\n",
      "Loss: 2.7726259231567383\n",
      "Loss: 2.7725095748901367\n",
      "Loss: 2.7725961208343506\n",
      "Loss: 2.7725725173950195\n",
      "Loss: 2.7726118564605713\n",
      "Loss: 2.7724037170410156\n",
      "Loss: 2.772563934326172\n",
      "Loss: 2.772632122039795\n",
      "Loss: 2.772474765777588\n",
      "Loss: 2.772512912750244\n",
      "Loss: 2.7725844383239746\n",
      "Loss: 2.772174119949341\n",
      "Loss: 2.7724781036376953\n",
      "Loss: 2.7724967002868652\n",
      "Loss: 2.772408962249756\n",
      "Loss: 2.7726283073425293\n",
      "Loss: 2.7729992866516113\n",
      "Loss: 2.772859573364258\n",
      "Loss: 2.7723751068115234\n",
      "Loss: 2.7723541259765625\n",
      "Loss: 2.7724380493164062\n",
      "Loss: 2.7724151611328125\n",
      "Loss: 2.772456645965576\n",
      "Loss: 2.7726283073425293\n",
      "Loss: 1.3863894939422607\n",
      "Validation: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b4698a0bca4bdcb98f73f74e1f923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7726528644561768\n",
      "Loss: 2.7724432945251465\n",
      "Loss: 2.772304058074951\n",
      "Loss: 2.7731902599334717\n",
      "Loss: 2.7725586891174316\n",
      "Loss: 2.7728071212768555\n",
      "Loss: 2.7728374004364014\n",
      "Loss: 2.7731199264526367\n",
      "Loss: 2.772249937057495\n",
      "Loss: 2.7732996940612793\n",
      "Loss: 2.7728142738342285\n",
      "Loss: 2.772268772125244\n",
      "Loss: 2.7727017402648926\n",
      "Loss: 2.7729077339172363\n",
      "Loss: 2.7724108695983887\n",
      "Loss: 2.7727136611938477\n",
      "Loss: 2.7724609375\n",
      "Loss: 2.7727599143981934\n",
      "Loss: 2.772067070007324\n",
      "Loss: 2.772838592529297\n",
      "Loss: 2.772514581680298\n",
      "Loss: 2.77232027053833\n",
      "Loss: 2.7725155353546143\n",
      "Loss: 2.77262020111084\n",
      "Loss: 2.772892475128174\n",
      "Loss: 2.772512912750244\n",
      "Loss: 2.7723917961120605\n",
      "Loss: 2.773008346557617\n",
      "Loss: 2.772019386291504\n",
      "Loss: 2.7725117206573486\n",
      "Loss: 2.7724452018737793\n",
      "Loss: 1.3863768577575684\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478d4c9df8b447198880bf01e51adc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7727980613708496\n",
      "Loss: 2.772094249725342\n",
      "Loss: 2.771908760070801\n",
      "Loss: 2.771920680999756\n",
      "Loss: 2.771993637084961\n",
      "Loss: 2.7693285942077637\n",
      "Loss: 2.771723747253418\n",
      "Loss: 2.76641583442688\n",
      "Loss: 2.821023464202881\n",
      "Loss: 2.7757468223571777\n",
      "Loss: 2.773881435394287\n",
      "Loss: 2.7729673385620117\n",
      "Loss: 2.7727973461151123\n",
      "Loss: 2.7726454734802246\n",
      "Loss: 2.772643804550171\n",
      "Loss: 2.7727279663085938\n",
      "Loss: 2.7727456092834473\n",
      "Loss: 2.7726128101348877\n",
      "Loss: 2.7726058959960938\n",
      "Loss: 2.77262282371521\n",
      "Loss: 2.7727174758911133\n",
      "Loss: 2.772616386413574\n",
      "Loss: 2.772675037384033\n",
      "Loss: 2.7726993560791016\n",
      "Loss: 2.7726259231567383\n",
      "Loss: 2.772615432739258\n",
      "Loss: 2.772603988647461\n",
      "Loss: 2.7726082801818848\n",
      "Loss: 2.772594451904297\n",
      "Loss: 2.772623300552368\n",
      "Loss: 2.7726168632507324\n",
      "Loss: 1.3863545656204224\n",
      "Validation: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c45f7e4137b43ceb20bcc09e0676caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7725839614868164\n",
      "Loss: 2.772569179534912\n",
      "Loss: 2.7726125717163086\n",
      "Loss: 2.772552251815796\n",
      "Loss: 2.7725753784179688\n",
      "Loss: 2.7727112770080566\n",
      "Loss: 2.772584915161133\n",
      "Loss: 2.77258563041687\n",
      "Loss: 2.772670269012451\n",
      "Loss: 2.7726211547851562\n",
      "Loss: 2.7726173400878906\n",
      "Loss: 2.7726221084594727\n",
      "Loss: 2.7726352214813232\n",
      "Loss: 2.772585391998291\n",
      "Loss: 2.772608518600464\n",
      "Loss: 2.7725915908813477\n",
      "Loss: 2.7726194858551025\n",
      "Loss: 2.772636890411377\n",
      "Loss: 2.7725648880004883\n",
      "Loss: 2.772613763809204\n",
      "Loss: 2.7725882530212402\n",
      "Loss: 2.772603988647461\n",
      "Loss: 2.7725915908813477\n",
      "Loss: 2.772645950317383\n",
      "Loss: 2.7725830078125\n",
      "Loss: 2.7726073265075684\n",
      "Loss: 2.772613286972046\n",
      "Loss: 2.772630214691162\n",
      "Loss: 2.772617816925049\n",
      "Loss: 2.7726635932922363\n",
      "Loss: 2.772597551345825\n",
      "Loss: 1.386338472366333\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7fdf4b5927429aa7f835bf57e4816c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7725889682769775\n",
      "Loss: 2.7725942134857178\n",
      "Loss: 2.7726328372955322\n",
      "Loss: 2.772606134414673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss, outputs\u001b[38;5;241m.\u001b[39mlogits_per_image\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoss:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(input_ids=batch['input_ids'], \n",
    "                        token_type_ids=batch['token_type_ids'],\n",
    "                        attention_mask=batch['attention_mask'], \n",
    "                        pixel_values=batch['pixel_values'],\n",
    "                       return_loss = True)\n",
    "        \n",
    "        labels = batch['labels']\n",
    "        loss, logits = outputs.loss, outputs.logits_per_image\n",
    "        print(\"Loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(f\"Validation: {epoch}\")\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'], \n",
    "                        token_type_ids=batch['token_type_ids'],\n",
    "                        attention_mask=batch['attention_mask'], \n",
    "                        pixel_values=batch['pixel_values'],\n",
    "                       return_loss = True)\n",
    "            labels = batch['labels']\n",
    "            loss, logits = outputs.loss, outputs.logits_per_image\n",
    "            print(\"Loss:\", loss.item())\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b381f01-9330-4f67-af1f-a3c5979b8a89",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665bfb-84b4-4162-8890-9d1bb84cec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'add your path')\n",
    "torch.save(model, 'add your path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aced4c-dbe0-4bd6-9556-d371a6fcf3cc",
   "metadata": {},
   "source": [
    "## With Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e39ec9-92c0-4a1e-85ac-dd60f4ea485c",
   "metadata": {},
   "source": [
    "This section tries to use a different loss function, but has been unsuccessful with the one hot encoding and categorical labels. Will return to this and try to get it working as the previous model doesn't really learn anything so likely nothing is being updated properly.\n",
    "\n",
    "I think the way forward is to construct the models initiating them from the EncoderModel/Model from Config function (see few sections below for a BERT ensemble example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263d621-0f9b-4f9d-8266-931576d14ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# one hot CE loss if we use one hot labels, this one seems to work but not great\n",
    "\n",
    "def one_hot_ce_loss(outputs, targets):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    _, labels = torch.max(targets, dim=1)\n",
    "    return criterion(outputs, labels)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):  \n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for itr, batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(input_ids=batch['input_ids'], \n",
    "                        token_type_ids=batch['token_type_ids'],\n",
    "                        attention_mask=batch['attention_mask'], \n",
    "                        pixel_values=batch['pixel_values'],\n",
    "                       return_loss = True)\n",
    "        \n",
    "        labels = batch['labels']\n",
    "        int_loss, logits = outputs.loss, outputs.logits_per_image\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        print(\"Loss:\", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     print(f\"Validation: {epoch}\")\n",
    "    #     for batch in tqdm(val_dataloader):\n",
    "    #         batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    #         outputs = model(input_ids=batch['input_ids'], \n",
    "    #                     token_type_ids=batch['token_type_ids'],\n",
    "    #                     attention_mask=batch['attention_mask'], \n",
    "    #                     pixel_values=batch['pixel_values'],\n",
    "    #                    return_loss = True)\n",
    "    #         labels = batch['labels']\n",
    "    #         loss, logits = outputs.loss, outputs.logits_per_image\n",
    "    #         print(\"Loss:\", loss.item())\n",
    "    #         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746ef42-05d1-4c3d-b045-a9c47947423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just experimenting with what comes out of the model for the loss function\n",
    "\n",
    "for itr in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v for k,v in batch.items()}\n",
    "\n",
    "        outputs = model(input_ids=batch['input_ids'], \n",
    "                        token_type_ids=batch['token_type_ids'],\n",
    "                        attention_mask=batch['attention_mask'], \n",
    "                        pixel_values=batch['pixel_values'],\n",
    "                       return_loss = True)\n",
    "        int_loss, logits = outputs.loss, outputs.logits_per_image\n",
    "        #print(logits)\n",
    "        criterion = nn.MSELoss()\n",
    "        labels = itr['labels']\n",
    "        #score = logits.argmax(1)\n",
    "        #print(score)\n",
    "        print(logits)\n",
    "        print(labels)\n",
    "        loss = criterion(logits, labels)\n",
    "        #print(loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498bc14-ddca-476d-8b2f-1cff4e01a1f2",
   "metadata": {},
   "source": [
    "# Some Other Helper Code\n",
    "Aside from the data functions, here's another bit of code that uses a ensemble of Bert models which can be modified for a multimodal task. It's simpler than the previous code and should be easier to use and modify. This is for next sentence prediction but generally you can see how models are constructed in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6c3e3-9f79-4bb7-9c7b-7f959a569f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
    "\t    def __init__(self, config, *args, **kwargs):\n",
    "\t        super().__init__(config)\n",
    "\t\n",
    "\t        self.n_models = kwargs[\"n_models\"]\n",
    "\t\n",
    "\t        self.bert_model_1 = BertModel(config)\n",
    "\t        self.bert_model_2 = BertModel(config)\n",
    "\t\n",
    "\t        self.cls = nn.Linear(self.n_models * self.config.hidden_size, 2)\n",
    "\t        self.init_weights()\n",
    "\t\n",
    "\t    def forward(\n",
    "\t            self,\n",
    "\t            input_ids=None,\n",
    "\t            attention_mask=None,\n",
    "\t            token_type_ids=None,\n",
    "\t            position_ids=None,\n",
    "\t            head_mask=None,\n",
    "\t            inputs_embeds=None,\n",
    "\t            next_sentence_label=None,\n",
    "\t    ):\n",
    "\t        outputs = []\n",
    "            input_ids_1 = input_ids[0]\n",
    "\t        attention_mask_1 = attention_mask[0]\n",
    "\t        token_type_ids_1 = token_type_ids[0]\n",
    "\t        outputs.append(self.bert_model_1(input_ids_1,\n",
    "\t                                         attention_mask=attention_mask_1,\n",
    "\t                                         token_type_ids=token_type_ids_1))\n",
    "\t\n",
    "\t        input_ids_2 = input_ids[1]\n",
    "\t        attention_mask_2 = attention_mask[1]\n",
    "\t        token_type_ids_2 = token_type_ids[1]\n",
    "\t        outputs.append(self.bert_model_2(input_ids_2,\n",
    "\t                                         attention_mask=attention_mask_2,\n",
    "\t                                         token_type_ids=token_type_ids_2))\n",
    "\t\n",
    "\t        # just get the [CLS] embeddings\n",
    "\t        last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
    "\t        logits = self.cls(last_hidden_states)\n",
    "\t\n",
    "\t        # crossentropyloss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "\t        if next_sentence_label is not None:\n",
    "\t            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\t            next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
    "\t            return next_sentence_loss, logits\n",
    "\t        else:\n",
    "\t            return logits\n",
    "\n",
    "self.bert_model_1 = BertModel(config)        \n",
    "self.bert_model_2 = BertModel(config)         \n",
    "self.cls = nn.Linear(self.n_models * self.config.hidden_size, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
