{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9b9c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data, batch_size, bert_model, bert_tokenizer, clip_processor, clip_model, data_type, label_binarizer, max_length=77, is_training=True, labeled=True, image_dir=None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.bert_model = bert_model\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.clip_processor = clip_processor\n",
    "        self.clip_model = clip_model\n",
    "        self.label_binarizer = label_binarizer\n",
    "        self.max_length = max_length\n",
    "        self.indices = np.arange(len(self.data))\n",
    "        self.is_training = is_training\n",
    "        self.labeled = labeled\n",
    "        self.image_dir = data_paths[data_type]['image_dir']  # Get image directory based on data_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.data[k] for k in batch_indices]\n",
    "        \n",
    "        if self.labeled:\n",
    "            X, y = self.preprocess_data(batch)\n",
    "            if self.is_training:\n",
    "                return X, y\n",
    "            else:\n",
    "                batch_ids = [sample[\"id\"] for sample in batch]\n",
    "                return X, y, batch_ids\n",
    "        else:\n",
    "            X = self.preprocess_data(batch, labeled=False)\n",
    "            if not self.is_training:\n",
    "                batch_ids = [sample[\"id\"] for sample in batch]\n",
    "            return X, batch_ids \n",
    "\n",
    "        \n",
    "    def load_image(self, image_path):\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                return img.convert('RGB')\n",
    "        except IOError:\n",
    "            print(f\"Error in loading image: {full_path}. Using a placeholder image.\")\n",
    "            return Image.new('RGB', (224, 224), color='white')\n",
    "\n",
    "    def preprocess_data(self, batch, labeled=True):\n",
    "        default_bert_text = \"NaN\" # a placeholder text for samples without text\n",
    "        texts_bert = [sample.get(\"external_data\", default_bert_text) for sample in batch]\n",
    "        texts_clip = [sample[\"text\"] for sample in batch]\n",
    "        image_filenames = [sample[\"image\"] for sample in batch]\n",
    "        if labeled and self.label_binarizer:\n",
    "            labels = [sample.get(\"labels\", []) for sample in batch]\n",
    "            default_label = ['None']\n",
    "            labels = [label if label else default_label for label in labels]\n",
    "            y = self.label_binarizer.transform(labels)\n",
    "        else:\n",
    "            y = None\n",
    "        \n",
    "        # BERT processing for 'external_data'\n",
    "        inputs_bert = self.bert_tokenizer(texts_bert, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        outputs_bert = self.bert_model(**inputs_bert)\n",
    "        bert_embeddings = outputs_bert.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "        # CLIP processing for 'text' and 'image'\n",
    "        images = [self.load_image(os.path.join(self.image_dir, filename)) for filename in image_filenames]\n",
    "        processed_texts = self.clip_processor(text=texts_clip, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs_text = {k: v.to(self.clip_model.device) for k, v in processed_texts.items()}\n",
    "        text_embeddings = self.clip_model.get_text_features(**inputs_text).cpu().detach().numpy()\n",
    "\n",
    "        processed_images = self.clip_processor(images=images, return_tensors=\"pt\")\n",
    "        inputs_image = {k: v.to(self.clip_model.device) for k, v in processed_images.items()}\n",
    "        image_embeddings = self.clip_model.get_image_features(**inputs_image).cpu().detach().numpy()\n",
    "\n",
    "        combined_embeddings_clip = np.concatenate((text_embeddings, image_embeddings), axis=1)\n",
    "\n",
    "        # Combine BERT and CLIP embeddings\n",
    "        combined_embeddings = np.concatenate((bert_embeddings, combined_embeddings_clip), axis=1)\n",
    "\n",
    "        if labeled:\n",
    "            return combined_embeddings, y\n",
    "        else:\n",
    "            return combined_embeddings\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices) \n",
    "        \n",
    "class EarlyFusion:\n",
    "    def __init__(self, label_tree, data_paths):\n",
    "        self.data_paths = data_paths\n",
    "        self.label_binarizer = MultiLabelBinarizer()\n",
    "\n",
    "        # BERT Initialization\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # CLIP Initialization\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.image_dir = None  # You can set this when loading data\n",
    "\n",
    "    def load_and_preprocess_data(self, data_type, sample_size=None):\n",
    "        # Select the file and image paths based on the data_type\n",
    "        data_info = self.data_paths.get(data_type)\n",
    "        if not data_info:\n",
    "            raise ValueError(f\"Invalid data type: {data_type}\")\n",
    "\n",
    "        json_file_path = data_info.get('json_path')\n",
    "        image_dir = data_info.get('image_dir')\n",
    "\n",
    "        # Load JSON data\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Check if image files exist\n",
    "        for sample in data:\n",
    "            image_path = os.path.join(image_dir, sample.get(\"image\", \"\"))\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image file not found at {image_path}\")\n",
    "\n",
    "        # Handling sample size\n",
    "        if sample_size:\n",
    "            # Ensure that the sample size is not larger than the dataset\n",
    "            sample_size = min(sample_size, len(data))\n",
    "            data = np.random.choice(data, sample_size, replace=False).tolist()\n",
    "\n",
    "        # Handling labels differently based on data_type\n",
    "        if data_type == 'test':\n",
    "            # For 'test' data, labels may not be present\n",
    "            labels = [sample.get(\"labels\", None) for sample in data]\n",
    "            # Keep only samples with labels (filter out None)\n",
    "            labels = [label for label in labels if label is not None]\n",
    "        else:\n",
    "            # For 'train' and 'dev' data, assign a default label if none exist\n",
    "            default_label = ['None']\n",
    "            labels = [sample.get(\"labels\", []) for sample in data]\n",
    "            labels = [label if label else default_label for label in labels]\n",
    "\n",
    "        self.label_binarizer.fit(labels)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def explore_data(self, sample_size=None, data_type='train', examples_to_show=2):\n",
    "        data = self.load_and_preprocess_data(data_type, sample_size)\n",
    "\n",
    "        print(f\"Total number of samples: {len(data)}\")\n",
    "        print(f\"Total number of unique labels: {len(self.label_binarizer.classes_)}\")\n",
    "        print(\"Unique labels:\", self.label_binarizer.classes_)\n",
    "\n",
    "        data_generator = DataGenerator(data, batch_size=1, \n",
    "                                       bert_model=self.bert_model, \n",
    "                                       bert_tokenizer=self.bert_tokenizer,\n",
    "                                       clip_processor=self.clip_processor, \n",
    "                                       clip_model=self.clip_model, \n",
    "                                       image_dir=self.image_dir, \n",
    "                                       label_binarizer=self.label_binarizer,\n",
    "                                       data_type=data_type)\n",
    "\n",
    "        for i in range(examples_to_show):\n",
    "            sample = [data[i]]  # Wrap the single sample in a list\n",
    "            print(f\"\\nSample {i+1} Original Labels:\", sample[0].get(\"labels\", []))\n",
    "\n",
    "            _, labels = data_generator.preprocess_data(sample, labeled=True)\n",
    "            print(\"Binarized Labels:\", labels)\n",
    "            combined_embeddings, _ = data_generator.preprocess_data(sample, labeled=True)\n",
    "\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(\"Text:\", sample[0][\"text\"])\n",
    "            print(\"Image:\", sample[0][\"image\"])\n",
    "            print(\"Combined Embedding Shape:\", combined_embeddings.shape)\n",
    "            print(\"Combined Embedding:\", combined_embeddings)\n",
    "\n",
    "    def build_model(self, num_classes, embedding_size=1792, dropout_rate=0.05, learning_rate=0.01):\n",
    "        # Adjust the input size based on the combined embeddings of BERT and CLIP\n",
    "        input_layer = Input(shape=(embedding_size,), dtype='float32', name=\"input\")\n",
    "        dense_layer = Dense(256, activation='relu')(input_layer)\n",
    "        dropout_layer = Dropout(dropout_rate)(dense_layer)\n",
    "        output_layer = Dense(num_classes, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "        self.model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, save_model_path, batch_size=34, epochs=15, data_type='train', validation_size=0.2, learning_rate=0.01, random_state=42, sample_size=None, is_training=True):\n",
    "        data = self.load_and_preprocess_data(data_type, sample_size)\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X_train_indices, X_val_indices = train_test_split(\n",
    "            range(len(data)), test_size=validation_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Generate training and validation data using indices\n",
    "        train_data = [data[i] for i in X_train_indices]\n",
    "        val_data = [data[i] for i in X_val_indices]\n",
    "\n",
    "        # Initialize data generators\n",
    "        train_generator = DataGenerator(train_data, batch_size, \n",
    "                                        self.bert_model, \n",
    "                                        self.bert_tokenizer,\n",
    "                                        self.clip_processor, \n",
    "                                        self.clip_model,\n",
    "                                        data_type,\n",
    "                                        self.label_binarizer,\n",
    "                                        is_training=True\n",
    "                                        )\n",
    "        \n",
    "        val_generator = DataGenerator(val_data, batch_size, \n",
    "                                        self.bert_model, \n",
    "                                        self.bert_tokenizer,\n",
    "                                        self.clip_processor, \n",
    "                                        self.clip_model,\n",
    "                                        data_type,\n",
    "                                        self.label_binarizer,\n",
    "                                        is_training=True\n",
    "                                        )\n",
    "        \n",
    "        for sample_batch in train_generator:\n",
    "            print(\"Sample Input Shape:\", sample_batch[0].shape, \"Type:\", sample_batch[0].dtype)\n",
    "            print(\"Sample Labels Shape:\", sample_batch[1].shape, \"Type:\", sample_batch[1].dtype)\n",
    "            break \n",
    "    \n",
    "        tf.debugging.enable_check_numerics()\n",
    "        # Build the model\n",
    "        self.build_model(num_classes=len(self.label_binarizer.classes_), dropout_rate=0.05, learning_rate=learning_rate)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            train_generator, epochs=epochs, validation_data=val_generator, callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        self.model.save_weights(save_model_path)\n",
    "        print(f\"Model saved at {save_model_path}\")\n",
    "\n",
    "        return history\n",
    "    \n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        #plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        #plot loss \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()      \n",
    "\n",
    "    def calculate_hierarchy_distance(self, node1, node2):\n",
    "        def find_path(tree, node, path=[]):\n",
    "            if node in tree:\n",
    "                return path + [node]\n",
    "            for k, v in tree.items():\n",
    "                if isinstance(v, dict):\n",
    "                    new_path = find_path(v, node, path + [k])\n",
    "                    if new_path:\n",
    "                        return new_path\n",
    "            return []\n",
    "\n",
    "        node1_tuple = (node1,) if isinstance(node1, str) else node1\n",
    "        node2_tuple = (node2,) if isinstance(node2, str) else node2\n",
    "\n",
    "        path1 = find_path(self.label_tree, node1_tuple)\n",
    "        path2 = find_path(self.label_tree, node2_tuple)\n",
    "\n",
    "        common_length = len(set(path1) & set(path2))\n",
    "        distance = len(path1) + len(path2) - 2 * common_length\n",
    "        \n",
    "        return distance\n",
    "\n",
    "    def evaluate_model(self, batch_size, save_model_path, num_classes, output_json_path, data_type='dev', learning_rate=0.005):\n",
    "        # Load and preprocess test data\n",
    "        dev_data = self.load_and_preprocess_data(data_type)\n",
    "        test_generator = DataGenerator(dev_data, batch_size, \n",
    "                                       bert_model=self.bert_model, \n",
    "                                       bert_tokenizer=self.bert_tokenizer,\n",
    "                                       clip_processor=self.clip_processor, \n",
    "                                       clip_model=self.clip_model, \n",
    "                                       vision_image_data_path=self.vision_image_data_path, \n",
    "                                       label_binarizer=self.label_binarizer,\n",
    "                                       is_training=False)\n",
    "\n",
    "        # Build the model and load saved weights\n",
    "        self.build_model(num_classes=len(self.label_binarizer.classes_), dropout_rate=0.5, learning_rate=learning_rate)\n",
    "        self.model.load_weights(save_model_path)\n",
    "\n",
    "        # Initialize variables for metrics calculation\n",
    "        total_precision = 0\n",
    "        total_recall = 0\n",
    "        total_samples = 0\n",
    "        true_labels_all = []\n",
    "        predicted_labels_all = []\n",
    "\n",
    "        # Initialize MultiLabelBinarizer\n",
    "        mlb = MultiLabelBinarizer(classes=self.label_binarizer.classes_)\n",
    "        mlb.fit([self.label_binarizer.classes_])\n",
    "\n",
    "        results = []  # List to store results\n",
    "\n",
    "        # Iterate over batches in the test generator\n",
    "        for X, y_true in test_generator:\n",
    "            y_pred = self.model.predict(X)\n",
    "\n",
    "            # Iterate over predictions in the batch\n",
    "            for prediction, true_label in zip(y_pred, y_true):\n",
    "                gold_labels = [self.label_binarizer.classes_[j] for j in range(len(self.label_binarizer.classes_)) if true_label[j] == 1]\n",
    "                predicted_labels = [self.label_binarizer.classes_[j] for j in range(len(self.label_binarizer.classes_)) if prediction[j] > 0.5]\n",
    "                prediction_list = prediction.tolist()\n",
    "\n",
    "                label_probabilities = {label: float(prob) for label, prob in zip(self.label_binarizer.classes_, prediction_list)}\n",
    "\n",
    "                true_labels_all.append(gold_labels)\n",
    "                predicted_labels_all.append(predicted_labels)\n",
    "\n",
    "                results.append({\n",
    "                    'true_labels': gold_labels,\n",
    "                    'predicted_labels': predicted_labels,\n",
    "                    'predicted_probabilities': label_probabilities\n",
    "                })\n",
    "\n",
    "                # Calculate precision and recall\n",
    "                for predicted_label in predicted_labels:\n",
    "                    if predicted_label in gold_labels:\n",
    "                        total_precision += 1\n",
    "                        total_recall += 1\n",
    "\n",
    "                total_samples += 1\n",
    "\n",
    "        # Aggregate metrics over all samples\n",
    "        average_precision = total_precision / total_samples if total_samples > 0 else 0\n",
    "        average_recall = total_recall / total_samples if total_samples > 0 else 0\n",
    "        hierarchical_f1 = 2 * (average_precision * average_recall) / (average_precision + average_recall) if (average_precision + average_recall) != 0 else 0\n",
    "\n",
    "        true_labels_all_binary = mlb.transform(true_labels_all)\n",
    "        predicted_labels_all_binary = mlb.transform(predicted_labels_all)\n",
    "        target_names = self.label_binarizer.classes_\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(true_labels_all_binary, predicted_labels_all_binary, target_names=target_names))\n",
    "\n",
    "        # Save results to JSON file\n",
    "        result_file_name = f\"evaluation_results_{data_type}.json\"\n",
    "        destination_path = os.path.join(output_json_path, result_file_name)\n",
    "        os.makedirs(output_json_path, exist_ok=True)\n",
    "        with open(destination_path, 'w') as json_file:\n",
    "            json.dump(results, json_file, indent=4)\n",
    "\n",
    "        return hierarchical_f1\n",
    "  \n",
    "    def test_model(self, batch_size, save_model_path, output_json_path, data_type='test', learning_rate=0.001):\n",
    "        test_data = self.load_and_preprocess_data(data_type)\n",
    "        test_generator = DataGenerator(test_data, batch_size, \n",
    "                                       bert_model=self.bert_model, \n",
    "                                       bert_tokenizer=self.bert_tokenizer,\n",
    "                                       clip_processor=self.clip_processor, \n",
    "                                       clip_model=self.clip_model, \n",
    "                                       vision_image_data_path=self.vision_image_data_path, \n",
    "                                       label_binarizer=self.label_binarizer,\n",
    "                                       is_training=False, labeled=False)\n",
    "\n",
    "        # Load the trained model weights\n",
    "        self.build_model(num_classes=len(self.label_binarizer.classes_), dropout_rate=0.5, learning_rate=learning_rate)\n",
    "        self.model.load_weights(save_model_path)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        # Iterate over batches in the test generator\n",
    "        for X, batch_ids in test_generator:\n",
    "            y_pred = self.model.predict(X)\n",
    "\n",
    "            # Iterate over predictions in the batch\n",
    "            for sample_id, prediction in zip(batch_ids, y_pred):\n",
    "                predicted_labels = [self.label_binarizer.classes_[j] for j in range(len(self.label_binarizer.classes_)) if prediction[j] > 0.001]\n",
    "                prediction_list = prediction.tolist()\n",
    "\n",
    "                label_probabilities = {label: float(prob) for label, prob in zip(self.label_binarizer.classes_, prediction_list)}\n",
    "                \n",
    "                predictions.append({\n",
    "                    'id': sample_id,\n",
    "                    'predicted_labels': predicted_labels,\n",
    "                    'predicted_probabilities': label_probabilities\n",
    "                })\n",
    "\n",
    "        # Save predictions to a JSON file\n",
    "        result_file_name = \"subtask2a_test_pred.json\"\n",
    "        destination_path = os.path.join(output_json_path, result_file_name)\n",
    "        os.makedirs(output_json_path, exist_ok=True)\n",
    "        with open(destination_path, 'w') as json_file:\n",
    "            json.dump(predictions, json_file, indent=4)\n",
    "\n",
    "        return \"Predictions completed.\"\n",
    "    \n",
    "#hierarchical tree\n",
    "#the assigned number are hypothetically\n",
    "label_tree = {\n",
    "    'Persuasion': {\n",
    "        'Pathos': {\n",
    "            'Appeal to Emotion(visual)': 1,\n",
    "            'Exaggeration/Minimisation': 2,\n",
    "            'Loaded Language': 3,\n",
    "            'Flag waving': 4,\n",
    "            'Appeal to fear/prejudice': 5,\n",
    "            'Transfer': 6\n",
    "        },\n",
    "        'Ethos': {\n",
    "            'Transfer': 6,\n",
    "            'Glittering generalities': 7,\n",
    "            'Appeal to authority': 8,\n",
    "            'Bandwagon': 9,\n",
    "            'Ad Hominem': {\n",
    "                'Name calling/Labelling': 10,\n",
    "                'Doubt': 11,\n",
    "                'Smears': 12,\n",
    "                'Reduction and Hitlerium': 13,\n",
    "                'Whataboutism': 14\n",
    "            }\n",
    "        },\n",
    "        'Logos': {\n",
    "            'Repetition': 15,\n",
    "            'Obfuscation, Intentional vagueness, Confusion': 16,\n",
    "            'Justification': {\n",
    "                'Flag waving': 4,\n",
    "                'Appeal to fear/prejudice': 5,\n",
    "                'Appeal to Authority': 8,\n",
    "                'Bandwagon': 9,\n",
    "                'Slogans': 17\n",
    "            },\n",
    "            'Reasoning': {\n",
    "                'Distraction': {\n",
    "                    'Whataboutism': 14,\n",
    "                    'Presenting Irrelevant Data (Red Herring)': 18,\n",
    "                    'Straw Man': 19\n",
    "                },\n",
    "                'Simplification': {\n",
    "                    'Black-and-white Fallacy/Dictatorship': 20,\n",
    "                    'Casual Oversimplification': 21,\n",
    "                    'Thought-terminating cliché': 22\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'None': 23 # a label for empty samples\n",
    "    }\n",
    "}\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 23\n",
    "output_json_path = '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/dev_gold_labels'\n",
    "save_model_path = '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/subtask2a/CLIP_model_weights'\n",
    "\n",
    "data_paths = {\n",
    "    'train': {\n",
    "        'json_path': '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/subtask2a/merged_data_final.json',\n",
    "        'image_dir': '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/subtask2a/merged_images'\n",
    "\n",
    "    },\n",
    "    'dev': {\n",
    "        'json_path': '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/dev_gold_labels/dev_subtask2a_en.json',\n",
    "        'image_dir': '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/subtask2a/subtask2a_images/dev_images'\n",
    "    },\n",
    "    'test': {\n",
    "        'json_path': '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/test_data/english/en_subtask2a_test_unlabeled.json',\n",
    "        'image_dir': '/Users/jamessmith/Desktop/Desktop/SemEval_Task/data/test_data/test_images/subtask1_2a/english'\n",
    "    }\n",
    "}\n",
    "\n",
    "EarlyFusion_classifier2a = EarlyFusion(label_tree, data_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef94dc3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 7500\n",
      "Total number of unique labels: 23\n",
      "Unique labels: ['Appeal to (Strong) Emotions' 'Appeal to authority'\n",
      " 'Appeal to fear/prejudice' 'Bandwagon'\n",
      " 'Black-and-white Fallacy/Dictatorship' 'Causal Oversimplification'\n",
      " 'Doubt' 'Exaggeration/Minimisation' 'Flag-waving'\n",
      " 'Glittering generalities (Virtue)' 'Loaded Language'\n",
      " \"Misrepresentation of Someone's Position (Straw Man)\"\n",
      " 'Name calling/Labeling' 'None'\n",
      " 'Obfuscation, Intentional vagueness, Confusion'\n",
      " 'Presenting Irrelevant Data (Red Herring)' 'Reductio ad hitlerum'\n",
      " 'Repetition' 'Slogans' 'Smears' 'Thought-terminating cliché' 'Transfer'\n",
      " 'Whataboutism']\n",
      "\n",
      "Sample 1 Original Labels: ['Causal Oversimplification', 'Transfer', 'Flag-waving']\n",
      "Binarized Labels: [[0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "\n",
      "Sample 1:\n",
      "Text: This is why we're free\\n\\nThis is why we're safe\\n\n",
      "Image: prop_meme_556.png\n",
      "Combined Embedding Shape: (1, 1792)\n",
      "Combined Embedding: [[-0.01492107  0.15178593  0.5006324  ... -0.6260278  -0.4458599\n",
      "   0.36731935]]\n",
      "\n",
      "Sample 2 Original Labels: ['Transfer', 'Black-and-white Fallacy/Dictatorship', 'Smears']\n",
      "Binarized Labels: [[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]]\n",
      "\n",
      "Sample 2:\n",
      "Text: THIS IS WHY YOU NEED\\n\\nA SHARPIE WITH YOU AT ALL TIMES\n",
      "Image: prop_meme_4839.png\n",
      "Combined Embedding Shape: (1, 1792)\n",
      "Combined Embedding: [[ 0.5535194   0.49418992 -0.07969636 ...  0.00429224  0.06807035\n",
      "  -0.12142507]]\n"
     ]
    }
   ],
   "source": [
    "#explore the data\n",
    "EarlyFusion_classifier2a.explore_data(data_type='train', \n",
    "                                    examples_to_show=2\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input Shape: (64, 1792) Type: float32\n",
      "Sample Labels Shape: (64, 23) Type: int64\n",
      "INFO:tensorflow:Enabled check-numerics callback in thread MainThread\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabled check-numerics callback in thread MainThread\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/94 [====>.........................] - ETA: 54:36 - loss: 0.3393 - accuracy: 0.2075"
     ]
    }
   ],
   "source": [
    "history= EarlyFusion_classifier2a.train_model(save_model_path, \n",
    "                                          batch_size=64, \n",
    "                                          epochs=10,\n",
    "                                          learning_rate=0.01,\n",
    "                                          data_type='train'\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training history\n",
    "EarlyFusion_classifier2a.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model using the dev data\n",
    "hierarchical_f1 = EarlyFusion_classifier2a.evaluate_model(batch_size, \n",
    "                                                      save_model_path, \n",
    "                                                      num_classes, \n",
    "                                                      output_json_path, \n",
    "                                                      data_type='dev'\n",
    "                                                     )\n",
    "print(f\"Average Hierarchical F1: {hierarchical_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d88ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85610c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
