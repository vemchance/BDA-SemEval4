{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3f1205-57f7-4108-b7a4-554b4ed12c5f",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- If a label has a poor confidence, can we swap to predict the next label in the hiearchy instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1d442-797d-424b-8b59-528578cf1e29",
   "metadata": {},
   "source": [
    "# Calculations for Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1718d816-b8ac-4610-8d51-8ce17358c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import networkx\n",
    "from networkx import DiGraph, relabel_nodes, all_pairs_shortest_path_length\n",
    "from sklearn_hierarchical_classification.constants import ROOT\n",
    "from sklearn_hierarchical_classification.metrics import h_fbeta_score, h_recall_score, h_precision_score, fill_ancestors, multi_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2d11a9-4007-4936-970f-f23437942b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = DiGraph()\n",
    "G.add_edge(ROOT, \"Logos\")\n",
    "G.add_edge(\"Logos\", \"Repetition\")\n",
    "G.add_edge(\"Logos\", \"Obfuscation, Intentional vagueness, Confusion\")\n",
    "G.add_edge(\"Logos\", \"Reasoning\")\n",
    "G.add_edge(\"Logos\", \"Justification\")\n",
    "G.add_edge('Justification', \"Slogans\")\n",
    "G.add_edge('Justification', \"Bandwagon\")\n",
    "G.add_edge('Justification', \"Appeal to authority\")\n",
    "G.add_edge('Justification', \"Flag-waving\")\n",
    "G.add_edge('Justification', \"Appeal to fear/prejudice\")\n",
    "G.add_edge('Reasoning', \"Simplification\")\n",
    "G.add_edge('Simplification', \"Causal Oversimplification\")\n",
    "G.add_edge('Simplification', \"Black-and-white Fallacy/Dictatorship\")\n",
    "G.add_edge('Simplification', \"Thought-terminating cliché\")\n",
    "G.add_edge('Reasoning', \"Distraction\")\n",
    "G.add_edge('Distraction', \"Misrepresentation of Someone's Position (Straw Man)\")\n",
    "G.add_edge('Distraction', \"Presenting Irrelevant Data (Red Herring)\")\n",
    "G.add_edge('Distraction', \"Whataboutism\")\n",
    "G.add_edge(ROOT, \"Ethos\")\n",
    "G.add_edge('Ethos', \"Appeal to authority\")\n",
    "G.add_edge('Ethos', \"Glittering generalities (Virtue)\")\n",
    "G.add_edge('Ethos', \"Bandwagon\")\n",
    "G.add_edge('Ethos', \"Ad Hominem\")\n",
    "G.add_edge('Ethos', \"Transfer\")\n",
    "G.add_edge('Ad Hominem', \"Doubt\")\n",
    "G.add_edge('Ad Hominem', \"Name calling/Labeling\")\n",
    "G.add_edge('Ad Hominem', \"Smears\")\n",
    "G.add_edge('Ad Hominem', \"Reductio ad hitlerum\")\n",
    "G.add_edge('Ad Hominem', \"Whataboutism\")\n",
    "G.add_edge(ROOT, \"Pathos\")\n",
    "G.add_edge('Pathos', \"Exaggeration/Minimisation\")\n",
    "G.add_edge('Pathos', \"Loaded Language\")\n",
    "G.add_edge('Pathos', \"Appeal to (Strong) Emotions\")\n",
    "G.add_edge('Pathos', \"Appeal to fear/prejudice\")\n",
    "G.add_edge('Pathos', \"Flag-waving\")\n",
    "G.add_edge('Pathos', \"Transfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b8b7a97-9e54-4425-a70e-b9d533bd679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classes_from_graph(graph):\n",
    "    return [\n",
    "        node\n",
    "        for node in graph.nodes\n",
    "        if node != ROOT\n",
    "        ]\n",
    "\n",
    "def _h_fbeta_score(y_true, y_pred, class_hierarchy, beta=1., root=ROOT):\n",
    "    hP = _h_precision_score(y_true, y_pred, class_hierarchy, root=root)\n",
    "    hR = _h_recall_score(y_true, y_pred, class_hierarchy, root=root)\n",
    "    return (1. + beta ** 2.) * hP * hR / (beta ** 2. * hP + hR)\n",
    "    \n",
    "def _fill_ancestors(y, graph, root, copy=True):\n",
    "    y_ = y.copy() if copy else y\n",
    "    paths = all_pairs_shortest_path_length(graph.reverse(copy=False))\n",
    "    for target, distances in paths:\n",
    "        if target == root:\n",
    "            continue\n",
    "        ix_rows = np.where(y[:, target] > 0)[0]\n",
    "        ancestors = list(filter(lambda x: x != ROOT,distances.keys()))\n",
    "        y_[tuple(np.meshgrid(ix_rows, ancestors))] = 1\n",
    "    graph.reverse(copy=False)\n",
    "    return y_\n",
    "    \n",
    "def _h_recall_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
    "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
    "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
    "\n",
    "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
    "\n",
    "    true_positives = len(ix[0])\n",
    "    all_positives = np.count_nonzero(y_true_)\n",
    "\n",
    "    return true_positives / all_positives\n",
    "\n",
    "def _h_precision_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
    "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
    "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
    "\n",
    "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
    "\n",
    "    true_positives = len(ix[0])\n",
    "    all_results = np.count_nonzero(y_pred_)\n",
    "\n",
    "    return true_positives / all_results\n",
    "\n",
    "def evaluate_h(gold, pred):\n",
    "    with multi_labeled(gold, pred, G) as (gold_, pred_, graph_):\n",
    "        return  _h_precision_score(gold_, pred_,graph_), _h_recall_score(gold_, pred_,graph_), _h_fbeta_score(gold_, pred_,graph_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf0ceb-7895-4c41-bcf1-083244fe8b69",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c06bed-4b37-45e1-9814-ecc003e7973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Labels: 20\n",
      "Label Names: {'Smears', 'Appeal to fear/prejudice', 'Name calling/Labeling', 'Bandwagon', 'Reductio ad hitlerum', 'Loaded Language', 'Causal Oversimplification', 'Slogans', 'Doubt', 'Exaggeration/Minimisation', 'Glittering generalities (Virtue)', 'Whataboutism', 'Repetition', 'Appeal to authority', 'Thought-terminating cliché', 'Presenting Irrelevant Data (Red Herring)', 'Black-and-white Fallacy/Dictatorship', 'Flag-waving', \"Misrepresentation of Someone's Position (Straw Man)\", 'Obfuscation, Intentional vagueness, Confusion'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "json_file_path = r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask1\\train.json'\n",
    "json_file_path_2 = r'X:\\PhD\\SemEval Task4\\Data\\annotations\\data\\subtask1\\validation.json'\n",
    "\n",
    "# Swap the file opening and data loading statements\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "    data1 = json.load(file)\n",
    "\n",
    "with open(json_file_path_2, 'r', encoding='utf-8') as file2:\n",
    "    data2 = json.load(file2)\n",
    "\n",
    "data = [{**d1, **d2} for d1, d2 in zip(data1, data2)]\n",
    "\n",
    "labels = [sample.get(\"labels\", []) for sample in data]\n",
    "\n",
    "# lists to get all labels\n",
    "all_labels = [label for sublist in labels for label in sublist]\n",
    "\n",
    "num_unique_labels = len(set(all_labels))\n",
    "print(f\"Number of Unique Labels: {num_unique_labels}\")\n",
    "print(\"Label Names:\", set(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8799eb2-68da-4c68-bc09-ec8402bc1e51",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c314556-58fa-4679-8f6b-cba4cfda9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "class MemeClassification:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "        self.model = None\n",
    "\n",
    "    def load_data(self, json_file_path, json_file_path_2):\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data1 = json.load(file)\n",
    "\n",
    "        with open(json_file_path_2, 'r', encoding='utf-8') as file2:\n",
    "            data2 = json.load(file2)\n",
    "        data = [{**d1, **d2} for d1, d2 in zip(data1, data2)]\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        texts = [sample[\"text\"] for sample in data]\n",
    "        labels = [sample.get(\"labels\", []) for sample in data]\n",
    "        \n",
    "        #checks if labels are empty and assign a default label \n",
    "        default_label = ['None']  \n",
    "        labels = [label if label else default_label for label in labels]\n",
    "\n",
    "        #converting labels to one hot encoded\n",
    "        if any(isinstance(label, list) for label in labels):\n",
    "            self.label_binarizer = MultiLabelBinarizer()\n",
    "            y = self.label_binarizer.fit_transform(labels)\n",
    "        else:\n",
    "            y = np.array(labels)\n",
    "        return texts, y\n",
    "\n",
    "    def tokenize_and_pad(self, texts):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for text in texts:\n",
    "            encoded_text = self.tokenizer(text, max_length=128, truncation=True, padding='max_length', return_tensors='tf')\n",
    "            input_ids.append(encoded_text['input_ids'])\n",
    "            attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "        input_ids = np.concatenate(input_ids, axis=0)\n",
    "        attention_masks = np.concatenate(attention_masks, axis=0)\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "    def build_model(self, num_classes=21):\n",
    "        input_ids_input = Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "        attention_masks_input = Input(shape=(128,), dtype=tf.int32, name=\"attention_masks\")\n",
    "\n",
    "        bert_output = self.bert_model(input_ids_input, attention_mask=attention_masks_input)[0]\n",
    "        pooled_output = GlobalAveragePooling1D()(bert_output)\n",
    "        drop = tf.keras.layers.Dropout(0.4)(pooled_output)\n",
    "        output_layer = Dense(num_classes, activation='sigmoid')(drop)\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(lr=1e-5,\n",
    "                                    decay=0.01,\n",
    "                                    beta_1=0.9,\n",
    "                                    beta_2=0.999,\n",
    "                                    epsilon=1e-07,)\n",
    "\n",
    "        self.model = Model(inputs=[input_ids_input, attention_masks_input], outputs=output_layer)\n",
    "        self.model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def train_model(self, json_file_path, json_file_path2, num_classes=21, epochs=3, batch_size=8, test_size=0.3, random_state=42):\n",
    "        data = self.load_data(json_file_path, json_file_path_2)\n",
    "        texts, y = self.preprocess_data(data)\n",
    "        input_ids, attention_masks = self.tokenize_and_pad(texts)\n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_temp, y_train, y_temp, attention_masks_train, attention_masks_temp = train_test_split(\n",
    "            input_ids, y, attention_masks, test_size=test_size, random_state=random_state, shuffle=True\n",
    "        )\n",
    "\n",
    "        X_val, X_test, y_val, y_test, attention_masks_val, attention_masks_test = train_test_split(\n",
    "            X_temp, y_temp, attention_masks_temp, test_size=0.1, random_state=random_state, shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"Shapes:\")\n",
    "        print(\"X_train:\", X_train.shape)\n",
    "        print(\"y_train:\", y_train.shape)\n",
    "        print(\"X_val:\", X_val.shape)\n",
    "        print(\"y_val:\", y_val.shape)\n",
    "        \n",
    "        # Build and compile the model\n",
    "        self.build_model(num_classes)\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            [X_train, attention_masks_train],\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=([X_val, attention_masks_val], y_val),\n",
    "        )\n",
    "        \n",
    "        return history, X_test, attention_masks_test, y_test\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        #plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        #plot loss \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()      \n",
    "\n",
    "    def evaluate_with_hierarchy(self, X_test, attention_masks_test, y_test):\n",
    "        print(\"X_test:\", X_test.shape)\n",
    "        print(\"y_test:\", y_test.shape)\n",
    "\n",
    "        predictions = self.model.predict([X_test, attention_masks_test])\n",
    "\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "        total_samples = len(y_test)\n",
    "\n",
    "        predicted_labels_all = []\n",
    "        true_labels_all = []\n",
    "\n",
    "        for i in range(total_samples):\n",
    "            predicted_probabilities = predictions[i]\n",
    "\n",
    "            #decoding predicted labels\n",
    "            \n",
    "            predicted_labels = [self.label_binarizer.classes_[j] for j in range(len(self.label_binarizer.classes_)) \n",
    "                                 if predicted_probabilities[j] > 0.5]\n",
    "            gold_labels = [self.label_binarizer.classes_[j] for j in range(len(self.label_binarizer.classes_)) if\n",
    "                           y_test[i][j] == 1]\n",
    "\n",
    "       \n",
    "\n",
    "            predicted_labels_all.append(predicted_labels)\n",
    "            true_labels_all.append(gold_labels)\n",
    "    \n",
    "        true_labels_all_binary = self.label_binarizer.transform(true_labels_all)\n",
    "        predicted_labels_all_binary = self.label_binarizer.transform(predicted_labels_all)\n",
    "        \n",
    "        precision, recall, f1 = evaluate_h(true_labels_all, predicted_labels_all)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(true_labels_all_binary, predicted_labels_all_binary, target_names=self.label_binarizer.classes_))\n",
    "        print(f'Average Precision: {precision}, Average Recall: {recall}, Average F1: {f1}')\n",
    "\n",
    "        return precision, recall, f1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa493daf-a59c-43ef-908f-d6281ec5063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Texts: ['Critical Thinking Essentials\\\\n\\\\Are my biases affecting how I examine the issue?\\\\\\\\n\\\\n\\\\Am I using information that can be verified with reliable data?\\\\\\\\n\\\\n\\\\Am I basing my position on what I KNOW to be the truth, or what I WANT to be the truth?\\\\\\\\n\\\\n\\\\I might be wrong.\\\\ (A little humility goes a long way.)\\\\n']\n",
      "Processed Labels: [[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "texts, y = meme_classifier.preprocess_data(data)\n",
    "print(\"Processed Texts:\", texts[:1]) \n",
    "print(\"Processed Labels:\", y[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82986be9-aa92-4d9a-aa14-1f5aeca239a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: (500, 128)\n",
      "Attention Masks: (500, 128)\n",
      "Example 1:\n",
      "Tokenized Input IDs: [  101  4187  3241  6827  2015  1032  1050  1032  2024  2026 13827  2229\n",
      " 12473  2129  1045 11628  1996  3277  1029  1032  1032  1050  1032  1050\n",
      "  1032  2572  1045  2478  2592  2008  2064  2022 20119  2007 10539  2951\n",
      "  1029  1032  1032  1050  1032  1050  1032  2572  1045  6403  2290  2026\n",
      "  2597  2006  2054  1045  2113  2000  2022  1996  3606  1010  2030  2054\n",
      "  1045  2215  2000  2022  1996  3606  1029  1032  1032  1050  1032  1050\n",
      "  1032  1045  2453  2022  3308  1012  1032  1006  1037  2210 14910 15148\n",
      "  3632  1037  2146  2126  1012  1007  1032  1050   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "Attention Masks: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "Example 2:\n",
      "Tokenized Input IDs: [  101  2667  2000  2228  1997  1037  2309 24718  1012  1012  1012   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "Attention Masks: [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "\n",
      "Example 3:\n",
      "Tokenized Input IDs: [  101  5971  3791  2017  2000  2424  1996  4489  2090  2023  3861  1998\n",
      "  2023  3861  1032  1050  1032 23961 14844  1005  2128  1996  2168  3861\n",
      "  1032  1050   102     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "Attention Masks: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad\n",
    "input_ids, attention_masks = meme_classifier.tokenize_and_pad(texts)\n",
    "print(\"Tokenized Input IDs:\", input_ids.shape)  \n",
    "print(\"Attention Masks:\", attention_masks.shape)\n",
    "# Display a few examples\n",
    "num_examples = 3\n",
    "for i in range(num_examples): \n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(\"Tokenized Input IDs:\", input_ids[i])\n",
    "    print(\"Attention Masks:\", attention_masks[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f93319d6-171c-4cbc-9b5f-6a4206318ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X_train: (350, 128)\n",
      "y_train: (350, 21)\n",
      "X_val: (135, 128)\n",
      "y_val: (135, 21)\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_11/bert/pooler/dense/kernel:0', 'tf_bert_model_11/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "44/44 [==============================] - 16s 177ms/step - loss: 5.4082 - accuracy: 0.1057 - val_loss: 5.2971 - val_accuracy: 0.1037\n",
      "Epoch 2/3\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 5.4330 - accuracy: 0.1543 - val_loss: 5.6832 - val_accuracy: 0.1111\n",
      "Epoch 3/3\n",
      "44/44 [==============================] - 5s 117ms/step - loss: 5.2691 - accuracy: 0.1686 - val_loss: 5.5973 - val_accuracy: 0.1407\n"
     ]
    }
   ],
   "source": [
    "meme_classifier = MemeClassification()\n",
    "history, X_test, attention_masks_test, y_test = meme_classifier.train_model(json_file_path, json_file_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d291f52-2594-4e88-ac1d-a8e2a1d6f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test: (15, 128)\n",
      "y_test: (15, 21)\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "Classification Report:\n",
      "                                                     precision    recall  f1-score   support\n",
      "\n",
      "                                Appeal to authority       0.13      1.00      0.24         2\n",
      "                           Appeal to fear/prejudice       0.00      0.00      0.00         0\n",
      "                                          Bandwagon       0.50      0.50      0.50         2\n",
      "               Black-and-white Fallacy/Dictatorship       0.07      1.00      0.12         1\n",
      "                          Causal Oversimplification       0.00      0.00      0.00         0\n",
      "                                              Doubt       0.00      0.00      0.00         0\n",
      "                          Exaggeration/Minimisation       0.13      1.00      0.24         2\n",
      "                                        Flag-waving       0.13      1.00      0.24         2\n",
      "                   Glittering generalities (Virtue)       0.07      1.00      0.12         1\n",
      "                                    Loaded Language       0.27      1.00      0.42         4\n",
      "Misrepresentation of Someone's Position (Straw Man)       0.00      0.00      0.00         0\n",
      "                              Name calling/Labeling       0.20      1.00      0.33         3\n",
      "                                               None       0.20      1.00      0.33         3\n",
      "      Obfuscation, Intentional vagueness, Confusion       0.00      0.00      0.00         0\n",
      "           Presenting Irrelevant Data (Red Herring)       0.00      0.00      0.00         0\n",
      "                               Reductio ad hitlerum       0.00      0.00      0.00         0\n",
      "                                         Repetition       0.07      1.00      0.12         1\n",
      "                                            Slogans       0.07      1.00      0.12         1\n",
      "                                             Smears       0.40      1.00      0.57         6\n",
      "                         Thought-terminating cliché       0.07      1.00      0.12         1\n",
      "                                       Whataboutism       0.00      0.00      0.00         0\n",
      "\n",
      "                                          micro avg       0.11      0.97      0.20        29\n",
      "                                          macro avg       0.11      0.60      0.17        29\n",
      "                                       weighted avg       0.23      0.97      0.35        29\n",
      "                                        samples avg       0.11      0.98      0.19        29\n",
      "\n",
      "Average Precision: 0.1790633608815427, Average Recall: 0.9848484848484849, Average F1: 0.30303030303030304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:900: UserWarning: unknown class(es) ['None'] will be ignored\n",
      "  warnings.warn(\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = meme_classifier.evaluate_with_hierarchy(X_test, attention_masks_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e44b8-8591-4eb9-b19d-277a2aa75699",
   "metadata": {},
   "source": [
    "# F1 Hierarchy Outside the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a967c74c-128a-4288-b196-7b3cfe7228dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import networkx\n",
    "from networkx import DiGraph, relabel_nodes, all_pairs_shortest_path_length\n",
    "from sklearn_hierarchical_classification.constants import ROOT\n",
    "from sklearn_hierarchical_classification.metrics import h_fbeta_score, h_recall_score, h_precision_score, fill_ancestors, multi_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e5fc1fe-af29-4c9d-8e60-6fc146ce7f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label tree in graph format\n",
    "\n",
    "# there is no 'none' label in this tree\n",
    "\n",
    "G = DiGraph()\n",
    "G.add_edge(ROOT, \"Logos\")\n",
    "G.add_edge(\"Logos\", \"Repetition\")\n",
    "G.add_edge(\"Logos\", \"Obfuscation, Intentional vagueness, Confusion\")\n",
    "G.add_edge(\"Logos\", \"Reasoning\")\n",
    "G.add_edge(\"Logos\", \"Justification\")\n",
    "G.add_edge('Justification', \"Slogans\")\n",
    "G.add_edge('Justification', \"Bandwagon\")\n",
    "G.add_edge('Justification', \"Appeal to authority\")\n",
    "G.add_edge('Justification', \"Flag-waving\")\n",
    "G.add_edge('Justification', \"Appeal to fear/prejudice\")\n",
    "G.add_edge('Reasoning', \"Simplification\")\n",
    "G.add_edge('Simplification', \"Causal Oversimplification\")\n",
    "G.add_edge('Simplification', \"Black-and-white Fallacy/Dictatorship\")\n",
    "G.add_edge('Simplification', \"Thought-terminating cliché\")\n",
    "G.add_edge('Reasoning', \"Distraction\")\n",
    "G.add_edge('Distraction', \"Misrepresentation of Someone's Position (Straw Man)\")\n",
    "G.add_edge('Distraction', \"Presenting Irrelevant Data (Red Herring)\")\n",
    "G.add_edge('Distraction', \"Whataboutism\")\n",
    "G.add_edge(ROOT, \"Ethos\")\n",
    "G.add_edge('Ethos', \"Appeal to authority\")\n",
    "G.add_edge('Ethos', \"Glittering generalities (Virtue)\")\n",
    "G.add_edge('Ethos', \"Bandwagon\")\n",
    "G.add_edge('Ethos', \"Ad Hominem\")\n",
    "G.add_edge('Ethos', \"Transfer\")\n",
    "G.add_edge('Ad Hominem', \"Doubt\")\n",
    "G.add_edge('Ad Hominem', \"Name calling/Labeling\")\n",
    "G.add_edge('Ad Hominem', \"Smears\")\n",
    "G.add_edge('Ad Hominem', \"Reductio ad hitlerum\")\n",
    "G.add_edge('Ad Hominem', \"Whataboutism\")\n",
    "G.add_edge(ROOT, \"Pathos\")\n",
    "G.add_edge('Pathos', \"Exaggeration/Minimisation\")\n",
    "G.add_edge('Pathos', \"Loaded Language\")\n",
    "G.add_edge('Pathos', \"Appeal to (Strong) Emotions\")\n",
    "G.add_edge('Pathos', \"Appeal to fear/prejudice\")\n",
    "G.add_edge('Pathos', \"Flag-waving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c2dfe-0feb-42db-90c7-4b14a682f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate score\n",
    "\n",
    "def get_all_classes_from_graph(graph):\n",
    "    return [\n",
    "        node\n",
    "        for node in graph.nodes\n",
    "        if node != ROOT\n",
    "        ]\n",
    "\n",
    "def _h_fbeta_score(y_true, y_pred, class_hierarchy, beta=1., root=ROOT):\n",
    "    hP = _h_precision_score(y_true, y_pred, class_hierarchy, root=root)\n",
    "    hR = _h_recall_score(y_true, y_pred, class_hierarchy, root=root)\n",
    "    return (1. + beta ** 2.) * hP * hR / (beta ** 2. * hP + hR)\n",
    "    \n",
    "def _fill_ancestors(y, graph, root, copy=True):\n",
    "    y_ = y.copy() if copy else y\n",
    "    paths = all_pairs_shortest_path_length(graph.reverse(copy=False))\n",
    "    for target, distances in paths:\n",
    "        if target == root:\n",
    "            continue\n",
    "        ix_rows = np.where(y[:, target] > 0)[0]\n",
    "        ancestors = list(filter(lambda x: x != ROOT,distances.keys()))\n",
    "        y_[tuple(np.meshgrid(ix_rows, ancestors))] = 1\n",
    "    graph.reverse(copy=False)\n",
    "    return y_\n",
    "    \n",
    "def _h_recall_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
    "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
    "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
    "\n",
    "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
    "\n",
    "    true_positives = len(ix[0])\n",
    "    all_positives = np.count_nonzero(y_true_)\n",
    "\n",
    "    return true_positives / all_positives\n",
    "\n",
    "def _h_precision_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
    "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
    "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
    "\n",
    "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
    "\n",
    "    true_positives = len(ix[0])\n",
    "    all_results = np.count_nonzero(y_pred_)\n",
    "\n",
    "    return true_positives / all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a16ed3c-5b79-4569-bd90-d112e282b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "\n",
    "def evaluate_h_test(gold_labels, pred_labels):\n",
    "    \"\"\" Modified from the scoring baselines, labels should be a list of labels per samples e.g.:\n",
    "    gold = [['Smears'], ['Smears, 'Flag-waving]] \"\"\"\n",
    "    \n",
    "    with multi_labeled(gold, pred, G) as (gold_, pred_, graph_):\n",
    "        return  _h_precision_score(gold_, pred_,graph_), _h_recall_score(gold_, pred_,graph_), _h_fbeta_score(gold_, pred_,graph_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea5fc876-4297-44f9-ade0-5419e7243f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.5454545454545454, 0.6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example:\n",
    "# precision, recall, f1\n",
    "gold = [['Loaded Language', 'Smears'], ['Whataboutism']]\n",
    "pred = [['Loaded Language', 'Smears'], ['Flag-waving']]\n",
    "evaluate_h_test(gold, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2107042f-f048-485b-bad2-451ef82d389c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 1.0, 0.8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with half reward:\n",
    "# precision, recall, f1\n",
    "gold = [['Loaded Language', 'Smears'], ['Pathos']]\n",
    "pred = [['Loaded Language', 'Smears'], ['Flag-waving']]\n",
    "evaluate_h_test(gold, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d2dee19-3bda-48f4-a27b-5e5abc57d34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.875, 0.6363636363636364, 0.7368421052631579)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of partial reward, predicting label of the same ancestor but not moving up to the top label:\n",
    "# precision, recall, f1\n",
    "\n",
    "gold = [['Loaded Language', 'Smears'], ['Whataboutism']]\n",
    "pred = [['Loaded Language', 'Smears'], ['Smears']]\n",
    "evaluate_h_test(gold, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
