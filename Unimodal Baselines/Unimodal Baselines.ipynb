{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd94236b-2b06-4689-8be4-cd11119163b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ae8f7-8568-4a0f-90fc-dce9d3957a77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Reading (ignore if have CSVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691edcf7-0ad6-4a83-a8e8-0b00a6dd6bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65635</td>\n",
       "      <td>THIS IS WHY YOU NEED\\n\\nA SHARPIE WITH YOU AT ...</td>\n",
       "      <td>[Black-and-white Fallacy/Dictatorship]</td>\n",
       "      <td>https://www.facebook.com/photo/?fbid=402355213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67927</td>\n",
       "      <td>GOOD NEWS!\\n\\nNAZANIN ZAGHARI-RATCLIFFE AND AN...</td>\n",
       "      <td>[Loaded Language, Glittering generalities (Vir...</td>\n",
       "      <td>https://www.facebook.com/amnesty/photos/531198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68031</td>\n",
       "      <td>PAING PHYO MIN IS FREE!</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/amnesty/photos/427419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77490</td>\n",
       "      <td>Move your ships away!\\n\\noooook\\n\\nMove your s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/rightpatriots/photos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67641</td>\n",
       "      <td>WHEN YOU'RE THE FBI, THEY LET YOU DO IT.</td>\n",
       "      <td>[Thought-terminating cliché]</td>\n",
       "      <td>https://www.facebook.com/AddictingInfoOrg/phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>78774</td>\n",
       "      <td>Level: Easy\\n\\nLevel: Hard\\n\\nLevel: For Mothe...</td>\n",
       "      <td>[Slogans, Name calling/Labeling, Repetition]</td>\n",
       "      <td>https://www.facebook.com/themotherlandcalls/ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>78299</td>\n",
       "      <td>Europeans:\\n\\nYou are trespassing</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/communism101/photos/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>67096</td>\n",
       "      <td>\\NONSENSE DETECTED! \\nFIRE AT WILL!\\\\n\\n\\Time ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/photo/?fbid=181158225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>68464</td>\n",
       "      <td>ANNA AND VIRA:\\nATTACKED FOR DEFENDING LGBTI A...</td>\n",
       "      <td>[Causal Oversimplification, Loaded Language]</td>\n",
       "      <td>https://www.facebook.com/amnesty/photos/490311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>71038</td>\n",
       "      <td>\\Our food system belongs in the hands of many ...</td>\n",
       "      <td>[Name calling/Labeling, Appeal to authority, B...</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0    65635  THIS IS WHY YOU NEED\\n\\nA SHARPIE WITH YOU AT ...   \n",
       "1    67927  GOOD NEWS!\\n\\nNAZANIN ZAGHARI-RATCLIFFE AND AN...   \n",
       "2    68031                            PAING PHYO MIN IS FREE!   \n",
       "3    77490  Move your ships away!\\n\\noooook\\n\\nMove your s...   \n",
       "4    67641           WHEN YOU'RE THE FBI, THEY LET YOU DO IT.   \n",
       "..     ...                                                ...   \n",
       "495  78774  Level: Easy\\n\\nLevel: Hard\\n\\nLevel: For Mothe...   \n",
       "496  78299                  Europeans:\\n\\nYou are trespassing   \n",
       "497  67096  \\NONSENSE DETECTED! \\nFIRE AT WILL!\\\\n\\n\\Time ...   \n",
       "498  68464  ANNA AND VIRA:\\nATTACKED FOR DEFENDING LGBTI A...   \n",
       "499  71038  \\Our food system belongs in the hands of many ...   \n",
       "\n",
       "                                                labels  \\\n",
       "0               [Black-and-white Fallacy/Dictatorship]   \n",
       "1    [Loaded Language, Glittering generalities (Vir...   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4                         [Thought-terminating cliché]   \n",
       "..                                                 ...   \n",
       "495       [Slogans, Name calling/Labeling, Repetition]   \n",
       "496                                                 []   \n",
       "497                                                 []   \n",
       "498       [Causal Oversimplification, Loaded Language]   \n",
       "499  [Name calling/Labeling, Appeal to authority, B...   \n",
       "\n",
       "                                                  link  \n",
       "0    https://www.facebook.com/photo/?fbid=402355213...  \n",
       "1    https://www.facebook.com/amnesty/photos/531198...  \n",
       "2    https://www.facebook.com/amnesty/photos/427419...  \n",
       "3    https://www.facebook.com/rightpatriots/photos/...  \n",
       "4    https://www.facebook.com/AddictingInfoOrg/phot...  \n",
       "..                                                 ...  \n",
       "495  https://www.facebook.com/themotherlandcalls/ph...  \n",
       "496  https://www.facebook.com/communism101/photos/5...  \n",
       "497  https://www.facebook.com/photo/?fbid=181158225...  \n",
       "498  https://www.facebook.com/amnesty/photos/490311...  \n",
       "499                                               null  \n",
       "\n",
       "[7500 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = 'X:\\\\PhD\\\\SemEval Task4\\\\Data'\n",
    "\n",
    "\n",
    "anno_subtask1_train = pd.read_json('X:\\\\PhD\\\\SemEval Task4\\\\Data\\\\annotations\\\\data\\\\subtask1\\\\train.json')\n",
    "anno_subtask1_val = pd.read_json('X:\\\\PhD\\\\SemEval Task4\\\\Data\\\\annotations\\\\data\\\\subtask1\\\\validation.json')\n",
    "anno_subtask1_dev = pd.read_json('X:\\\\PhD\\\\SemEval Task4\\\\Data\\\\annotations\\\\data\\\\subtask1\\\\dev_unlabeled.json')\n",
    "\n",
    "anno_subtask1_combined = pd.concat([anno_subtask1_train, anno_subtask1_val])\n",
    "anno_subtask1_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8899b0a3-8f4e-4b83-86a0-dfeafcbf8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_subtask1_train.drop(columns=['link']).to_csv('./testdata/train/s1_train.csv')\n",
    "anno_subtask1_val.drop(columns=['link']).to_csv('./testdata/val/s1_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdaf41c3-6c8f-4319-947a-495691577f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reductio ad hitlerum',\n",
       " \"Misrepresentation of Someone's Position (Straw Man)\",\n",
       " 'Repetition',\n",
       " 'Name calling/Labeling',\n",
       " 'Presenting Irrelevant Data (Red Herring)',\n",
       " 'Glittering generalities (Virtue)',\n",
       " 'Bandwagon',\n",
       " 'Loaded Language',\n",
       " 'Smears',\n",
       " 'Exaggeration/Minimisation',\n",
       " 'Black-and-white Fallacy/Dictatorship',\n",
       " 'Thought-terminating cliché',\n",
       " 'Appeal to fear/prejudice',\n",
       " 'Causal Oversimplification',\n",
       " 'Obfuscation, Intentional vagueness, Confusion',\n",
       " 'Flag-waving',\n",
       " 'Slogans',\n",
       " 'Whataboutism',\n",
       " 'Doubt',\n",
       " 'Appeal to authority']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set([x for xs in anno_subtask1_combined['labels'].to_list() for x in xs]))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0f951-aa50-4062-bc15-878879bb0ee9",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59968382-a91a-486a-9cb1-07772238fed5",
   "metadata": {},
   "source": [
    "Modified version of: https://github.com/kinit-sk/semeval2023-task3-persuasion-techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0341ceb6-5a1a-45f6-bb4a-0369f4134200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reqs\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer\n",
    "import ast\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f549086-6f3d-41a3-b55f-77ccbbaadec2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad13423-46be-40c0-bd6b-4943d06d24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBase, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 19) # number of classes/techniques\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EnglishRobertaBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnglishRobertaBase, self).__init__()\n",
    "        self.l1 = transformers.RobertaModel.from_pretrained('roberta-base', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 19)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EnglishRobertaLarge(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnglishRobertaLarge, self).__init__()\n",
    "        self.l1 = transformers.RobertaModel.from_pretrained('roberta-large', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(1024, 19)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class mBERTBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mBERTBase, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-multilingual-cased', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 19)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class XLMRobertaBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaBase, self).__init__()\n",
    "        self.l1 = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 19)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class XLMRobertaLarge(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaLarge, self).__init__()\n",
    "        self.l1 = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-large', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(1024, 19)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f13ba4-fd26-48d0-a393-10f96bc8d027",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e053e9c6-d502-44f8-8b0e-bac177ced3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.techniques_encoded\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dataset_training(MODEL_NAME, PATHS_TO_TRAIN, PATHS_TO_VALIDATION, TRAINING_LANGUAGES):\n",
    "    \"\"\"\n",
    "    Function to load and process the _input_data into suitable format. Includes setting up tokenizer, one hot encoding of\n",
    "    techniques, concatenation of csv files if necessary, language filtering and loading _input_data into CustomDataset class.\n",
    "\n",
    "    :param MODEL_NAME: name of the model as per Hugging face to use for tokenizer.\n",
    "    :param PATHS_TO_TRAIN: List of paths to csv files we want to use for training.\n",
    "    :param PATHS_TO_VALIDATION: List of paths to csv files we want to use for validation.\n",
    "    :param TRAINING_LANGUAGES: List of languages we are using.\n",
    "    :return: Tuple of training and testing set in Dataset format.\n",
    "    \"\"\"\n",
    "\n",
    "    #Create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, return_dict=False)\n",
    "    MAX_LEN = tokenizer.model_max_length\n",
    "    if MAX_LEN > 1024:\n",
    "        print(f'hmm, seems the tokenizer is saying too much: {MAX_LEN}, setting down to 512.')\n",
    "        MAX_LEN = 512\n",
    "    print(f'max length of tokens for < {MODEL_NAME} > is: < {MAX_LEN} >')\n",
    "\n",
    "    #Concatenate csv files\n",
    "    df_train = pd.DataFrame()\n",
    "    for train_path in PATHS_TO_TRAIN:\n",
    "        df_train = pd.concat([df_train, pd.read_csv(train_path)], ignore_index=True)\n",
    "\n",
    "    #One hot encoding of techniques\n",
    "    df_train['labels'] = df_train['labels'].apply(ast.literal_eval)\n",
    "    one_hot = MultiLabelBinarizer()\n",
    "    df_train['techniques_encoded'] = one_hot.fit_transform(df_train['labels'])[:, 1:].tolist()\n",
    "\n",
    "    #Load validation dataset and encode\n",
    "    if PATHS_TO_VALIDATION:\n",
    "        df_dev = pd.DataFrame()\n",
    "        for dev_path in PATHS_TO_VALIDATION:\n",
    "            df_dev = pd.concat([df_dev, pd.read_csv(dev_path)], ignore_index=True)\n",
    "\n",
    "        df_dev['labels'] = df_dev['labels'].apply(ast.literal_eval)\n",
    "        # use fitted one-hot encoder transform in case some techniques are present in training but not in dev\n",
    "        df_dev['techniques_encoded'] = one_hot.transform(df_dev['labels'])[:, 1:].tolist()\n",
    "    else:\n",
    "        df_dev = pd.DataFrame(\n",
    "            columns=[['id', 'text', 'labels', 'techniques_encoded']])\n",
    "\n",
    "    #Language filtering - Not Needed yet\n",
    "    # df_train = df_train[df_train['language'].isin(TRAINING_LANGUAGES)]\n",
    "    # df_dev = df_dev[df_dev['language'].isin(TRAINING_LANGUAGES)]\n",
    "    \n",
    "    train_dataset = df_train\n",
    "    test_dataset = df_dev\n",
    "\n",
    "    print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    print(f\"TEST Dataset: {test_dataset.shape}\")\n",
    "\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "    testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "    return training_set, testing_set\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, return_dict=False)\n",
    "    MAX_LEN = tokenizer.model_max_length\n",
    "    if MAX_LEN > 1024:\n",
    "        print(f'hmm, seems the tokenizer is saying too much: {MAX_LEN}, setting down to 512.')\n",
    "        MAX_LEN = 512\n",
    "    print(f'max length of tokens for < {MAX_LEN} > is: < {MAX_LEN} >')\n",
    "    return tokenizer, MAX_LEN\n",
    "\n",
    "\n",
    "def load_dataframe(args):\n",
    "    test_df = pd.read_csv(args[\"EVALUATION_SET\"])\n",
    "\n",
    "    # language_to_texts = {lan: list(text) for lan, text in test_df.groupby('language')['text']}\n",
    "    # for language, texts in language_to_texts.items():\n",
    "    #     print(f'language {language} has {len(texts)} lines of data.')\n",
    "\n",
    "    \n",
    "    print('Encoding techniques into one-hot-encoded lists!\\n')\n",
    "        # Create sentence and label lists\n",
    "    test_sentences = test_df.text.values\n",
    "\n",
    "    test_df['labels'] = test_df['labels'].apply(ast.literal_eval)\n",
    "    one_hot = MultiLabelBinarizer()\n",
    "    test_df['techniques_encoded'] = one_hot.fit_transform(test_df['labels'])[:, 1:].tolist()\n",
    "    test_labels = test_df.techniques_encoded.values\n",
    "    test_labels = np.array([labels for labels in test_labels])\n",
    "\n",
    "    mappings = one_hot.classes_[1:]    \n",
    "\n",
    "    return test_df, mappings\n",
    "\n",
    "def create_dataloader(args, test_df, language, language_counts):\n",
    "    # print(\n",
    "    #     f'testing on {language_counts[language]} data points (around {language_counts[language] / args[\"BATCH_SIZE\"]} iterations)')\n",
    "    filtered_test_df = test_df.copy()\n",
    "\n",
    "    # not required until we use multilingual data\n",
    "    # filtered_test_df = filtered_test_df[filtered_test_df['language'] == language]\n",
    "    # filtered_test_df = filtered_test_df.reset_index()\n",
    "    \n",
    "    tokenizer, MAX_LEN = get_tokenizer(args['MODEL_NAME'])\n",
    "\n",
    "    testing_set = CustomDataset(\n",
    "        test_df,\n",
    "        tokenizer,\n",
    "        MAX_LEN\n",
    "    )\n",
    "\n",
    "    testing_loader = DataLoader(\n",
    "        testing_set,\n",
    "        batch_size=args['BATCH_SIZE'],\n",
    "        shuffle=args['SHUFFLE'],\n",
    "        num_workers=args['NUM_WORKERS']\n",
    "    )\n",
    "\n",
    "    return testing_loader, filtered_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c7c90-9e64-493e-8c90-74d3e175b63d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d14344-a643-490c-aede-4c9c9563359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(args):\n",
    "    \"\"\"\n",
    "    Function creates a dataset based on given arguments. Afterwards it trains and validates a model.\n",
    "\n",
    "    :param args: dictionary of arguments needed to create the model\n",
    "    \"\"\"\n",
    "    training_set, testing_set = create_dataset_training(MODEL_NAME=args['MODEL_NAME'], PATHS_TO_TRAIN=args['PATHS_TO_TRAIN'], PATHS_TO_VALIDATION=args['PATHS_TO_VALIDATION'], TRAINING_LANGUAGES=args['TRAINING_LANGUAGES'])\n",
    "\n",
    "    training_loader = DataLoader(\n",
    "        training_set,\n",
    "        batch_size=args['TRAIN_BATCH_SIZE'],\n",
    "        shuffle=args['TRAIN_SHUFFLE'],\n",
    "        num_workers=args['TRAIN_NUM_WORKERS']\n",
    "    )\n",
    "\n",
    "    validation_loader = DataLoader(\n",
    "        testing_set,\n",
    "        batch_size=args['VALIDATION_BATCH_SIZE'],\n",
    "        shuffle=args['VALIDATION_SHUFFLE'],\n",
    "        num_workers=args['VALIDATION_NUM_WORKERS']\n",
    "    )\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    lm = LanguageModel(training_loader=training_loader, validation_loader=validation_loader, device=device, args=args)\n",
    "    return lm.train_over_epochs()\n",
    "\n",
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"\n",
    "    Class to hold the model as well as train and validate it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_loader, validation_loader, device, args):\n",
    "\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.validation_loader = validation_loader\n",
    "        self.training_loader = training_loader\n",
    "        if args['MODEL_NAME'] == 'roberta-large':\n",
    "            self.model = EnglishRobertaLarge()\n",
    "        elif args['MODEL_NAME'] == 'roberta-base':\n",
    "            self.model = EnglishRobertaBase()\n",
    "        elif args['MODEL_NAME'] == 'bert-base-uncased':\n",
    "            self.model = BERTBase()\n",
    "        elif args['MODEL_NAME'] == 'bert-base-multilingual-cased':\n",
    "            self.model = mBERTBase()\n",
    "        elif args['MODEL_NAME'] == 'xlm-roberta-base':\n",
    "            self.model = XLMRobertaBase()\n",
    "        elif args['MODEL_NAME'] == 'xlm-roberta-large':\n",
    "            self.model = XLMRobertaLarge()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=self.args['LEARNING_RATE'])\n",
    "        if args['FREEZE']:\n",
    "            counter = 0\n",
    "            no_layers = len([param for param in self.model.parameters()])\n",
    "            for param in self.model.parameters():\n",
    "                counter += 1\n",
    "                param.requires_grad = False\n",
    "                if counter == (no_layers//5) * 4:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def save_model_and_weights(self, epoch):\n",
    "        print(f'\\n\\n --- Currently saving model! --- \\n')\n",
    "        model_path = f'{self.args[\"MODELS_DIR\"]}{self.args[\"MODEL_NAME_PREFIX\"]}_model_{epoch}epoch_batch{self.args[\"TRAIN_BATCH_SIZE\"]}_lr{self.args[\"LEARNING_RATE\"]}.pth'\n",
    "        model_weights_path = f'{self.args[\"MODELS_DIR\"]}{self.args[\"MODEL_NAME_PREFIX\"]}_model_weights_{epoch}epoch_batch{self.args[\"TRAIN_BATCH_SIZE\"]}_lr{self.args[\"LEARNING_RATE\"]}.pth'\n",
    "        torch.save(self.model.state_dict(), model_weights_path)\n",
    "        torch.save(self.model, model_path)\n",
    "        return model_path\n",
    "\n",
    "    def train(self, epoch):\n",
    "        print(f'\\nNow training on Epoch {epoch}.')\n",
    "        print(\n",
    "            f'Testing on {len(self.training_loader) * self.training_loader.batch_size - 1} data points (around {len(self.training_loader)} iterations)')\n",
    "\n",
    "        self.args['CURRENT_TRAIN_LOSS'] = 0\n",
    "        self.model.train()\n",
    "        for itr, data in tqdm(enumerate(self.training_loader, 0)):\n",
    "            ids = data['ids'].to(self.device, dtype=torch.long)\n",
    "            mask = data['mask'].to(self.device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "            targets = data['targets'].to(self.device, dtype=torch.float)\n",
    "\n",
    "            outputs = self.model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "            if itr % 200 == 0:\n",
    "                print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.args['CURRENT_TRAIN_LOSS'] += loss\n",
    "\n",
    "        print(f'Accumulated Training Loss after Epoch {epoch} is: {self.args[\"CURRENT_TRAIN_LOSS\"]}')\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        print(f'\\nNow validating on Epoch {epoch}.')\n",
    "        print(\n",
    "            f'Validating on {len(self.validation_loader) * self.validation_loader.batch_size} data points (around {len(self.validation_loader)} iterations)')\n",
    "        self.model.eval()\n",
    "        self.args['CURRENT_VALIDATION_LOSS'] = 0\n",
    "        with torch.no_grad():\n",
    "            for val_itr, data in tqdm(enumerate(self.validation_loader)):\n",
    "                ids = data['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = data['mask'].to(self.device, dtype=torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                targets = data['targets'].to(self.device, dtype=torch.float)\n",
    "\n",
    "                outputs = self.model(ids, mask, token_type_ids)\n",
    "                loss = torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "                if val_itr % 200 == 0:\n",
    "                    print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "\n",
    "                # accumulate the loss across all batches\n",
    "                self.args['CURRENT_VALIDATION_LOSS'] += loss\n",
    "\n",
    "                # clear cache\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        print(f'Accumulated Validation Loss after Epoch {epoch} is: {self.args[\"CURRENT_VALIDATION_LOSS\"]}')\n",
    "\n",
    "        # Early stopping if necessary\n",
    "        if self.args['CURRENT_VALIDATION_LOSS'] < self.args['BEST_LOSS']:\n",
    "            self.args['BEST_LOSS'] = self.args['CURRENT_VALIDATION_LOSS']\n",
    "            self.save_model_and_weights(epoch)\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            # stop training\n",
    "            print('Validation loss increased! Stopping training...')\n",
    "            print(f'Last validation loss was: {self.args[\"BEST_LOSS\"]}')\n",
    "            print(f'Current validation loss is: {self.args[\"CURRENT_VALIDATION_LOSS\"]}')\n",
    "\n",
    "            return False\n",
    "\n",
    "    def train_over_epochs(self):\n",
    "        for epoch in range(self.args['EPOCHS']):\n",
    "            self.train(epoch)\n",
    "            if self.args['PATHS_TO_VALIDATION'] and not self.validate(epoch):\n",
    "                break  # the last epoch was NOT counted due to early stopping\n",
    "            else:\n",
    "                # store latest version\n",
    "                path = self.save_model_and_weights(epoch)\n",
    "        print(\"Done.\")\n",
    "        return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af63979-0c35-4280-aede-2a19459cd57e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training Different Models (should be a loop but is not a loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c8316e6-79ea-4d40-8e3f-147f51b4e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < xlm-roberta-large > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n",
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.6796550750732422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [38:37, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.22325746715068817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [1:17:15, 11.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.1882757842540741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [1:56:20, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2372388392686844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [2:34:48, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.24288859963417053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [2:49:13, 11.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 212.21343994140625\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:09,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.24209730327129364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:31,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 13.597033500671387\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.17399227619171143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [38:52, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.22938214242458344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [1:17:44, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.14239315688610077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [1:56:37, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.21883244812488556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [2:35:29, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.23012728989124298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [2:50:04, 11.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 176.20091247558594\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.21410717070102692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:13,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 12.640756607055664\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1780775934457779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [53:18, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.16186870634555817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [1:46:36, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2536025941371918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [2:39:55, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.22132770717144012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [3:33:14, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.12633655965328217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [3:53:13, 15.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 153.43455505371094\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2355465143918991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [03:27,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 12.39066219329834\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.16450731456279755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [52:22, 15.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.14485228061676025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [1:44:50, 15.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.10938655585050583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [2:39:07, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2023271769285202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [3:40:44, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.14098167419433594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [4:04:24, 16.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 133.15428161621094\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.22417588531970978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [04:21,  4.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 12.381818771362305\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1778809130191803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [58:06, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.06397739052772522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [1:54:36, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.0672091469168663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [2:50:01, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1909969300031662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [3:42:19, 15.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1478714942932129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [4:01:56, 16.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 113.25543975830078\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.20653827488422394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [04:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 12.515527725219727\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 12.381818771362305\n",
      "Current validation loss is: 12.515527725219727\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': '',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'xlm-roberta-large',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "                                                            # unfreeze for PTC fine-tuning?\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86e13755-203d-470c-bd92-2bed9c5b7ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < bert-base-multilingual-cased > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n",
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.6773495674133301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:57,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.28486016392707825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [18:08,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2414165884256363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [26:55,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.27533861994743347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [35:55,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2402072250843048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [39:31,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 231.0172576904297\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.25061848759651184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:09,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.706016540527344\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2030051052570343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [14:52,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.18518470227718353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [29:44,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.17730334401130676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [44:36,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2838349938392639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [59:28,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.3091575801372528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [1:05:03,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 199.37339782714844\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2498093694448471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:05, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 14.063506126403809\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.19667458534240723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [16:13,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2836587131023407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [32:46,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2297101765871048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [48:53,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.17762181162834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [1:04:27,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.18972259759902954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [1:10:12,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 187.03121948242188\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2492693066596985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 14.015921592712402\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.254861980676651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [15:38,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.14487753808498383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [31:32,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.12822268903255463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [46:48,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.17995351552963257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [1:01:25,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.155323788523674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [1:06:53,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 173.15943908691406\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2323458343744278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 13.681135177612305\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1548755168914795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [11:57,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.21165232360363007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [23:46,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.11364316940307617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [35:34,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.11460655927658081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [47:51,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.15762251615524292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [52:32,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 159.1197052001953\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.23882943391799927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:16,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 13.922234535217285\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 13.681135177612305\n",
      "Current validation loss is: 13.922234535217285\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'bert-base-multilingual-cased',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'bert-base-multilingual-cased',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eef5fd12-a8ee-4b4b-a358-34ab1c0c21c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b171c75021484ae8953218ec54a8c989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf6674a272e4fe290f85cf12cc26807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9486dbdd14e94887af640fd4f4c6e58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f489dd6c4a74208afebf1ee52a917ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < roberta-large > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47db649cf34562a3f426ca9ffb208b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.7196247577667236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [02:58,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.14794889092445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [05:57,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.19413061439990997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [08:55,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.1944851577281952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [11:54,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2094479501247406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [13:01,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 212.99249267578125\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2421095073223114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:17,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 13.489632606506348\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20842787623405457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:40,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.23920315504074097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:21,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.18115803599357605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20896288752555847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:44,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.13941295444965363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:21,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 173.28250122070312\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20948049426078796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 12.475747108459473\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.14728398621082306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:42,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.19809940457344055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:23,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.18450607359409332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:04,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.17762085795402527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:45,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.17555472254753113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:23,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 150.40342712402344\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.20318101346492767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 12.033329010009766\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.18346114456653595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:41,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1284378319978714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:22,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.17629924416542053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:03,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.16613200306892395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:44,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1318967193365097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:22,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 127.09971618652344\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.20633022487163544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 12.021674156188965\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1886201947927475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:41,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.10580242425203323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:22,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.21124684810638428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:03,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.061535660177469254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:44,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1605372130870819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:22,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 103.2136001586914\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.18887023627758026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 12.16552734375\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 12.021674156188965\n",
      "Current validation loss is: 12.16552734375\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'roberta-large',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'roberta-large',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d541fef-4eb8-4d1f-98e2-d54b7c764486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3f607e2c51482ead775e43f59b5e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989a653391c64173a4d630b8a47d8dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c1b3edfc6c4bfc9184e3092e8aa0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0d13438c774fb5bb58d087ca4a7e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < roberta-base > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f494935388ed448494898940e43a1e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.7152462005615234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "201it [00:32,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2909088134765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:04,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2278163880109787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:37,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.27329012751579285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:09,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.15474048256874084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:22,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 225.4221954345703\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2550155222415924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.2190523147583\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2324271947145462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.16518595814704895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:05,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1875995248556137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.28147995471954346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:10,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1673932522535324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:22,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 186.32395935058594\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.24053393304347992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 13.328704833984375\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.24704541265964508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:33,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.21066510677337646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:05,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.15657512843608856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2531723976135254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:11,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.22788824141025543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:23,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 167.74563598632812\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.24051441252231598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 12.881023406982422\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1890919804573059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.16152901947498322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:05,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1279347985982895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.13523069024085999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:10,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.21843478083610535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:22,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 152.10427856445312\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.23517917096614838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 12.661491394042969\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.12344952672719955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:33,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.15436004102230072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:06,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.09720074385404587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.09447702765464783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:11,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.13167890906333923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:23,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 136.10641479492188\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.23383788764476776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 12.972590446472168\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 12.661491394042969\n",
      "Current validation loss is: 12.972590446472168\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'roberta-base',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'roberta-base',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b3fddf-ca04-4d6a-beac-4726afb2b8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < bert-base-uncased > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n",
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Goat\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.6837515830993652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2346428781747818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [08:17,  2.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m      1\u001b[0m args_train \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# general\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODEL_NAME_PREFIX\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m,                                \u001b[38;5;66;03m# so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCURRENT_VALIDATION_LOSS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     27\u001b[0m }\n\u001b[1;32m---> 29\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     23\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m lm \u001b[38;5;241m=\u001b[39m LanguageModel(training_loader\u001b[38;5;241m=\u001b[39mtraining_loader, validation_loader\u001b[38;5;241m=\u001b[39mvalidation_loader, device\u001b[38;5;241m=\u001b[39mdevice, args\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_over_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 143\u001b[0m, in \u001b[0;36mLanguageModel.train_over_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_over_epochs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m--> 143\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPATHS_TO_VALIDATION\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(epoch):\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# the last epoch was NOT counted due to early stopping\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 93\u001b[0m, in \u001b[0;36mLanguageModel.train\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 93\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCURRENT_TRAIN_LOSS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'bert-base-uncased',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 1,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'bert-base-uncased',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f86b8434-b17d-4945-964d-ea75402ea03a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < bert-base-multilingual-cased > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n",
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.6666839718818665"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:33,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.23557336628437042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:07,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2597467601299286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:41,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.256039559841156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:15,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.26465317606925964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:27,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 228.52093505859375\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.24523764848709106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.681147575378418\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.261654794216156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2212589681148529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:08,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.22010673582553864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:42,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.24758169054985046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:16,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1730608195066452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:28,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 196.7558135986328\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2491392344236374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 13.986952781677246\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.21785090863704681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1267060935497284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:08,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.18764036893844604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:42,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1653461754322052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:16,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1524803787469864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:28,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 183.50254821777344\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.25220245122909546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 13.564451217651367\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.15715853869915009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.18217726051807404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:07,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.21207435429096222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:41,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.19616058468818665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:15,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.13681496679782867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:27,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 168.31837463378906\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2412455677986145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 13.5440092086792\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.16728709638118744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1010003387928009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:07,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.2247316986322403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:41,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.2190481722354889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:15,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1160217821598053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:27,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 153.6635284423828\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.2382974624633789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 13.570302963256836\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 13.5440092086792\n",
      "Current validation loss is: 13.570302963256836\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'bert-base-multilingual-cased',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'bert-base-multilingual-cased',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "205e6efc-f3ac-4fe8-9464-113e3bdaa09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8bf067944044a3b5a0b326f4312037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387c0c5670784ec2a7fe973dbe741387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7367c5eba04db08c020bef44b9b39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < xlm-roberta-base > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0be639c5ffa40e98283e198a8b4b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.7227253913879395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:35,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.23435835540294647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:11,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2802368104457855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:47,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2693515717983246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:23,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2652474045753479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:36,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 236.19778442382812\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2521989345550537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.92094898223877\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.22549903392791748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:36,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1729382574558258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:12,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20894300937652588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:48,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20800527930259705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:24,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2513335943222046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:37,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 203.90008544921875\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.25143009424209595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 14.571717262268066\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.22406335175037384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:36,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.23353950679302216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:12,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.26549744606018066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:48,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.21081149578094482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:25,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.14100949466228485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:38,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 199.7682342529297\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 19.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.255021333694458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 14.353126525878906\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1511322259902954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:36,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.23527321219444275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:12,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2530156672000885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:48,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.19035103917121887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:23,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.24796320497989655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:36,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 192.7353973388672\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.25359097123146057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 13.806134223937988\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.17935985326766968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:35,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.20921297371387482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:11,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.19051088392734528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:47,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1933344006538391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:22,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.25753507018089294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:35,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 180.2266082763672\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.23572345077991486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 13.673762321472168\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'xlm-roberta-base',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'xlm-roberta-base',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4d270-74be-46c6-acc0-7a774660dda6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30937178-38ab-4714-910e-288253d61c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testing_loader, only_test):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "\n",
    "            # if there are no targets available\n",
    "            if not only_test:\n",
    "                targets = data['targets'].to(device, dtype=torch.float)\n",
    "                fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets\n",
    "\n",
    "\n",
    "def run_tests(model, testing_loader, evaluation_threshold, only_test, lang, results_list):\n",
    "    outputs, targets = test(model, testing_loader, only_test)\n",
    "    outputs = np.array(outputs) >= evaluation_threshold\n",
    "\n",
    "    if targets:\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "        f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "        print(f\"Accuracy Score = {accuracy}\")\n",
    "        print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "        print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "        results_list.append([lang, accuracy, f1_score_micro, f1_score_macro])\n",
    "        print(results_list)\n",
    "\n",
    "    return outputs, results_list\n",
    "\n",
    "def run_eval(args):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = torch.load(args[\"MODEL_FILE_NAME\"])\n",
    "    model.to(device)\n",
    "\n",
    "    test_df, mappings = load_dataframe(args)\n",
    "    language_counts = 1 # hard code to one since just testing on English\n",
    "    language = args['LANGUAGES']\n",
    "\n",
    "    results_list = []\n",
    "    \n",
    "    testing_loader, filtered_test_df = create_dataloader(args, test_df, args['LANGUAGES'], language_counts)\n",
    "\n",
    "    outputs, results_list = run_tests(model, testing_loader, args['EVALUATION_THRESHOLD'], args['ONLY_TEST'], language, results_list)\n",
    "\n",
    "    #path = f'results/{args[\"RESULT_DIR_NAME\"]}/{language}.txt'\n",
    "\n",
    "    df_results = pd.DataFrame(results_list, columns=['language', 'accuracy', 'F1_micro', 'F1_macro'])\n",
    "    df_results['model'] = args['MODEL_TITLE']\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1e0c856-c54b-4dcc-b35e-6d57920a380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models names: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "\n",
    "args_eval = {\n",
    "    'MODEL_FILE_NAME': path, # path to saved model\n",
    "    'RESULT_DIR_NAME': '',                                  # name of subdirectory within 'results' folder where to store the txt result files (for submission), e.g., 'distilbert_base_threshold_0_4'\n",
    "    'EVALUATION_THRESHOLD': 0.5,                            # select the confidence threshold for when to assign a label. Default is 0.5\n",
    "    'MODEL_NAME': 'bert-base-uncased',\n",
    "    'MODEL_TITLE': 'title',\n",
    "    'ONLY_TEST': False,                                      # set to `True` ONLY IF running on test set (not dev set or anything else)\n",
    "    'EVALUATION_SET': './testdata/s1_val.csv',       # path to csv with evaluation set\n",
    "    'LANGUAGES': ['en'],    # select which languages to evaluate, packaged in a list. # e.g. ['en', 'fr', 'ge', 'it', 'po', 'ru', 'ka', 'gr', 'es'],\n",
    "\n",
    "    # test params\n",
    "    'BATCH_SIZE': 32,\n",
    "    'SHUFFLE': False,  \n",
    "    'NUM_WORKERS': 0\n",
    "}\n",
    "\n",
    "#run_eval(args_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4caa3b5-416c-4579-9325-e9fdfef5cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert_base.pth', 'mbert.pth', 'roberta_base.pth', 'roberta_large.pth', 'xlm_roberta_base.pth', 'xlm_roberta_large.pth']\n"
     ]
    }
   ],
   "source": [
    "path = r'change this to where the models are saved'\n",
    "models_path = os.listdir(r'change this to where the models are saved')\n",
    "\n",
    "models = [i for i in models_path if 'weights' not in i]\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d9cbcc8-59fe-4ee3-ba7b-bdae193351bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding techniques into one-hot-encoded lists!\n",
      "\n",
      "max length of tokens for < 512 > is: < 512 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Goat\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "16it [00:03,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.214\n",
      "F1 Score (Micro) = 0.291015625\n",
      "F1 Score (Macro) = 0.10118576656955787\n",
      "[[['en'], 0.214, 0.291015625, 0.10118576656955787]]\n",
      "Encoding techniques into one-hot-encoded lists!\n",
      "\n",
      "max length of tokens for < 512 > is: < 512 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Goat\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "16it [00:03,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.21\n",
      "F1 Score (Micro) = 0.1218961625282167\n",
      "F1 Score (Macro) = 0.028919578560726886\n",
      "[[['en'], 0.21, 0.1218961625282167, 0.028919578560726886]]\n",
      "Encoding techniques into one-hot-encoded lists!\n",
      "\n",
      "max length of tokens for < 512 > is: < 512 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Goat\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "16it [00:03,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.136\n",
      "F1 Score (Micro) = 0.15894039735099338\n",
      "F1 Score (Macro) = 0.03899035700807065\n",
      "[[['en'], 0.136, 0.15894039735099338, 0.03899035700807065]]\n",
      "Encoding techniques into one-hot-encoded lists!\n",
      "\n",
      "max length of tokens for < 512 > is: < 512 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Goat\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "16it [00:10,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.116\n",
      "F1 Score (Micro) = 0.13050847457627118\n",
      "F1 Score (Macro) = 0.033793371329977485\n",
      "[[['en'], 0.116, 0.13050847457627118, 0.033793371329977485]]\n",
      "Encoding techniques into one-hot-encoded lists!\n",
      "\n",
      "max length of tokens for < 512 > is: < 512 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Goat\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "16it [00:03,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.2\n",
      "F1 Score (Micro) = 0.0731995277449823\n",
      "F1 Score (Macro) = 0.026169902367859946\n",
      "[[['en'], 0.2, 0.0731995277449823, 0.026169902367859946]]\n",
      "Encoding techniques into one-hot-encoded lists!\n",
      "\n",
      "max length of tokens for < 512 > is: < 512 >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Goat\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "16it [00:03,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.188\n",
      "F1 Score (Micro) = 0.010217113665389528\n",
      "F1 Score (Macro) = 0.0029651593773165306\n",
      "[[['en'], 0.188, 0.010217113665389528, 0.0029651593773165306]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "for i in models:\n",
    "    args_eval['MODEL_FILE_NAME'] = os.path.join(path, i)\n",
    "    args_eval['MODEL_TITLE'] = i.split('.')[0]\n",
    "    results = run_eval(args_eval)\n",
    "    results_df = pd.concat([results_df, results])\n",
    "    results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ecf1b9f-e484-4a1f-9647-f1dbc7330e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>F1_macro</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[en]</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>0.101186</td>\n",
       "      <td>bert_base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[en]</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.121896</td>\n",
       "      <td>0.028920</td>\n",
       "      <td>mbert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[en]</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.158940</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>roberta_base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[en]</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.130508</td>\n",
       "      <td>0.033793</td>\n",
       "      <td>roberta_large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[en]</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.026170</td>\n",
       "      <td>xlm_roberta_base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[en]</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.010217</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>xlm_roberta_large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language  accuracy  F1_micro  F1_macro              model\n",
       "0     [en]     0.214  0.291016  0.101186          bert_base\n",
       "0     [en]     0.210  0.121896  0.028920              mbert\n",
       "0     [en]     0.136  0.158940  0.038990       roberta_base\n",
       "0     [en]     0.116  0.130508  0.033793      roberta_large\n",
       "0     [en]     0.200  0.073200  0.026170   xlm_roberta_base\n",
       "0     [en]     0.188  0.010217  0.002965  xlm_roberta_large"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401b4d6-5d95-4687-84c5-bb8358eb4513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "51ccb613-aee6-4116-8f6c-771dad6d4ad9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
