{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd94236b-2b06-4689-8be4-cd11119163b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e14858-a87f-478a-a57c-dfb680b6743c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13827ed-9589-44cc-851a-5678be761ece",
   "metadata": {},
   "source": [
    "Notebook uses modified version of another task set of transformer codes. The text data is just converted back to .csv with the correct headers - can skip to the next steps if the CSV are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691edcf7-0ad6-4a83-a8e8-0b00a6dd6bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65635</td>\n",
       "      <td>THIS IS WHY YOU NEED\\n\\nA SHARPIE WITH YOU AT ...</td>\n",
       "      <td>[Black-and-white Fallacy/Dictatorship]</td>\n",
       "      <td>https://www.facebook.com/photo/?fbid=402355213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67927</td>\n",
       "      <td>GOOD NEWS!\\n\\nNAZANIN ZAGHARI-RATCLIFFE AND AN...</td>\n",
       "      <td>[Loaded Language, Glittering generalities (Vir...</td>\n",
       "      <td>https://www.facebook.com/amnesty/photos/531198...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68031</td>\n",
       "      <td>PAING PHYO MIN IS FREE!</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/amnesty/photos/427419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77490</td>\n",
       "      <td>Move your ships away!\\n\\noooook\\n\\nMove your s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/rightpatriots/photos/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67641</td>\n",
       "      <td>WHEN YOU'RE THE FBI, THEY LET YOU DO IT.</td>\n",
       "      <td>[Thought-terminating cliché]</td>\n",
       "      <td>https://www.facebook.com/AddictingInfoOrg/phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>78774</td>\n",
       "      <td>Level: Easy\\n\\nLevel: Hard\\n\\nLevel: For Mothe...</td>\n",
       "      <td>[Slogans, Name calling/Labeling, Repetition]</td>\n",
       "      <td>https://www.facebook.com/themotherlandcalls/ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>78299</td>\n",
       "      <td>Europeans:\\n\\nYou are trespassing</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/communism101/photos/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>67096</td>\n",
       "      <td>\\NONSENSE DETECTED! \\nFIRE AT WILL!\\\\n\\n\\Time ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.facebook.com/photo/?fbid=181158225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>68464</td>\n",
       "      <td>ANNA AND VIRA:\\nATTACKED FOR DEFENDING LGBTI A...</td>\n",
       "      <td>[Causal Oversimplification, Loaded Language]</td>\n",
       "      <td>https://www.facebook.com/amnesty/photos/490311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>71038</td>\n",
       "      <td>\\Our food system belongs in the hands of many ...</td>\n",
       "      <td>[Name calling/Labeling, Appeal to authority, B...</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0    65635  THIS IS WHY YOU NEED\\n\\nA SHARPIE WITH YOU AT ...   \n",
       "1    67927  GOOD NEWS!\\n\\nNAZANIN ZAGHARI-RATCLIFFE AND AN...   \n",
       "2    68031                            PAING PHYO MIN IS FREE!   \n",
       "3    77490  Move your ships away!\\n\\noooook\\n\\nMove your s...   \n",
       "4    67641           WHEN YOU'RE THE FBI, THEY LET YOU DO IT.   \n",
       "..     ...                                                ...   \n",
       "495  78774  Level: Easy\\n\\nLevel: Hard\\n\\nLevel: For Mothe...   \n",
       "496  78299                  Europeans:\\n\\nYou are trespassing   \n",
       "497  67096  \\NONSENSE DETECTED! \\nFIRE AT WILL!\\\\n\\n\\Time ...   \n",
       "498  68464  ANNA AND VIRA:\\nATTACKED FOR DEFENDING LGBTI A...   \n",
       "499  71038  \\Our food system belongs in the hands of many ...   \n",
       "\n",
       "                                                labels  \\\n",
       "0               [Black-and-white Fallacy/Dictatorship]   \n",
       "1    [Loaded Language, Glittering generalities (Vir...   \n",
       "2                                                   []   \n",
       "3                                                   []   \n",
       "4                         [Thought-terminating cliché]   \n",
       "..                                                 ...   \n",
       "495       [Slogans, Name calling/Labeling, Repetition]   \n",
       "496                                                 []   \n",
       "497                                                 []   \n",
       "498       [Causal Oversimplification, Loaded Language]   \n",
       "499  [Name calling/Labeling, Appeal to authority, B...   \n",
       "\n",
       "                                                  link  \n",
       "0    https://www.facebook.com/photo/?fbid=402355213...  \n",
       "1    https://www.facebook.com/amnesty/photos/531198...  \n",
       "2    https://www.facebook.com/amnesty/photos/427419...  \n",
       "3    https://www.facebook.com/rightpatriots/photos/...  \n",
       "4    https://www.facebook.com/AddictingInfoOrg/phot...  \n",
       "..                                                 ...  \n",
       "495  https://www.facebook.com/themotherlandcalls/ph...  \n",
       "496  https://www.facebook.com/communism101/photos/5...  \n",
       "497  https://www.facebook.com/photo/?fbid=181158225...  \n",
       "498  https://www.facebook.com/amnesty/photos/490311...  \n",
       "499                                               null  \n",
       "\n",
       "[7500 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = 'X:\\\\PhD\\\\SemEval Task4\\\\Data' # modify to your local path\n",
    "\n",
    "\n",
    "anno_subtask1_train = pd.read_json('X:\\\\PhD\\\\SemEval Task4\\\\Data\\\\annotations\\\\data\\\\subtask1\\\\train.json')\n",
    "anno_subtask1_val = pd.read_json('X:\\\\PhD\\\\SemEval Task4\\\\Data\\\\annotations\\\\data\\\\subtask1\\\\validation.json')\n",
    "anno_subtask1_dev = pd.read_json('X:\\\\PhD\\\\SemEval Task4\\\\Data\\\\annotations\\\\data\\\\subtask1\\\\dev_unlabeled.json')\n",
    "\n",
    "anno_subtask1_combined = pd.concat([anno_subtask1_train, anno_subtask1_val])\n",
    "anno_subtask1_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53215860-9248-485c-82e0-5204b5576029",
   "metadata": {},
   "source": [
    "## CSV Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8899b0a3-8f4e-4b83-86a0-dfeafcbf8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_subtask1_train.drop(columns=['link']).to_csv('./testdata/train/s1_train.csv')\n",
    "anno_subtask1_val.drop(columns=['link']).to_csv('./testdata/val/s1_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdaf41c3-6c8f-4319-947a-495691577f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Appeal to fear/prejudice',\n",
       " 'Whataboutism',\n",
       " 'Flag-waving',\n",
       " 'Appeal to authority',\n",
       " 'Black-and-white Fallacy/Dictatorship',\n",
       " 'Bandwagon',\n",
       " 'Slogans',\n",
       " 'Obfuscation, Intentional vagueness, Confusion',\n",
       " 'Smears',\n",
       " 'Exaggeration/Minimisation',\n",
       " 'Thought-terminating cliché',\n",
       " 'Presenting Irrelevant Data (Red Herring)',\n",
       " 'Name calling/Labeling',\n",
       " 'Repetition',\n",
       " 'Loaded Language',\n",
       " \"Misrepresentation of Someone's Position (Straw Man)\",\n",
       " 'Doubt',\n",
       " 'Reductio ad hitlerum',\n",
       " 'Causal Oversimplification',\n",
       " 'Glittering generalities (Virtue)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set([x for xs in anno_subtask1_combined['labels'].to_list() for x in xs]))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0f951-aa50-4062-bc15-878879bb0ee9",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59968382-a91a-486a-9cb1-07772238fed5",
   "metadata": {},
   "source": [
    "Modified version of: https://github.com/kinit-sk/semeval2023-task3-persuasion-techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0341ceb6-5a1a-45f6-bb4a-0369f4134200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reqs\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import AutoTokenizer\n",
    "import ast\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f549086-6f3d-41a3-b55f-77ccbbaadec2",
   "metadata": {},
   "source": [
    "## Transformer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f62221f-1249-4da7-9f2f-212fae5c1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 19 # number of classes, hardcoded but should change depending on data read in:\n",
    "\n",
    "# change anno_task1_combined to whatever the csv is\n",
    "#labels = list(set([x for xs in anno_subtask1_combined['labels'].to_list() for x in xs]))\n",
    "# classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad13423-46be-40c0-bd6b-4943d06d24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBase, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, classes) # number of classes/techniques\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EnglishRobertaBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnglishRobertaBase, self).__init__()\n",
    "        self.l1 = transformers.RobertaModel.from_pretrained('roberta-base', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EnglishRobertaLarge(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnglishRobertaLarge, self).__init__()\n",
    "        self.l1 = transformers.RobertaModel.from_pretrained('roberta-large', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(1024, classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class mBERTBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mBERTBase, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-multilingual-cased', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class XLMRobertaBase(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaBase, self).__init__()\n",
    "        self.l1 = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-base', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class XLMRobertaLarge(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XLMRobertaLarge, self).__init__()\n",
    "        self.l1 = transformers.XLMRobertaModel.from_pretrained('xlm-roberta-large', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(1024, classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f13ba4-fd26-48d0-a393-10f96bc8d027",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "Data loader in this instance uses a dataframe, which is the csv converted. Can be changed just to handle csv but either seems fine as the data is small. At present the language part of the models (e.g., selecting the languages used in the data) is excluded and commented out until we use multilingual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e053e9c6-d502-44f8-8b0e-bac177ced3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.techniques_encoded\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dataset_training(MODEL_NAME, PATHS_TO_TRAIN, PATHS_TO_VALIDATION, TRAINING_LANGUAGES):\n",
    "    \"\"\"\n",
    "    Function to load and process the _input_data into suitable format. Includes setting up tokenizer, one hot encoding of\n",
    "    techniques, concatenation of csv files if necessary, language filtering and loading _input_data into CustomDataset class.\n",
    "\n",
    "    :param MODEL_NAME: name of the model as per Hugging face to use for tokenizer.\n",
    "    :param PATHS_TO_TRAIN: List of paths to csv files we want to use for training.\n",
    "    :param PATHS_TO_VALIDATION: List of paths to csv files we want to use for validation.\n",
    "    :param TRAINING_LANGUAGES: List of languages we are using.\n",
    "    :return: Tuple of training and testing set in Dataset format.\n",
    "    \"\"\"\n",
    "\n",
    "    #Create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, return_dict=False)\n",
    "    MAX_LEN = tokenizer.model_max_length\n",
    "    if MAX_LEN > 1024:\n",
    "        print(f'{MAX_LEN} greater than model len, setting down to 512.')\n",
    "        MAX_LEN = 512\n",
    "    print(f'Max length of tokens for < {MODEL_NAME} > is: < {MAX_LEN} >')\n",
    "\n",
    "    # Concatenate csv files\n",
    "    # useful for including all files in training\n",
    "    df_train = pd.DataFrame()\n",
    "    for train_path in PATHS_TO_TRAIN:\n",
    "        df_train = pd.concat([df_train, pd.read_csv(train_path)], ignore_index=True)\n",
    "\n",
    "    # One hot encoding of techniques\n",
    "    df_train['labels'] = df_train['labels'].apply(ast.literal_eval)\n",
    "    one_hot = MultiLabelBinarizer()\n",
    "    df_train['techniques_encoded'] = one_hot.fit_transform(df_train['labels'])[:, 1:].tolist()\n",
    "\n",
    "    #Load validation dataset and encode\n",
    "    if PATHS_TO_VALIDATION:\n",
    "        df_dev = pd.DataFrame()\n",
    "        for dev_path in PATHS_TO_VALIDATION:\n",
    "            df_dev = pd.concat([df_dev, pd.read_csv(dev_path)], ignore_index=True)\n",
    "\n",
    "        df_dev['labels'] = df_dev['labels'].apply(ast.literal_eval)\n",
    "        # use fitted one-hot encoder transform in case some techniques are present in training but not in dev\n",
    "        df_dev['techniques_encoded'] = one_hot.transform(df_dev['labels'])[:, 1:].tolist()\n",
    "    else:\n",
    "        df_dev = pd.DataFrame(\n",
    "            columns=[['id', 'text', 'labels', 'techniques_encoded']])\n",
    "\n",
    "    #Language filtering - Not Needed yet\n",
    "    # df_train = df_train[df_train['language'].isin(TRAINING_LANGUAGES)]\n",
    "    # df_dev = df_dev[df_dev['language'].isin(TRAINING_LANGUAGES)]\n",
    "    \n",
    "    train_dataset = df_train\n",
    "    test_dataset = df_dev\n",
    "\n",
    "    print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    print(f\"TEST Dataset: {test_dataset.shape}\")\n",
    "\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "    testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "    return training_set, testing_set\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, return_dict=False)\n",
    "    MAX_LEN = tokenizer.model_max_length\n",
    "    if MAX_LEN > 1024:\n",
    "        print(f'{MAX_LEN} greater than model len, setting down to 512.')\n",
    "        MAX_LEN = 512\n",
    "    print(f'Max length of tokens for < {MODEL_NAME} > is: < {MAX_LEN} >')\n",
    "\n",
    "\n",
    "def load_dataframe(args):\n",
    "    test_df = pd.read_csv(args[\"EVALUATION_SET\"])\n",
    "\n",
    "    # language_to_texts = {lan: list(text) for lan, text in test_df.groupby('language')['text']}\n",
    "    # for language, texts in language_to_texts.items():\n",
    "    #     print(f'language {language} has {len(texts)} lines of data.')\n",
    "\n",
    "    if not args['ONLY_TEST']:\n",
    "        print('Encoding techniques into one-hot-encoded lists!\\n')\n",
    "        # Create sentence and label lists\n",
    "        test_sentences = test_df.text.values\n",
    "\n",
    "        test_df.techniques = test_df.techniques.apply(lambda x: ast.literal_eval(x))\n",
    "        one_hot = MultiLabelBinarizer()\n",
    "        test_df['techniques_encoded'] = one_hot.fit_transform(test_df['labels'])[:, 1:].tolist()\n",
    "        test_labels = test_df.techniques_encoded.values\n",
    "        test_labels = np.array([labels for labels in test_labels])\n",
    "\n",
    "        mappings = one_hot.classes_[1:]\n",
    "    else:\n",
    "        print('No targets to create one-hot-encodings from! Loading from pre-set list instead...\\n')\n",
    "        mappings = np.array(['Reductio ad hitlerum',\n",
    "                             \"Misrepresentation of Someone's Position (Straw Man)\",\n",
    "                             'Repetition',\n",
    "                             'Name calling/Labeling',\n",
    "                             'Presenting Irrelevant Data (Red Herring)',\n",
    "                             'Glittering generalities (Virtue)',\n",
    "                             'Bandwagon',\n",
    "                             'Loaded Language',\n",
    "                             'Smears',\n",
    "                             'Exaggeration/Minimisation',\n",
    "                             'Black-and-white Fallacy/Dictatorship',\n",
    "                             'Thought-terminating cliché',\n",
    "                             'Appeal to fear/prejudice',\n",
    "                             'Causal Oversimplification',\n",
    "                             'Obfuscation, Intentional vagueness, Confusion',\n",
    "                             'Flag-waving',\n",
    "                             'Slogans',\n",
    "                             'Whataboutism',\n",
    "                             'Doubt',\n",
    "                             'Appeal to authority']) #list from oue dataset\n",
    "\n",
    "    return test_df, mappings\n",
    "\n",
    "def create_dataloader(args, test_df, language, language_counts):\n",
    "    # print(\n",
    "    #     f'testing on {language_counts[language]} data points (around {language_counts[language] / args[\"BATCH_SIZE\"]} iterations)')\n",
    "    # filtered_test_df = test_df.copy()\n",
    "\n",
    "    # not required until we use multilingual data\n",
    "    # filtered_test_df = filtered_test_df[filtered_test_df['language'] == language]\n",
    "    # filtered_test_df = filtered_test_df.reset_index()\n",
    "    \n",
    "    tokenizer, MAX_LEN = get_tokenizer(args['MODEL_NAME'])\n",
    "\n",
    "    testing_set = CustomDataset(\n",
    "        test_df,\n",
    "        tokenizer,\n",
    "        MAX_LEN\n",
    "    )\n",
    "\n",
    "    testing_loader = DataLoader(\n",
    "        testing_set,\n",
    "        batch_size=args['BATCH_SIZE'],\n",
    "        shuffle=args['SHUFFLE'],\n",
    "        num_workers=args['NUM_WORKERS']\n",
    "    )\n",
    "\n",
    "    return testing_loader, filtered_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c7c90-9e64-493e-8c90-74d3e175b63d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d14344-a643-490c-aede-4c9c9563359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(args):\n",
    "    \"\"\"\n",
    "    Function creates a dataset based on given arguments. Afterwards it trains and validates a model.\n",
    "\n",
    "    :param args: dictionary of arguments needed to create the model\n",
    "    \"\"\"\n",
    "    training_set, testing_set = create_dataset_training(MODEL_NAME=args['MODEL_NAME'], PATHS_TO_TRAIN=args['PATHS_TO_TRAIN'], PATHS_TO_VALIDATION=args['PATHS_TO_VALIDATION'], TRAINING_LANGUAGES=args['TRAINING_LANGUAGES'])\n",
    "\n",
    "    training_loader = DataLoader(\n",
    "        training_set,\n",
    "        batch_size=args['TRAIN_BATCH_SIZE'],\n",
    "        shuffle=args['TRAIN_SHUFFLE'],\n",
    "        num_workers=args['TRAIN_NUM_WORKERS']\n",
    "    )\n",
    "\n",
    "    validation_loader = DataLoader(\n",
    "        testing_set,\n",
    "        batch_size=args['VALIDATION_BATCH_SIZE'],\n",
    "        shuffle=args['VALIDATION_SHUFFLE'],\n",
    "        num_workers=args['VALIDATION_NUM_WORKERS']\n",
    "    )\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    lm = LanguageModel(training_loader=training_loader, validation_loader=validation_loader, device=device, args=args)\n",
    "    return lm.train_over_epochs()\n",
    "\n",
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"\n",
    "    Class to hold the model as well as train and validate it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_loader, validation_loader, device, args):\n",
    "\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.validation_loader = validation_loader\n",
    "        self.training_loader = training_loader\n",
    "        if args['MODEL_NAME'] == 'roberta-large':\n",
    "            self.model = EnglishRobertaLarge()\n",
    "        elif args['MODEL_NAME'] == 'roberta-base':\n",
    "            self.model = EnglishRobertaBase()\n",
    "        elif args['MODEL_NAME'] == 'bert-base-uncased':\n",
    "            self.model = BERTBase()\n",
    "        elif args['MODEL_NAME'] == 'bert-base-multilingual-cased':\n",
    "            self.model = mBERTBase()\n",
    "        elif args['MODEL_NAME'] == 'xlm-roberta-base':\n",
    "            self.model = XLMRobertaBase()\n",
    "        elif args['MODEL_NAME'] == 'xlm-roberta-large':\n",
    "            self.model = XLMRobertaLarge()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=self.args['LEARNING_RATE'])\n",
    "        if args['FREEZE']:\n",
    "            counter = 0\n",
    "            no_layers = len([param for param in self.model.parameters()])\n",
    "            for param in self.model.parameters():\n",
    "                counter += 1\n",
    "                param.requires_grad = False\n",
    "                if counter == (no_layers//5) * 4:\n",
    "                    break\n",
    "\n",
    "\n",
    "    def save_model_and_weights(self, epoch):\n",
    "        print(f'\\n\\n --- Currently saving model! --- \\n')\n",
    "        model_path = f'{self.args[\"MODELS_DIR\"]}{self.args[\"MODEL_NAME_PREFIX\"]}_model_{epoch}epoch_batch{self.args[\"TRAIN_BATCH_SIZE\"]}_lr{self.args[\"LEARNING_RATE\"]}.pth'\n",
    "        model_weights_path = f'{self.args[\"MODELS_DIR\"]}{self.args[\"MODEL_NAME_PREFIX\"]}_model_weights_{epoch}epoch_batch{self.args[\"TRAIN_BATCH_SIZE\"]}_lr{self.args[\"LEARNING_RATE\"]}.pth'\n",
    "        torch.save(self.model.state_dict(), model_weights_path)\n",
    "        torch.save(self.model, model_path)\n",
    "        return model_path\n",
    "\n",
    "    def train(self, epoch):\n",
    "        print(f'\\nNow training on Epoch {epoch}.')\n",
    "        print(\n",
    "            f'Testing on {len(self.training_loader) * self.training_loader.batch_size - 1} data points (around {len(self.training_loader)} iterations)')\n",
    "\n",
    "        self.args['CURRENT_TRAIN_LOSS'] = 0\n",
    "        self.model.train()\n",
    "        for itr, data in tqdm(enumerate(self.training_loader, 0)):\n",
    "            ids = data['ids'].to(self.device, dtype=torch.long)\n",
    "            mask = data['mask'].to(self.device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "            targets = data['targets'].to(self.device, dtype=torch.float)\n",
    "\n",
    "            outputs = self.model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "            if itr % 200 == 0:\n",
    "                print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.args['CURRENT_TRAIN_LOSS'] += loss\n",
    "\n",
    "        print(f'Accumulated Training Loss after Epoch {epoch} is: {self.args[\"CURRENT_TRAIN_LOSS\"]}')\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        print(f'\\nNow validating on Epoch {epoch}.')\n",
    "        print(\n",
    "            f'Validating on {len(self.validation_loader) * self.validation_loader.batch_size} data points (around {len(self.validation_loader)} iterations)')\n",
    "        self.model.eval()\n",
    "        self.args['CURRENT_VALIDATION_LOSS'] = 0\n",
    "        with torch.no_grad():\n",
    "            for val_itr, data in tqdm(enumerate(self.validation_loader)):\n",
    "                ids = data['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = data['mask'].to(self.device, dtype=torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                targets = data['targets'].to(self.device, dtype=torch.float)\n",
    "\n",
    "                outputs = self.model(ids, mask, token_type_ids)\n",
    "                loss = torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "                if val_itr % 200 == 0:\n",
    "                    print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "\n",
    "                # accumulate the loss across all batches\n",
    "                self.args['CURRENT_VALIDATION_LOSS'] += loss\n",
    "\n",
    "                # clear cache\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        print(f'Accumulated Validation Loss after Epoch {epoch} is: {self.args[\"CURRENT_VALIDATION_LOSS\"]}')\n",
    "\n",
    "        # Early stopping if necessary\n",
    "        if self.args['CURRENT_VALIDATION_LOSS'] < self.args['BEST_LOSS']:\n",
    "            self.args['BEST_LOSS'] = self.args['CURRENT_VALIDATION_LOSS']\n",
    "            self.save_model_and_weights(epoch)\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            # stop training\n",
    "            print('Validation loss increased! Stopping training...')\n",
    "            print(f'Last validation loss was: {self.args[\"BEST_LOSS\"]}')\n",
    "            print(f'Current validation loss is: {self.args[\"CURRENT_VALIDATION_LOSS\"]}')\n",
    "\n",
    "            return False\n",
    "\n",
    "    def train_over_epochs(self):\n",
    "        for epoch in range(self.args['EPOCHS']):\n",
    "            self.train(epoch)\n",
    "            if self.args['PATHS_TO_VALIDATION'] and not self.validate(epoch):\n",
    "                break  # the last epoch was NOT counted due to early stopping\n",
    "            else:\n",
    "                # store latest version\n",
    "                path = self.save_model_and_weights(epoch)\n",
    "        print(\"Done.\")\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8316e6-79ea-4d40-8e3f-147f51b4e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': '',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'xlm-roberta-large',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "                                                            # unfreeze for PTC fine-tuning?\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86e13755-203d-470c-bd92-2bed9c5b7ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < bert-base-multilingual-cased > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n",
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.6773495674133301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [08:57,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.28486016392707825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [18:08,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2414165884256363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [26:55,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.27533861994743347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [35:55,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2402072250843048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [39:31,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 231.0172576904297\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.25061848759651184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:09,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.706016540527344\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2030051052570343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [14:52,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.18518470227718353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [29:44,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.17730334401130676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [44:36,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2838349938392639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [59:28,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.3091575801372528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [1:05:03,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 199.37339782714844\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2498093694448471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:05, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 14.063506126403809\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.19667458534240723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [16:13,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2836587131023407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [32:46,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2297101765871048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [48:53,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.17762181162834167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [1:04:27,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.18972259759902954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [1:10:12,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 187.03121948242188\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2492693066596985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 14.015921592712402\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.254861980676651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [15:38,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.14487753808498383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [31:32,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.12822268903255463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [46:48,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.17995351552963257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [1:01:25,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.155323788523674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [1:06:53,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 173.15943908691406\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2323458343744278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:07,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 13.681135177612305\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1548755168914795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [11:57,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.21165232360363007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [23:46,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.11364316940307617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [35:34,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.11460655927658081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [47:51,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.15762251615524292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [52:32,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 159.1197052001953\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.23882943391799927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:16,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 13.922234535217285\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 13.681135177612305\n",
      "Current validation loss is: 13.922234535217285\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'bert-base-multilingual-cased',                                \n",
    "    'EPOCHS': 5,                                            \n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'bert-base-multilingual-cased',                      \n",
    "    'MODELS_DIR': './unimodal-baselines/',              \n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    \n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                             \n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], \n",
    "    'FREEZE': False,                                        \n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eef5fd12-a8ee-4b4b-a358-34ab1c0c21c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b171c75021484ae8953218ec54a8c989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf6674a272e4fe290f85cf12cc26807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9486dbdd14e94887af640fd4f4c6e58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f489dd6c4a74208afebf1ee52a917ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < roberta-large > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47db649cf34562a3f426ca9ffb208b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.7196247577667236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [02:58,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.14794889092445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [05:57,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.19413061439990997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [08:55,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.1944851577281952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [11:54,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2094479501247406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [13:01,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 212.99249267578125\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2421095073223114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:17,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 13.489632606506348\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20842787623405457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:40,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.23920315504074097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:21,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.18115803599357605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20896288752555847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:44,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.13941295444965363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:21,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 173.28250122070312\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20948049426078796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 12.475747108459473\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.14728398621082306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:42,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.19809940457344055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:23,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.18450607359409332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:04,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.17762085795402527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:45,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.17555472254753113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:23,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 150.40342712402344\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.20318101346492767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 12.033329010009766\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.18346114456653595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:41,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1284378319978714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:22,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.17629924416542053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:03,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.16613200306892395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:44,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1318967193365097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:22,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 127.09971618652344\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.20633022487163544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 12.021674156188965\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1886201947927475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [01:41,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.10580242425203323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [03:22,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.21124684810638428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [05:03,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.061535660177469254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [06:44,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1605372130870819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [07:22,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 103.2136001586914\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.18887023627758026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:10,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 12.16552734375\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 12.021674156188965\n",
      "Current validation loss is: 12.16552734375\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'roberta-large',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'roberta-large',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d541fef-4eb8-4d1f-98e2-d54b7c764486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3f607e2c51482ead775e43f59b5e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989a653391c64173a4d630b8a47d8dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c1b3edfc6c4bfc9184e3092e8aa0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0d13438c774fb5bb58d087ca4a7e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < roberta-base > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f494935388ed448494898940e43a1e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.7152462005615234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "201it [00:32,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2909088134765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:04,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2278163880109787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:37,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.27329012751579285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:09,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.15474048256874084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:22,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 225.4221954345703\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2550155222415924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.2190523147583\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2324271947145462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.16518595814704895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:05,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1875995248556137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.28147995471954346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:10,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1673932522535324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:22,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 186.32395935058594\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.24053393304347992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 13.328704833984375\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.24704541265964508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:33,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.21066510677337646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:05,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.15657512843608856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2531723976135254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:11,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.22788824141025543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:23,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 167.74563598632812\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.24051441252231598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 12.881023406982422\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1890919804573059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.16152901947498322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:05,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1279347985982895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.13523069024085999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:10,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.21843478083610535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:22,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 152.10427856445312\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.23517917096614838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 12.661491394042969\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.12344952672719955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:33,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.15436004102230072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:06,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.09720074385404587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:38,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.09447702765464783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:11,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.13167890906333923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:23,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 136.10641479492188\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.23383788764476776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 12.972590446472168\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 12.661491394042969\n",
      "Current validation loss is: 12.972590446472168\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'roberta-base',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'roberta-base',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0b3fddf-ca04-4d6a-beac-4726afb2b8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < bert-base-uncased > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n",
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.696268618106842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2879495322704315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:05,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.22446462512016296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:37,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.32396551966667175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:10,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.20854122936725616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:22,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 239.33941650390625\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.24263863265514374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.650721549987793\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.16422156989574432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1819259077310562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:04,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.19665394723415375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:37,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.16881383955478668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:09,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2853495180606842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:21,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 191.20123291015625\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.24059787392616272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 13.617776870727539\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1347614824771881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.14323560893535614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:04,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1767999827861786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:37,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2491309493780136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:09,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.14882831275463104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:21,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 172.392578125\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.2440255582332611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 13.348442077636719\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.16594339907169342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2146722972393036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:04,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2504252791404724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:37,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2740512788295746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:09,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.21248029172420502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:21,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 153.98239135742188\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.24175171554088593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 13.213183403015137\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.12470854073762894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:32,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.17496167123317719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:04,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1564170867204666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:37,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.18935121595859528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:09,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.12412610650062561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:21,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 135.36709594726562\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.24746909737586975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 13.294083595275879\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 13.213183403015137\n",
      "Current validation loss is: 13.294083595275879\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'bert-base-uncased',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'bert-base-uncased',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f86b8434-b17d-4945-964d-ea75402ea03a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < bert-base-multilingual-cased > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n",
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.6666839718818665"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:33,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.23557336628437042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:07,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2597467601299286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:41,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.256039559841156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:15,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.26465317606925964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:27,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 228.52093505859375\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.24523764848709106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.681147575378418\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.261654794216156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2212589681148529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:08,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.22010673582553864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:42,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.24758169054985046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:16,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1730608195066452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:28,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 196.7558135986328\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2491392344236374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 13.986952781677246\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.21785090863704681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1267060935497284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:08,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.18764036893844604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:42,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1653461754322052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:16,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.1524803787469864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:28,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 183.50254821777344\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.25220245122909546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 13.564451217651367\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.15715853869915009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.18217726051807404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:07,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.21207435429096222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:41,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.19616058468818665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:15,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.13681496679782867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:27,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 168.31837463378906\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2412455677986145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 13.5440092086792\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.16728709638118744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:34,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1010003387928009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:07,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.2247316986322403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:41,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.2190481722354889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:15,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1160217821598053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:27,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 153.6635284423828\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.2382974624633789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 13.570302963256836\n",
      "Validation loss increased! Stopping training...\n",
      "Last validation loss was: 13.5440092086792\n",
      "Current validation loss is: 13.570302963256836\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'bert-base-multilingual-cased',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'bert-base-multilingual-cased',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "205e6efc-f3ac-4fe8-9464-113e3bdaa09e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8bf067944044a3b5a0b326f4312037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387c0c5670784ec2a7fe973dbe741387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7367c5eba04db08c020bef44b9b39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of tokens for < xlm-roberta-base > is: < 512 >\n",
      "TRAIN Dataset: (7000, 5)\n",
      "TEST Dataset: (500, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0be639c5ffa40e98283e198a8b4b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training on Epoch 0.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\users\\goat\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "1it [00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.7227253913879395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:35,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.23435835540294647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:11,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2802368104457855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:47,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2693515717983246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:23,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2652474045753479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:36,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 0 is: 236.19778442382812\n",
      "\n",
      "Now validating on Epoch 0.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 21.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss:  0.2521989345550537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 0 is: 14.92094898223877\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 1.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.22549903392791748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:36,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.1729382574558258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:12,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20894300937652588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:48,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.20800527930259705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:24,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.2513335943222046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:37,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 1 is: 203.90008544921875\n",
      "\n",
      "Now validating on Epoch 1.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss:  0.25143009424209595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 1 is: 14.571717262268066\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 2.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.22406335175037384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:36,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.23353950679302216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:12,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.26549744606018066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:48,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.21081149578094482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:25,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.14100949466228485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:38,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 2 is: 199.7682342529297\n",
      "\n",
      "Now validating on Epoch 2.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 19.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss:  0.255021333694458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 2 is: 14.353126525878906\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 3.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.1511322259902954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:36,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.23527321219444275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:12,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.2530156672000885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:48,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.19035103917121887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:23,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.24796320497989655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:36,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 3 is: 192.7353973388672\n",
      "\n",
      "Now validating on Epoch 3.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss:  0.25359097123146057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 3 is: 13.806134223937988\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "Now training on Epoch 4.\n",
      "Testing on 6999 data points (around 875 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.17935985326766968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:35,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.20921297371387482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [01:11,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.19051088392734528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "601it [01:47,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.1933344006538391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [02:22,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.25753507018089294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "875it [02:35,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Training Loss after Epoch 4 is: 180.2266082763672\n",
      "\n",
      "Now validating on Epoch 4.\n",
      "Validating on 504 data points (around 63 iterations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 20.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss:  0.23572345077991486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:03, 19.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated Validation Loss after Epoch 4 is: 13.673762321472168\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "\n",
      "\n",
      " --- Currently saving model! --- \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "args_train = {\n",
    "    # general\n",
    "    'MODEL_NAME_PREFIX': 'xlm-roberta-base',                                # so that model names don't get overwritten, write a UNIQUE name prefix to distinguish this current model\n",
    "    'EPOCHS': 5,                                            # Make sure to set the EXACT amount of epochs for non-validating runs. Typically BERT-like models won't need to go over 4 epochs\n",
    "    'LEARNING_RATE': 1e-05,\n",
    "    'MODEL_NAME': 'xlm-roberta-base',                      # model name as per hugging face library, currently supported model names are: roberta-large, roberta-base, bert-base-uncased, bert-base-multilingual-cased, xlm-roberta-base and xlm-roberta-large\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'MODELS_DIR': './unimodal-baselines/',               # name of subdirectory within 'models' folder where the trained models will be saved, e.g., 'distilbert_base_threshold_0_4'\n",
    "    'PATHS_TO_TRAIN': ['./testdata/s1_train.csv'],    # useful when training on more sets (e.g. train and dev), list of paths to the csv files\n",
    "    'PATHS_TO_VALIDATION': ['./testdata/s1_val.csv'],                              # leave empty if no validation required\n",
    "\n",
    "    # for training\n",
    "    'TRAIN_BATCH_SIZE': 8,\n",
    "    'TRAIN_SHUFFLE': True,\n",
    "    'TRAIN_NUM_WORKERS': 0,\n",
    "    'CURRENT_EPOCH': 0,\n",
    "    'CURRENT_TRAIN_LOSS': 0,\n",
    "    'TRAINING_LANGUAGES': ['en'], # full list is ['en', 'fr', 'ge', 'it', 'po', 'ru']\n",
    "    'FREEZE': False,                                        # If True first 80% of the layers will be frozen during training\n",
    "\n",
    "    # for validation\n",
    "    'VALIDATION_BATCH_SIZE': 8,\n",
    "    'VALIDATION_SHUFFLE': False,\n",
    "    'VALIDATION_NUM_WORKERS': 0,\n",
    "    'BEST_LOSS': float('inf'),\n",
    "    'CURRENT_VALIDATION_LOSS': 0\n",
    "}\n",
    "\n",
    "model_file = run_training(args_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19db98-970e-4f36-8aa4-9b3a63c8a79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
